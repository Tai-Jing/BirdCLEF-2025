{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f50f4ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-03T06:38:41.133850Z",
     "iopub.status.busy": "2025-06-03T06:38:41.133246Z",
     "iopub.status.idle": "2025-06-03T06:38:53.395473Z",
     "shell.execute_reply": "2025-06-03T06:38:53.394327Z"
    },
    "papermill": {
     "duration": 12.273618,
     "end_time": "2025-06-03T06:38:53.397841",
     "exception": false,
     "start_time": "2025-06-03T06:38:41.124223",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: ../input/openvino-wheels\r\n",
      "Processing /kaggle/input/openvino-wheels/openvino_dev-2024.6.0-17404-py3-none-any.whl (from openvino-dev[onnx]==2024.6.0->-r ../input/openvino-wheels/requirements.txt (line 1))\r\n",
      "Requirement already satisfied: defusedxml>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from openvino-dev==2024.6.0->openvino-dev[onnx]==2024.6.0->-r ../input/openvino-wheels/requirements.txt (line 1)) (0.7.1)\r\n",
      "Processing /kaggle/input/openvino-wheels/networkx-3.1-py3-none-any.whl (from openvino-dev==2024.6.0->openvino-dev[onnx]==2024.6.0->-r ../input/openvino-wheels/requirements.txt (line 1))\r\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.16.6 in /usr/local/lib/python3.11/dist-packages (from openvino-dev==2024.6.0->openvino-dev[onnx]==2024.6.0->-r ../input/openvino-wheels/requirements.txt (line 1)) (1.26.4)\r\n",
      "Processing /kaggle/input/openvino-wheels/openvino_telemetry-2025.1.0-py3-none-any.whl (from openvino-dev==2024.6.0->openvino-dev[onnx]==2024.6.0->-r ../input/openvino-wheels/requirements.txt (line 1))\r\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from openvino-dev==2024.6.0->openvino-dev[onnx]==2024.6.0->-r ../input/openvino-wheels/requirements.txt (line 1)) (25.0)\r\n",
      "Requirement already satisfied: pyyaml>=5.4.1 in /usr/local/lib/python3.11/dist-packages (from openvino-dev==2024.6.0->openvino-dev[onnx]==2024.6.0->-r ../input/openvino-wheels/requirements.txt (line 1)) (6.0.2)\r\n",
      "Requirement already satisfied: requests>=2.25.1 in /usr/local/lib/python3.11/dist-packages (from openvino-dev==2024.6.0->openvino-dev[onnx]==2024.6.0->-r ../input/openvino-wheels/requirements.txt (line 1)) (2.32.3)\r\n",
      "Processing /kaggle/input/openvino-wheels/openvino-2024.6.0-17404-cp311-cp311-manylinux2014_x86_64.whl (from openvino-dev==2024.6.0->openvino-dev[onnx]==2024.6.0->-r ../input/openvino-wheels/requirements.txt (line 1))\r\n",
      "Processing /kaggle/input/openvino-wheels/fastjsonschema-2.17.1-py3-none-any.whl (from openvino-dev[onnx]==2024.6.0->-r ../input/openvino-wheels/requirements.txt (line 1))\r\n",
      "Requirement already satisfied: onnx<=1.17.0,>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from openvino-dev[onnx]==2024.6.0->-r ../input/openvino-wheels/requirements.txt (line 1)) (1.17.0)\r\n",
      "Requirement already satisfied: protobuf<4.0.0,>=3.18.1 in /usr/local/lib/python3.11/dist-packages (from openvino-dev[onnx]==2024.6.0->-r ../input/openvino-wheels/requirements.txt (line 1)) (3.20.3)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<2.0.0,>=1.16.6->openvino-dev==2024.6.0->openvino-dev[onnx]==2024.6.0->-r ../input/openvino-wheels/requirements.txt (line 1)) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<2.0.0,>=1.16.6->openvino-dev==2024.6.0->openvino-dev[onnx]==2024.6.0->-r ../input/openvino-wheels/requirements.txt (line 1)) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<2.0.0,>=1.16.6->openvino-dev==2024.6.0->openvino-dev[onnx]==2024.6.0->-r ../input/openvino-wheels/requirements.txt (line 1)) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<2.0.0,>=1.16.6->openvino-dev==2024.6.0->openvino-dev[onnx]==2024.6.0->-r ../input/openvino-wheels/requirements.txt (line 1)) (2025.1.0)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<2.0.0,>=1.16.6->openvino-dev==2024.6.0->openvino-dev[onnx]==2024.6.0->-r ../input/openvino-wheels/requirements.txt (line 1)) (2022.1.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<2.0.0,>=1.16.6->openvino-dev==2024.6.0->openvino-dev[onnx]==2024.6.0->-r ../input/openvino-wheels/requirements.txt (line 1)) (2.4.1)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.25.1->openvino-dev==2024.6.0->openvino-dev[onnx]==2024.6.0->-r ../input/openvino-wheels/requirements.txt (line 1)) (3.4.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.25.1->openvino-dev==2024.6.0->openvino-dev[onnx]==2024.6.0->-r ../input/openvino-wheels/requirements.txt (line 1)) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.25.1->openvino-dev==2024.6.0->openvino-dev[onnx]==2024.6.0->-r ../input/openvino-wheels/requirements.txt (line 1)) (2.4.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.25.1->openvino-dev==2024.6.0->openvino-dev[onnx]==2024.6.0->-r ../input/openvino-wheels/requirements.txt (line 1)) (2025.4.26)\r\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.0.0,>=1.16.6->openvino-dev==2024.6.0->openvino-dev[onnx]==2024.6.0->-r ../input/openvino-wheels/requirements.txt (line 1)) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.0.0,>=1.16.6->openvino-dev==2024.6.0->openvino-dev[onnx]==2024.6.0->-r ../input/openvino-wheels/requirements.txt (line 1)) (2022.1.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<2.0.0,>=1.16.6->openvino-dev==2024.6.0->openvino-dev[onnx]==2024.6.0->-r ../input/openvino-wheels/requirements.txt (line 1)) (1.3.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<2.0.0,>=1.16.6->openvino-dev==2024.6.0->openvino-dev[onnx]==2024.6.0->-r ../input/openvino-wheels/requirements.txt (line 1)) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<2.0.0,>=1.16.6->openvino-dev==2024.6.0->openvino-dev[onnx]==2024.6.0->-r ../input/openvino-wheels/requirements.txt (line 1)) (2024.2.0)\r\n",
      "Installing collected packages: openvino-telemetry, fastjsonschema, networkx, openvino, openvino-dev\r\n",
      "  Attempting uninstall: fastjsonschema\r\n",
      "    Found existing installation: fastjsonschema 2.21.1\r\n",
      "    Uninstalling fastjsonschema-2.21.1:\r\n",
      "      Successfully uninstalled fastjsonschema-2.21.1\r\n",
      "  Attempting uninstall: networkx\r\n",
      "    Found existing installation: networkx 3.4.2\r\n",
      "    Uninstalling networkx-3.4.2:\r\n",
      "      Successfully uninstalled networkx-3.4.2\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "nx-cugraph-cu12 25.2.0 requires networkx>=3.2, but you have networkx 3.1 which is incompatible.\r\n",
      "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.9.0.13 which is incompatible.\r\n",
      "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\r\n",
      "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.4.0.6 which is incompatible.\r\n",
      "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.10.19 which is incompatible.\r\n",
      "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.7.4.40 which is incompatible.\r\n",
      "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.9.5 which is incompatible.\r\n",
      "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.9.41 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed fastjsonschema-2.17.1 networkx-3.1 openvino-2024.6.0 openvino-dev-2024.6.0 openvino-telemetry-2025.1.0\r\n"
     ]
    }
   ],
   "source": [
    "! python -m pip install --no-index --find-links=../input/openvino-wheels -r ../input/openvino-wheels/requirements.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ff14776",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-03T06:38:53.414061Z",
     "iopub.status.busy": "2025-06-03T06:38:53.413739Z",
     "iopub.status.idle": "2025-06-03T06:39:16.490244Z",
     "shell.execute_reply": "2025-06-03T06:39:16.488766Z"
    },
    "papermill": {
     "duration": 23.086889,
     "end_time": "2025-06-03T06:39:16.492375",
     "exception": false,
     "start_time": "2025-06-03T06:38:53.405486",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import warnings\n",
    "import logging\n",
    "import time\n",
    "import math\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "from typing import Union\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "from soundfile import SoundFile \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import timm\n",
    "import functools \n",
    "from tqdm.auto import tqdm\n",
    "from glob import glob\n",
    "import torchaudio\n",
    "import random\n",
    "import itertools\n",
    "from typing import Union\n",
    "\n",
    "import pickle\n",
    "import torchaudio\n",
    "import torchaudio.transforms as AT\n",
    "from contextlib import contextmanager\n",
    "import concurrent.futures\n",
    "\n",
    "\n",
    "from openvino.tools import mo # 用于模型转换\n",
    "import openvino as ov\n",
    "from openvino.runtime import Core # 用于模型加载和推理\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "\n",
    "model_dicts = {\n",
    "    \"efficientnet_b0\":[\n",
    "                       '/kaggle/input/generate-effinet-openvino/model_0/efficientnet_b0.xml' ,\n",
    "                        \"/kaggle/input/generate-effinet-openvino/model_1/efficientnet_b0.xml\",\n",
    "                        \"/kaggle/input/generate-effinet-openvino/model_2/efficientnet_b0.xml\" ],\n",
    "     \"seresnext26t_32x4d\":[\n",
    "         \"/kaggle/input/generate-seresnext-openvino/openvino_models/seresnext26t_32x4d.xml\"],\n",
    "     \n",
    "     'eca_nfnet_l0':[\"/kaggle/input/generate-eca-nfnet-openvino/model_0/eca_nfnet_l0_0.xml\",\n",
    "                     \"/kaggle/input/generate-eca-nfnet-openvino/model_1/eca_nfnet_l0_1.xml\",\n",
    "                     \"/kaggle/input/generate-eca-nfnet-openvino/model_2/eca_nfnet_l0_2.xml\"],\n",
    "     'convnextv2_nano.fcmae_ft_in22k_in1k':[\n",
    "         \"/kaggle/input/generate-convnextv2-openvino/model_0/convnextv2_nano.fcmae_ft_in22k_in1k.xml\",\n",
    "         \"/kaggle/input/generate-convnextv2-openvino/model_1/convnextv2_nano.fcmae_ft_in22k_in1k.xml\"]\n",
    " }\n",
    "\n",
    "def load_openvino_model(xml_paths, device=\"CPU\"):\n",
    "    \"\"\"\n",
    "    加载 OpenVINO IR 模型并编译。\n",
    "    \"\"\"\n",
    "    core = Core() # 创建 OpenVINO Core 对象\n",
    "    models = []\n",
    "    for xml_path in xml_paths:  \n",
    "        model = core.read_model(model=xml_path) # 读取 OpenVINO IR 模型\n",
    "        compiled_model = core.compile_model(model=model, device_name=device) # 编译模型以优化到指定设备\n",
    "        models.append(compiled_model)\n",
    "        print(f\"OpenVINO 模型 '{Path(xml_path).stem}' 已编译到设备: {device}\")\n",
    "    return models\n",
    "\n",
    "     \n",
    "     \n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6ea0b4",
   "metadata": {
    "papermill": {
     "duration": 0.006639,
     "end_time": "2025-06-03T06:39:16.506624",
     "exception": false,
     "start_time": "2025-06-03T06:39:16.499985",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# first efficientnet-b0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19e6de4a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-03T06:39:16.523938Z",
     "iopub.status.busy": "2025-06-03T06:39:16.522581Z",
     "iopub.status.idle": "2025-06-03T06:39:18.141246Z",
     "shell.execute_reply": "2025-06-03T06:39:18.140251Z"
    },
    "papermill": {
     "duration": 1.629155,
     "end_time": "2025-06-03T06:39:18.142656",
     "exception": false,
     "start_time": "2025-06-03T06:39:16.513501",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Loading taxonomy data...\n",
      "Number of classes: 206\n",
      "main:Starting BirdCLEF-2025 inference...\n",
      "TTA enabled: False (variations: 0)\n",
      "OpenVINO 模型 'efficientnet_b0' 已编译到设备: CPU\n",
      "OpenVINO 模型 'efficientnet_b0' 已编译到设备: CPU\n",
      "OpenVINO 模型 'efficientnet_b0' 已编译到设备: CPU\n",
      "Model usage: Ensemble of 3 models\n",
      "run_inference:Found 0 test soundscapes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5924b37aa6c447d5a042d2f105351c65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create_submission:Creating submission dataframe...\n",
      "Inference completed in 0.03 minutes\n"
     ]
    }
   ],
   "source": [
    "def apply_power_to_low_ranked_cols(\n",
    "    p: np.ndarray,\n",
    "    top_k: int = 30,\n",
    "    exponent: Union[int, float] = 2,\n",
    "    inplace: bool = True\n",
    ") -> np.ndarray:\n",
    "    if not inplace:\n",
    "        p = p.copy()\n",
    "    tail_cols = np.argsort(-p.max(axis=0))[top_k:]\n",
    "    p[:, tail_cols] = p[:, tail_cols] ** exponent\n",
    "    return p\n",
    "    \n",
    "class CFG:\n",
    " \n",
    "    test_soundscapes = '/kaggle/input/birdclef-2025/test_soundscapes'\n",
    "    submission_csv = '/kaggle/input/birdclef-2025/sample_submission.csv'\n",
    "    taxonomy_csv = '/kaggle/input/birdclef-2025/taxonomy.csv'\n",
    "\n",
    "    # Audio parameters\n",
    "    FS = 32000  \n",
    "    WINDOW_SIZE = 5  \n",
    "    \n",
    "    # Mel spectrogram parameters\n",
    "    N_FFT = 1024\n",
    "    HOP_LENGTH = 512\n",
    "    N_MELS = 128\n",
    "    FMIN = 50\n",
    "    FMAX = 16000\n",
    "    TARGET_SHAPE = (256, 256)\n",
    "\n",
    "  \n",
    "    in_channels = 1\n",
    "    device = 'cpu'  \n",
    "    \n",
    "    # Inference parameters\n",
    "    batch_size = 32\n",
    "    use_tta = False\n",
    "    # 是否使用测试时增强（Test Time Augmentation，TTA）。 TTA 是一种通过对测试样本进行增强来提高模型性能的技术。\n",
    "    tta_count = 2   \n",
    "    # TTA 的次数。 如果 use_tta 为 True，则指定对每个测试样本进行多少次增强。\n",
    "    threshold = 0.5\n",
    "\n",
    "    debug =  False\n",
    "    # True  False\n",
    "    debug_count = 3\n",
    "\n",
    "cfg = CFG()\n",
    "print(f\"Using device: {cfg.device}\")\n",
    "print(f\"Loading taxonomy data...\")\n",
    "taxonomy_df = pd.read_csv(cfg.taxonomy_csv)\n",
    "species_ids = taxonomy_df['primary_label'].tolist()\n",
    "num_classes = len(species_ids)\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "\n",
    "def audio2melspec(audio_data, cfg):\n",
    "    \"\"\"Convert audio data to mel spectrogram\"\"\"\n",
    "    if np.isnan(audio_data).any():\n",
    "        mean_signal = np.nanmean(audio_data)\n",
    "        audio_data = np.nan_to_num(audio_data, nan=mean_signal)\n",
    "\n",
    "    mel_spec = librosa.feature.melspectrogram(\n",
    "        y=audio_data,\n",
    "        sr=cfg.FS,\n",
    "        n_fft=cfg.N_FFT,\n",
    "        hop_length=cfg.HOP_LENGTH,\n",
    "        n_mels=cfg.N_MELS,\n",
    "        fmin=cfg.FMIN,\n",
    "        fmax=cfg.FMAX,\n",
    "        power=2.0,\n",
    "        pad_mode=\"reflect\",\n",
    "        norm='slaney',\n",
    "        htk=True,\n",
    "        center=True,\n",
    "    )\n",
    "\n",
    "    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "    mel_spec_norm = (mel_spec_db - mel_spec_db.min()) / (mel_spec_db.max() - mel_spec_db.min() + 1e-8)\n",
    "    \n",
    "    return mel_spec_norm\n",
    "\n",
    "def process_audio_segment(audio_data, cfg):\n",
    "    \"\"\"Process audio segment to get mel spectrogram\"\"\"\n",
    "    if len(audio_data) < cfg.FS * cfg.WINDOW_SIZE:\n",
    "        audio_data = np.pad(audio_data, \n",
    "                          (0, cfg.FS * cfg.WINDOW_SIZE - len(audio_data)), \n",
    "                          mode='constant')\n",
    "    \n",
    "    mel_spec = audio2melspec(audio_data, cfg)\n",
    "    \n",
    "    # Resize if needed\n",
    "    if mel_spec.shape != cfg.TARGET_SHAPE:\n",
    "        mel_spec = cv2.resize(mel_spec, cfg.TARGET_SHAPE, interpolation=cv2.INTER_LINEAR)\n",
    "        \n",
    "    return mel_spec\n",
    "def sigmoid(x): \n",
    "    return 1 / (1 + np.exp(-x))\n",
    "def predict_on_spectrogram(audio_path, models, cfg, species_ids):\n",
    "    \"\"\"Process a single audio file and predict species presence for each 5-second segment\"\"\"\n",
    "    predictions = []\n",
    "    row_ids = []\n",
    "    soundscape_id = Path(audio_path).stem   #提取音频文件的名称\n",
    "    \n",
    "    # try:\n",
    "    print(f\"predict_on_spectrogram:Processing {soundscape_id}\")\n",
    "    audio_data, _ = librosa.load(audio_path, sr=cfg.FS)\n",
    "    \n",
    "    total_segments = int(len(audio_data) / (cfg.FS * cfg.WINDOW_SIZE))\n",
    "        #将   soundscape文件中所有的ogg音频文件截取成  5s/5s 的片段。\n",
    "    for segment_idx in range(total_segments):\n",
    "        start_sample = segment_idx * cfg.FS * cfg.WINDOW_SIZE\n",
    "        end_sample = start_sample + cfg.FS * cfg.WINDOW_SIZE\n",
    "        segment_audio = audio_data[start_sample:end_sample]\n",
    "        \n",
    "        end_time_sec = (segment_idx + 1) * cfg.WINDOW_SIZE\n",
    "        row_id = f\"{soundscape_id}_{end_time_sec}\"\n",
    "        row_ids.append(row_id)\n",
    "\n",
    "\n",
    "        \n",
    "        mel_spec = process_audio_segment(segment_audio, cfg)\n",
    "        ov_input = mel_spec[np.newaxis, np.newaxis, :, :].astype(np.float32)\n",
    "        \n",
    "        if len(models) == 1:\n",
    "            outputs = model[0](ov_input)['output_0']\n",
    "            final_preds = sigmoid(outputs).squeeze()\n",
    "        else:\n",
    "            segment_preds = []\n",
    "            for model in models:\n",
    "                outputs = model(ov_input)['output_0']\n",
    "                probs = sigmoid(outputs).squeeze()\n",
    "                segment_preds.append(probs)\n",
    "\n",
    "            final_preds = np.mean(segment_preds, axis=0)\n",
    "                \n",
    "        predictions.append(final_preds)\n",
    "            \n",
    "    # except Exception as e:\n",
    "    #     print(f\"predict_on_spectrogram:Error processing {audio_path}: {e}\")\n",
    "        \n",
    "    # 第一平滑\n",
    "    # print(len(predictions))\n",
    "    # print(predictions)\n",
    "    predictions = np.vstack(predictions) \n",
    "    predictions = apply_power_to_low_ranked_cols(predictions, top_k=30,exponent=2)\n",
    "    # print(predictions.shape)\n",
    "    return row_ids, predictions\n",
    "\n",
    "def run_inference(cfg, models, species_ids):\n",
    "    \"\"\"Run inference on all test soundscapes\"\"\"\n",
    "    test_files = list(Path(cfg.test_soundscapes).glob('*.ogg'))\n",
    "    \n",
    "    if cfg.debug:\n",
    "        print(f\"run_inference:Debug mode enabled, using only {cfg.debug_count} files\")\n",
    "        test_files = test_files[:cfg.debug_count]\n",
    "    \n",
    "    print(f\"run_inference:Found {len(test_files)} test soundscapes\")\n",
    "\n",
    "    all_row_ids = []\n",
    "    all_predictions = []\n",
    "    \n",
    "\n",
    "    for audio_path in tqdm(test_files):\n",
    "        row_ids, predictions = predict_on_spectrogram(str(audio_path), models, cfg, species_ids)\n",
    "        all_row_ids.extend(row_ids)\n",
    "        all_predictions.extend(predictions.tolist())\n",
    "    \n",
    "    return all_row_ids, all_predictions   # 记录的id列表。还有对应的预测向量。\n",
    "\n",
    "def create_submission(row_ids, predictions, species_ids, cfg):\n",
    "    \"\"\"Create submission dataframe\"\"\"\n",
    "    print(\"create_submission:Creating submission dataframe...\")\n",
    "\n",
    "    submission_dict = {'row_id': row_ids}\n",
    "    # print(f\"create_submission:row_ids:{row_ids}\")\n",
    "    \n",
    "    for i, species in enumerate(species_ids):\n",
    "        submission_dict[species] = [pred[i] for pred in predictions]\n",
    "\n",
    "    submission_df = pd.DataFrame(submission_dict)\n",
    "\n",
    "    submission_df.set_index('row_id', inplace=True)\n",
    "\n",
    "    sample_sub = pd.read_csv(cfg.submission_csv, index_col='row_id')\n",
    "\n",
    "    missing_cols = set(sample_sub.columns) - set(submission_df.columns)\n",
    "    if missing_cols:\n",
    "        print(f\"create_submission:Warning: Missing {len(missing_cols)} species columns in submission\")\n",
    "        for col in missing_cols:\n",
    "            submission_df[col] = 0.0\n",
    "\n",
    "    submission_df = submission_df[sample_sub.columns]\n",
    "    # print(f'create_submission:使用row_id作为索引的submission_df:{submission_df}')\n",
    "\n",
    "    submission_df = submission_df.reset_index()\n",
    "    # print(f'create_submission:reset_index之后的submission_df:{submission_df}')\n",
    "    \n",
    "    return submission_df\n",
    "def main():\n",
    "\n",
    "    start_time = time.time()\n",
    "    print(\"main:Starting BirdCLEF-2025 inference...\")\n",
    "    print(f\"TTA enabled: {cfg.use_tta} (variations: {cfg.tta_count if cfg.use_tta else 0})\")\n",
    "\n",
    "    models = load_openvino_model(xml_paths=model_dicts[\"efficientnet_b0\"])\n",
    "\n",
    "    \n",
    "    if not models:\n",
    "        print(\"main:No models found! Please check model paths.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Model usage: {'Single model' if len(models) == 1 else f'Ensemble of {len(models)} models'}\")\n",
    "\n",
    "    row_ids, predictions = run_inference(cfg, models, species_ids)\n",
    "\n",
    "    submission_df = create_submission(row_ids, predictions, species_ids, cfg)\n",
    "\n",
    "    submission_path = 'submission0.csv'\n",
    "    submission_df.to_csv(submission_path, index=False)\n",
    "    # 时间平滑\n",
    "\n",
    "    sub = pd.read_csv('submission0.csv')\n",
    "    cols = sub.columns[1:]\n",
    "    groups = sub['row_id'].str.rsplit('_', n=1).str[0]\n",
    "    groups = groups.values\n",
    "    for group in np.unique(groups):\n",
    "        sub_group = sub[group == groups]\n",
    "        predictions = sub_group[cols].values\n",
    "        new_predictions = predictions.copy()\n",
    "        for i in range(1, predictions.shape[0]-1):\n",
    "            new_predictions[i] = (predictions[i-1] * 0.1) + (predictions[i] * 0.8) + (predictions[i+1] * 0.1)\n",
    "        new_predictions[0] = (predictions[0] * 0.8) + (predictions[1] * 0.2)\n",
    "        new_predictions[-1] = (predictions[-1] * 0.8) + (predictions[-2] * 0.2)\n",
    "        sub_group[cols] = new_predictions\n",
    "        sub[group == groups] = sub_group\n",
    "    sub.to_csv(\"submission0.csv\", index=False)\n",
    "        \n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"Inference completed in {(end_time - start_time)/60:.2f} minutes\")\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c0b68be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-03T06:39:18.159723Z",
     "iopub.status.busy": "2025-06-03T06:39:18.159375Z",
     "iopub.status.idle": "2025-06-03T06:39:18.163421Z",
     "shell.execute_reply": "2025-06-03T06:39:18.162427Z"
    },
    "papermill": {
     "duration": 0.014371,
     "end_time": "2025-06-03T06:39:18.164909",
     "exception": false,
     "start_time": "2025-06-03T06:39:18.150538",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df = pd.read_csv(\"/kaggle/working/submission0.csv\")\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3f50a5",
   "metadata": {
    "papermill": {
     "duration": 0.007239,
     "end_time": "2025-06-03T06:39:18.179525",
     "exception": false,
     "start_time": "2025-06-03T06:39:18.172286",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Second seresnext26t_32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "193198b6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-03T06:39:18.195546Z",
     "iopub.status.busy": "2025-06-03T06:39:18.195208Z",
     "iopub.status.idle": "2025-06-03T06:39:18.201251Z",
     "shell.execute_reply": "2025-06-03T06:39:18.200279Z"
    },
    "papermill": {
     "duration": 0.015619,
     "end_time": "2025-06-03T06:39:18.202678",
     "exception": false,
     "start_time": "2025-06-03T06:39:18.187059",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    \n",
    "    seed = 42\n",
    "    print_freq = 100\n",
    "    num_workers = 4\n",
    "\n",
    "    stage = 'train_bce'\n",
    "\n",
    "    train_datadir = '/kaggle/input/birdclef-2025/test_audio'\n",
    "    train_csv = '/kaggle/input/birdclef-2025/train.csv'\n",
    "    test_soundscapes = '/kaggle/input/birdclef-2025/test_soundscapes'\n",
    "    submission_csv = '/kaggle/input/birdclef-2025/sample_submission.csv'\n",
    "    taxonomy_csv = '/kaggle/input/birdclef-2025/taxonomy.csv'\n",
    "                  \n",
    " \n",
    "    pretrained = False\n",
    "    in_channels = 1\n",
    "\n",
    "    \n",
    "    SR = 32000\n",
    "    target_duration = 5\n",
    "    train_duration = 10\n",
    "    infer_duration  = 5\n",
    "\n",
    "\n",
    "    sample_rate=32000\n",
    "    hop_length=417\n",
    "    n_mels=256\n",
    "    f_min=20\n",
    "    f_max=16000\n",
    "    n_fft=2048\n",
    "    normal=80\n",
    "\n",
    "    device = 'cpu'\n",
    "\n",
    "cfg = CFG()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1cb15306",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-03T06:39:18.219784Z",
     "iopub.status.busy": "2025-06-03T06:39:18.219413Z",
     "iopub.status.idle": "2025-06-03T06:39:18.228927Z",
     "shell.execute_reply": "2025-06-03T06:39:18.227768Z"
    },
    "papermill": {
     "duration": 0.020263,
     "end_time": "2025-06-03T06:39:18.230427",
     "exception": false,
     "start_time": "2025-06-03T06:39:18.210164",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Loading taxonomy data...\n",
      "Number of classes: 206\n"
     ]
    }
   ],
   "source": [
    "print(f\"Using device: {cfg.device}\")\n",
    "print(f\"Loading taxonomy data...\")\n",
    "taxonomy_df = pd.read_csv(cfg.taxonomy_csv)\n",
    "species_ids = taxonomy_df['primary_label'].tolist()\n",
    "num_classes = len(species_ids)\n",
    "print(f\"Number of classes: {num_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30aabf7c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-03T06:39:18.248715Z",
     "iopub.status.busy": "2025-06-03T06:39:18.248398Z",
     "iopub.status.idle": "2025-06-03T06:39:18.265160Z",
     "shell.execute_reply": "2025-06-03T06:39:18.264244Z"
    },
    "papermill": {
     "duration": 0.027401,
     "end_time": "2025-06-03T06:39:18.266815",
     "exception": false,
     "start_time": "2025-06-03T06:39:18.239414",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    \"\"\"\n",
    "    Set seed for reproducibility\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(cfg.seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4227637",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-03T06:39:18.284583Z",
     "iopub.status.busy": "2025-06-03T06:39:18.284283Z",
     "iopub.status.idle": "2025-06-03T06:39:18.303245Z",
     "shell.execute_reply": "2025-06-03T06:39:18.302378Z"
    },
    "papermill": {
     "duration": 0.029947,
     "end_time": "2025-06-03T06:39:18.304665",
     "exception": false,
     "start_time": "2025-06-03T06:39:18.274718",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class BirdCLEFModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        \n",
    "        taxonomy_df = pd.read_csv('/kaggle/input/birdclef-2025/taxonomy.csv')\n",
    "        self.num_classes = len(taxonomy_df)\n",
    "\n",
    "        self.bn0 = nn.BatchNorm2d(cfg['n_mels'])\n",
    "        \n",
    "        self.backbone = timm.create_model(\n",
    "            cfg['model_name'],\n",
    "            pretrained=False,\n",
    "            in_chans=cfg['in_channels'],\n",
    "            drop_rate=0.0,\n",
    "            drop_path_rate=0.0,\n",
    "        )\n",
    "\n",
    "        layers = list(self.backbone.children())[:-2]\n",
    "        self.encoder = nn.Sequential(*layers)\n",
    "        \n",
    "        if \"efficientnet\" in self.cfg['model_name']:\n",
    "            backbone_out = self.backbone.classifier.in_features\n",
    "        elif \"eca\" in self.cfg['model_name']:\n",
    "            backbone_out = self.backbone.head.fc.in_features\n",
    "        elif \"res\" in self.cfg['model_name']:\n",
    "            backbone_out = self.backbone.fc.in_features\n",
    "        else:\n",
    "            backbone_out = self.backbone.num_features\n",
    "            \n",
    "        \n",
    "        self.fc1 = nn.Linear(backbone_out, backbone_out, bias=True)\n",
    "        self.att_block = AttBlockV2(backbone_out, self.num_classes, activation=\"sigmoid\")\n",
    "\n",
    "        self.melspec_transform = torchaudio.transforms.MelSpectrogram(\n",
    "            sample_rate=self.cfg['SR'],\n",
    "            hop_length=self.cfg['hop_length'],\n",
    "            n_mels=self.cfg['n_mels'],\n",
    "            f_min=self.cfg['f_min'],\n",
    "            f_max=self.cfg['f_max'],\n",
    "            n_fft=self.cfg['n_fft'],\n",
    "            pad_mode=\"constant\",\n",
    "            norm=\"slaney\",\n",
    "            onesided=True,\n",
    "            mel_scale=\"htk\",\n",
    "        )\n",
    "        if self.cfg['device'] == \"cuda\":\n",
    "            self.melspec_transform = self.melspec_transform.cuda()\n",
    "        else:\n",
    "            self.melspec_transform = self.melspec_transform.cpu()\n",
    "\n",
    "        self.db_transform = torchaudio.transforms.AmplitudeToDB(\n",
    "            stype=\"power\", top_db=80\n",
    "        )\n",
    "\n",
    "\n",
    "    def extract_feature(self,x):\n",
    "        x = x.permute((0, 1, 3, 2))\n",
    "        frames_num = x.shape[2]\n",
    "        \n",
    "        x = x.transpose(1, 3)\n",
    "        x = self.bn0(x)\n",
    "        x = x.transpose(1, 3)\n",
    "        \n",
    "        # if self.training:\n",
    "        #    x = self.spec_augmenter(x)\n",
    "        \n",
    "        x = x.transpose(2, 3)\n",
    "        # (batch_size, channels, freq, frames)\n",
    "        x = self.encoder(x)\n",
    "        \n",
    "        # (batch_size, channels, frames)\n",
    "        x = torch.mean(x, dim=2)\n",
    "        \n",
    "        # channel smoothing\n",
    "        x1 = F.max_pool1d(x, kernel_size=3, stride=1, padding=1)\n",
    "        x2 = F.avg_pool1d(x, kernel_size=3, stride=1, padding=1)\n",
    "        x = x1 + x2\n",
    "        \n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = x.transpose(1, 2)\n",
    "        x = F.relu_(self.fc1(x))\n",
    "        x = x.transpose(1, 2)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        return x, frames_num\n",
    "        \n",
    "    @torch.cuda.amp.autocast(enabled=False)\n",
    "    def transform_to_spec(self, audio):\n",
    "\n",
    "        audio = audio.float()\n",
    "        \n",
    "        spec = self.melspec_transform(audio)\n",
    "        spec = self.db_transform(spec)\n",
    "\n",
    "        if self.cfg['normal'] == 80:\n",
    "            spec = (spec + 80) / 80\n",
    "        elif self.cfg['normal'] == 255:\n",
    "            spec = spec / 255\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "                \n",
    "        if self.cfg['in_channels'] == 3:\n",
    "            spec = image_delta(spec)\n",
    "        \n",
    "        return spec\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        with torch.no_grad():\n",
    "            x = self.transform_to_spec(x)\n",
    "        \n",
    "        x, frames_num = self.extract_feature(x)\n",
    "        \n",
    "        (clipwise_output, norm_att, segmentwise_output) = self.att_block(x)\n",
    "        logit = torch.sum(norm_att * self.att_block.cla(x), dim=2)\n",
    "        segmentwise_logit = self.att_block.cla(x).transpose(1, 2)\n",
    "        segmentwise_output = segmentwise_output.transpose(1, 2)\n",
    "\n",
    "        return torch.logit(clipwise_output)\n",
    "\n",
    "    def infer(self, x, tta_delta=2):\n",
    "        with torch.no_grad():\n",
    "            x = self.transform_to_spec(x)\n",
    "        x,_ = self.extract_feature(x)\n",
    "        time_att = torch.tanh(self.att_block.att(x))\n",
    "        feat_time = x.size(-1)\n",
    "        start = (\n",
    "            feat_time / 2 - feat_time * (self.cfg['infer_duration'] / self.cfg['duration_train']) / 2\n",
    "        )\n",
    "        end = start + feat_time * (self.cfg['infer_duration'] / self.cfg['duration_train'])\n",
    "        start = int(start)\n",
    "        end = int(end)\n",
    "        pred = self.attention_infer(start,end,x,time_att)\n",
    "\n",
    "        start_minus = max(0, start-tta_delta)\n",
    "        end_minus=end-tta_delta\n",
    "        pred_minus = self.attention_infer(start_minus,end_minus,x,time_att)\n",
    "\n",
    "        start_plus = start+tta_delta\n",
    "        end_plus=min(feat_time, end+tta_delta)\n",
    "        pred_plus = self.attention_infer(start_plus,end_plus,x,time_att)\n",
    "\n",
    "        pred = 0.5*pred + 0.25*pred_minus + 0.25*pred_plus\n",
    "        return pred\n",
    "        \n",
    "    def attention_infer(self,start,end,x,time_att):\n",
    "        feat = x[:, :, start:end]\n",
    "        framewise_pred = torch.sigmoid(self.att_block.cla(feat))\n",
    "        framewise_pred_max = framewise_pred.max(dim=2)[0]\n",
    "        return framewise_pred_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96216a63",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-03T06:39:18.321570Z",
     "iopub.status.busy": "2025-06-03T06:39:18.320894Z",
     "iopub.status.idle": "2025-06-03T06:39:18.332027Z",
     "shell.execute_reply": "2025-06-03T06:39:18.330966Z"
    },
    "papermill": {
     "duration": 0.020795,
     "end_time": "2025-06-03T06:39:18.333411",
     "exception": false,
     "start_time": "2025-06-03T06:39:18.312616",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_sample(path, cfg):\n",
    "    audio, orig_sr = sf.read(path, dtype=\"float32\")\n",
    "    seconds = []\n",
    "    audio_length = cfg.SR * cfg.target_duration\n",
    "    step = audio_length\n",
    "    for i in range(audio_length, len(audio) + step, step):\n",
    "        start = max(0, i - audio_length)\n",
    "        end = start + audio_length\n",
    "        if end > len(audio):\n",
    "            pass\n",
    "        else:\n",
    "            seconds.append(int(end/cfg.SR))\n",
    "\n",
    "    audio = np.concatenate([audio,audio,audio])\n",
    "    audios = []\n",
    "    for i,second in enumerate(seconds):\n",
    "        end_seconds = int(second)\n",
    "        start_seconds = int(end_seconds - cfg.target_duration)\n",
    "    \n",
    "        end_index = int(cfg.SR * (end_seconds + (cfg.train_duration - cfg.target_duration) / 2) ) + len(audio) // 3\n",
    "        start_index = int(cfg.SR * (start_seconds - (cfg.train_duration - cfg.target_duration) / 2) ) + len(audio) // 3\n",
    "        end_pad = int(cfg.SR * (cfg.train_duration - cfg.target_duration) / 2) \n",
    "        start_pad = int(cfg.SR * (cfg.train_duration - cfg.target_duration) / 2) \n",
    "        y = audio[start_index:end_index].astype(np.float32)\n",
    "        if i==0:\n",
    "            y[:start_pad] = 0\n",
    "        elif i==(len(seconds)-1):\n",
    "            y[-end_pad:] = 0\n",
    "        audios.append(y)\n",
    "\n",
    "    return audios\n",
    "def gen_melspec(x):\n",
    "    melspec_transform = torchaudio.transforms.MelSpectrogram(\n",
    "            sample_rate = cfg.SR,\n",
    "            hop_length = cfg.hop_length,\n",
    "            n_mels = cfg.n_mels,\n",
    "            f_min = cfg.f_min,\n",
    "            f_max = cfg.f_max,\n",
    "            n_fft = cfg.n_fft,\n",
    "            pad_mode=\"constant\",\n",
    "            norm=\"slaney\",\n",
    "            onesided=True,\n",
    "            mel_scale=\"htk\")\n",
    "    \n",
    "    melspec_transform = melspec_transform.cpu()\n",
    "\n",
    "    db_transform = torchaudio.transforms.AmplitudeToDB(\n",
    "            stype=\"power\", top_db=80)\n",
    "    x = x.astype(np.float32)    \n",
    "    x  = torch.from_numpy(x)\n",
    "    spec = melspec_transform(x)\n",
    "    spec = db_transform(spec)\n",
    "\n",
    "    if cfg.normal == 80:\n",
    "        spec = (spec + 80) / 80\n",
    "    elif cfg.normal == 255:\n",
    "        spec = spec / 255\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    return spec\n",
    "    \n",
    "def sigmoid(x):\n",
    "    s = 1 / (1 + np.exp(-x))\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62978c34",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-03T06:39:18.349790Z",
     "iopub.status.busy": "2025-06-03T06:39:18.349438Z",
     "iopub.status.idle": "2025-06-03T06:39:18.358540Z",
     "shell.execute_reply": "2025-06-03T06:39:18.357647Z"
    },
    "papermill": {
     "duration": 0.019312,
     "end_time": "2025-06-03T06:39:18.360263",
     "exception": false,
     "start_time": "2025-06-03T06:39:18.340951",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_on_spectrogram(audio_path, compiled_model, cfg, species_ids):\n",
    "    \"\"\"Process a single audio file and predict species presence for each 5-second segment\"\"\"\n",
    "    audio_path = str(audio_path)\n",
    "    predictions = []\n",
    "    row_ids = []\n",
    "    soundscape_id = Path(audio_path).stem\n",
    "\n",
    "    print(f\"Processing {soundscape_id}\")\n",
    "    audio_data = load_sample(audio_path, cfg)\n",
    "\n",
    "    model_input_name = compiled_model.input(0).get_any_name()\n",
    "    model_output_name = compiled_model.output(1).get_any_name()\n",
    "    infer_request = compiled_model.create_infer_request()\n",
    "\n",
    "    \n",
    "\n",
    "    for segment_idx, audio_input in enumerate(audio_data):\n",
    "        \n",
    "        end_time_sec = (segment_idx + 1) * cfg.target_duration\n",
    "        row_id = f\"{soundscape_id}_{end_time_sec}\"\n",
    "        row_ids.append(row_id)\n",
    "        \n",
    "        x = gen_melspec(audio_input).numpy()\n",
    "        ov_input_audio = x[np.newaxis, np.newaxis, :].astype(np.float32)\n",
    "\n",
    "        infer_request.set_tensor(model_input_name, ov.Tensor(ov_input_audio))\n",
    "        infer_request.infer()\n",
    "        segmentwise_probabilities = infer_request.get_tensor(model_output_name).data\n",
    "        # print(f\"Shape of segmentwise_probabilities: {segmentwise_probabilities.shape}\") \n",
    "        \n",
    "        \n",
    "        feat_time = segmentwise_probabilities.shape[1] \n",
    "        \n",
    "        infer_duration_ratio = cfg.infer_duration / cfg.train_duration \n",
    "        start_center = int(feat_time / 2 - feat_time * infer_duration_ratio / 2)\n",
    "        end_center = int(start_center + feat_time * infer_duration_ratio)\n",
    "        tta_delta_feat = 2\n",
    "        \n",
    "        start_minus = max(0, start_center - tta_delta_feat)\n",
    "        end_minus = end_center - tta_delta_feat\n",
    "\n",
    "        start_plus = start_center + tta_delta_feat\n",
    "        end_plus = min(feat_time, end_center + tta_delta_feat)\n",
    "        pred_center = np.max(segmentwise_probabilities[:, start_center:end_center, :], axis=1)\n",
    "        pred_minus = np.max(segmentwise_probabilities[:, start_minus:end_minus, :], axis=1)\n",
    "        pred_plus = np.max(segmentwise_probabilities[:, start_plus:end_plus, :], axis=1)\n",
    "\n",
    "        final_preds = 0.5 * pred_center + 0.25 * pred_minus + 0.25 * pred_plus\n",
    "\n",
    "        predictions.append(final_preds.squeeze())\n",
    "\n",
    "    predictions = np.stack(predictions, axis=0)\n",
    "    \n",
    "    return row_ids, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d7f6c1e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-03T06:39:18.376917Z",
     "iopub.status.busy": "2025-06-03T06:39:18.376558Z",
     "iopub.status.idle": "2025-06-03T06:39:18.388927Z",
     "shell.execute_reply": "2025-06-03T06:39:18.387989Z"
    },
    "papermill": {
     "duration": 0.022556,
     "end_time": "2025-06-03T06:39:18.390301",
     "exception": false,
     "start_time": "2025-06-03T06:39:18.367745",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_inference(cfg, models, species_ids):\n",
    "    \"\"\"Run inference on all test soundscapes\"\"\"\n",
    "    test_files = list(Path(cfg.test_soundscapes).glob('*.ogg'))\n",
    "    if len(test_files) == 0:\n",
    "        test_files = sorted(glob(str(Path('/kaggle/input/birdclef-2025/train_soundscapes') / '*.ogg')))[:3]\n",
    "    \n",
    "    print(f\"Found {len(test_files)} test soundscapes\")\n",
    "\n",
    "    all_row_ids = []\n",
    "    all_predictions = []\n",
    "    compiled_model  = models[0] \n",
    "    \n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:\n",
    "\n",
    "        partial_predict_on_spectrogram = functools.partial(\n",
    "            predict_on_spectrogram,\n",
    "            compiled_model=compiled_model, \n",
    "            cfg=cfg,\n",
    "            species_ids=species_ids\n",
    "        )\n",
    "        \n",
    "        results = list(\n",
    "        executor.map(\n",
    "            partial_predict_on_spectrogram,\n",
    "            test_files\n",
    "        )\n",
    "    )\n",
    "\n",
    "    for rids, preds in results:\n",
    "        all_row_ids.extend(rids)\n",
    "        all_predictions.extend(preds)\n",
    "    \n",
    "    return all_row_ids, all_predictions\n",
    "\n",
    "def create_submission(row_ids, predictions, species_ids, cfg):\n",
    "    \"\"\"Create submission dataframe\"\"\"\n",
    "    print(\"Creating submission dataframe...\")\n",
    "\n",
    "    submission_dict = {'row_id': row_ids}\n",
    "    \n",
    "    for i, species in enumerate(species_ids):\n",
    "        submission_dict[species] = [pred[i] for pred in predictions]\n",
    "\n",
    "    submission_df = pd.DataFrame(submission_dict)\n",
    "\n",
    "    submission_df.set_index('row_id', inplace=True)\n",
    "\n",
    "    sample_sub = pd.read_csv(cfg.submission_csv, index_col='row_id')\n",
    "\n",
    "    missing_cols = set(sample_sub.columns) - set(submission_df.columns)\n",
    "    if missing_cols:\n",
    "        print(f\"Warning: Missing {len(missing_cols)} species columns in submission\")\n",
    "        for col in missing_cols:\n",
    "            submission_df[col] = 0.0\n",
    "\n",
    "    submission_df = submission_df[sample_sub.columns]\n",
    "\n",
    "    submission_df = submission_df.reset_index()\n",
    "    \n",
    "    return submission_df\n",
    "\n",
    "\n",
    "def smooth_submission(submission_path):\n",
    "        \"\"\"\n",
    "        Post-process the submission CSV by smoothing predictions to enforce temporal consistency.\n",
    "        \n",
    "        For each soundscape (grouped by the file name part of 'row_id'), each row's predictions\n",
    "        are averaged with those of its neighbors using defined weights.\n",
    "        \n",
    "        :param submission_path: Path to the submission CSV file.\n",
    "        \"\"\"\n",
    "        print(\"Smoothing submission predictions...\")\n",
    "        sub = pd.read_csv(submission_path)\n",
    "        cols = sub.columns[1:]\n",
    "        # Extract group names by splitting row_id on the last underscore\n",
    "        groups = sub['row_id'].str.rsplit('_', n=1).str[0].values\n",
    "        unique_groups = np.unique(groups)\n",
    "        \n",
    "        for group in unique_groups:\n",
    "            # Get indices for the current group\n",
    "            idx = np.where(groups == group)[0]\n",
    "            sub_group = sub.iloc[idx].copy()\n",
    "            predictions = sub_group[cols].values\n",
    "            new_predictions = predictions.copy()\n",
    "            \n",
    "            if predictions.shape[0] > 1:\n",
    "                # Smooth the predictions using neighboring segments\n",
    "                new_predictions[0] = (predictions[0] * 0.8) + (predictions[1] * 0.2)\n",
    "                new_predictions[-1] = (predictions[-1] * 0.8) + (predictions[-2] * 0.2)\n",
    "                for i in range(1, predictions.shape[0]-1):\n",
    "                    new_predictions[i] = (predictions[i-1] * 0.1) + (predictions[i] * 0.8) + (predictions[i+1] * 0.1)\n",
    "            # Replace the smoothed values in the submission dataframe\n",
    "            sub.iloc[idx, 1:] = new_predictions\n",
    "        \n",
    "        sub.to_csv(submission_path, index=False)\n",
    "        print(f\"Smoothed submission saved to {submission_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "161f5b14",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-03T06:39:18.407115Z",
     "iopub.status.busy": "2025-06-03T06:39:18.406785Z",
     "iopub.status.idle": "2025-06-03T06:39:18.413710Z",
     "shell.execute_reply": "2025-06-03T06:39:18.412707Z"
    },
    "papermill": {
     "duration": 0.017258,
     "end_time": "2025-06-03T06:39:18.415384",
     "exception": false,
     "start_time": "2025-06-03T06:39:18.398126",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    start_time = time.time()\n",
    "    print(\"Starting BirdCLEF-2025 inference...\")\n",
    "\n",
    "    models = load_openvino_model(xml_paths=model_dicts[\"seresnext26t_32x4d\"])\n",
    "    \n",
    "    if not models:\n",
    "        print(\"No models found! Please check model paths.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Model usage: {'Single model' if len(models) == 1 else f'Ensemble of {len(models)} models'}\")\n",
    "\n",
    "    row_ids, predictions = run_inference(cfg, models, species_ids)\n",
    "\n",
    "    submission_df = create_submission(row_ids, predictions, species_ids, cfg)\n",
    "\n",
    "    submission_path = 'submission1.csv'\n",
    "    submission_df.to_csv(submission_path, index=False)\n",
    "    print(f\"Submission saved to {submission_path}\")\n",
    "\n",
    "    smooth_submission(submission_path)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"Inference completed in {(end_time - start_time)/60:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "425adb7e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-03T06:39:18.432823Z",
     "iopub.status.busy": "2025-06-03T06:39:18.432498Z",
     "iopub.status.idle": "2025-06-03T06:39:26.863767Z",
     "shell.execute_reply": "2025-06-03T06:39:26.862733Z"
    },
    "papermill": {
     "duration": 8.441112,
     "end_time": "2025-06-03T06:39:26.865244",
     "exception": false,
     "start_time": "2025-06-03T06:39:18.424132",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting BirdCLEF-2025 inference...\n",
      "OpenVINO 模型 'seresnext26t_32x4d' 已编译到设备: CPU\n",
      "Model usage: Single model\n",
      "Found 3 test soundscapes\n",
      "Processing H02_20230420_074000\n",
      "Processing H02_20230420_112000\n",
      "Processing H02_20230420_154500\n",
      "Creating submission dataframe...\n",
      "Submission saved to submission1.csv\n",
      "Smoothing submission predictions...\n",
      "Smoothed submission saved to submission1.csv\n",
      "Inference completed in 0.14 minutes\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fd978433",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-03T06:39:26.882562Z",
     "iopub.status.busy": "2025-06-03T06:39:26.881823Z",
     "iopub.status.idle": "2025-06-03T06:39:26.885986Z",
     "shell.execute_reply": "2025-06-03T06:39:26.885068Z"
    },
    "papermill": {
     "duration": 0.014379,
     "end_time": "2025-06-03T06:39:26.887485",
     "exception": false,
     "start_time": "2025-06-03T06:39:26.873106",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df = pd.read_csv(\"/kaggle/working/submission1.csv\")\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224d584c",
   "metadata": {
    "papermill": {
     "duration": 0.007318,
     "end_time": "2025-06-03T06:39:26.903217",
     "exception": false,
     "start_time": "2025-06-03T06:39:26.895899",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# third eca_nfnet_l0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2b2841c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-03T06:39:26.921394Z",
     "iopub.status.busy": "2025-06-03T06:39:26.920530Z",
     "iopub.status.idle": "2025-06-03T06:39:26.927041Z",
     "shell.execute_reply": "2025-06-03T06:39:26.926145Z"
    },
    "papermill": {
     "duration": 0.017424,
     "end_time": "2025-06-03T06:39:26.928489",
     "exception": false,
     "start_time": "2025-06-03T06:39:26.911065",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def apply_power_to_low_ranked_cols(\n",
    "    p: np.ndarray,\n",
    "    top_k: int = 30,\n",
    "    exponent: Union[int, float] = 2,\n",
    "    inplace: bool = True\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Rank columns by their column‑wise maximum and raise every column whose\n",
    "    rank falls below `top_k` to a given power.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    p : np.ndarray\n",
    "        A 2‑D array of shape **(n_chunks, n_classes)**.\n",
    "\n",
    "        - **n_chunks** is the number of fixed‑length time chunks obtained\n",
    "          after slicing the input audio (or other sequential data).  \n",
    "          *Example:* In the BirdCLEF `test_soundscapes` set, each file is\n",
    "          60 s long. If you extract non‑overlapping 5 s windows,  \n",
    "          `n_chunks = 60 s / 5 s = 12`.\n",
    "        - **n_classes** is the number of classes being predicted.\n",
    "        - Each element `p[i, j]` is the score or probability of class *j*\n",
    "          in chunk *i*.\n",
    "\n",
    "    top_k : int, default=30\n",
    "        The highest‑ranked columns (by their maximum value) that remain\n",
    "        unchanged.\n",
    "\n",
    "    exponent : int or float, default=2\n",
    "        The power applied to the selected low‑ranked columns  \n",
    "        (e.g. `2` squares, `0.5` takes the square root, `3` cubes).\n",
    "\n",
    "    inplace : bool, default=True\n",
    "        If `True`, modify `p` in place.  \n",
    "        If `False`, operate on a copy and leave the original array intact.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        The transformed array. It is the same object as `p` when\n",
    "        `inplace=True`; otherwise, it is a new array.\n",
    "\n",
    "    \"\"\"\n",
    "    if not inplace:\n",
    "        p = p.copy()\n",
    "\n",
    "    # Identify columns whose max value ranks below `top_k`\n",
    "    tail_cols = np.argsort(-p.max(axis=0))[top_k:]\n",
    "\n",
    "    # Apply the power transformation to those columns\n",
    "    p[:, tail_cols] = p[:, tail_cols] ** exponent\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "076fdd27",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-03T06:39:26.945577Z",
     "iopub.status.busy": "2025-06-03T06:39:26.945280Z",
     "iopub.status.idle": "2025-06-03T06:39:26.951930Z",
     "shell.execute_reply": "2025-06-03T06:39:26.950955Z"
    },
    "papermill": {
     "duration": 0.017322,
     "end_time": "2025-06-03T06:39:26.953965",
     "exception": false,
     "start_time": "2025-06-03T06:39:26.936643",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debug mode: False\n",
      "Number of test soundscapes: 0\n"
     ]
    }
   ],
   "source": [
    "test_audio_dir = '../input/birdclef-2025/test_soundscapes/'\n",
    "file_list = [f for f in sorted(os.listdir(test_audio_dir))]\n",
    "file_list = [file.split('.')[0] for file in file_list if file.endswith('.ogg')]\n",
    "debug = False\n",
    "\n",
    "if debug == True:\n",
    "    file_list = file_list[:3]\n",
    "    \n",
    "print('Debug mode:', debug)\n",
    "print('Number of test soundscapes:', len(file_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bcc2acec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-03T06:39:26.971517Z",
     "iopub.status.busy": "2025-06-03T06:39:26.970622Z",
     "iopub.status.idle": "2025-06-03T06:39:26.988570Z",
     "shell.execute_reply": "2025-06-03T06:39:26.987644Z"
    },
    "papermill": {
     "duration": 0.02816,
     "end_time": "2025-06-03T06:39:26.990203",
     "exception": false,
     "start_time": "2025-06-03T06:39:26.962043",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "wav_sec = 5\n",
    "sample_rate = 32000\n",
    "min_segment = sample_rate*wav_sec\n",
    "\n",
    "class_labels = sorted(os.listdir('../input/birdclef-2025/train_audio/'))\n",
    "\n",
    "n_fft=1024\n",
    "win_length=1024\n",
    "hop_length=512\n",
    "f_min=50\n",
    "f_max=16000\n",
    "n_mels=128\n",
    "\n",
    "mel_spectrogram = AT.MelSpectrogram(\n",
    "    sample_rate=sample_rate,\n",
    "    n_fft=n_fft,\n",
    "    win_length=win_length,\n",
    "    hop_length=hop_length,\n",
    "    center=True,\n",
    "    f_min=f_min,\n",
    "    f_max=f_max,\n",
    "    pad_mode=\"reflect\",\n",
    "    power=2.0,\n",
    "    norm='slaney',\n",
    "    n_mels=n_mels,\n",
    "    mel_scale=\"htk\",\n",
    "    # normalized=True\n",
    ")\n",
    "\n",
    "def normalize_std(spec, eps=1e-6):\n",
    "    mean = torch.mean(spec)\n",
    "    std = torch.std(spec)\n",
    "    return torch.where(std == 0, spec-mean, (spec - mean) / (std+eps))\n",
    "\n",
    "def audio_to_mel(filepath=None):\n",
    "    waveform, sample_rate = torchaudio.load(filepath,backend=\"soundfile\")\n",
    "    len_wav = waveform.shape[1]\n",
    "    waveform = waveform[0,:].reshape(1, len_wav) # stereo->mono mono->mono\n",
    "    PREDS = []\n",
    "    for i in range(12):\n",
    "        waveform2 = waveform[:,i*sample_rate*5:i*sample_rate*5+sample_rate*5]\n",
    "        melspec = mel_spectrogram(waveform2)\n",
    "        melspec = torch.log(melspec+1e-6)\n",
    "        melspec = normalize_std(melspec)\n",
    "        melspec = torch.unsqueeze(melspec, dim=0)\n",
    "        \n",
    "        PREDS.append(melspec)\n",
    "    return torch.vstack(PREDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "31c39586",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-03T06:39:27.009114Z",
     "iopub.status.busy": "2025-06-03T06:39:27.008798Z",
     "iopub.status.idle": "2025-06-03T06:39:27.026929Z",
     "shell.execute_reply": "2025-06-03T06:39:27.025958Z"
    },
    "papermill": {
     "duration": 0.028837,
     "end_time": "2025-06-03T06:39:27.028465",
     "exception": false,
     "start_time": "2025-06-03T06:39:26.999628",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def init_layer(layer):\n",
    "    nn.init.xavier_uniform_(layer.weight)\n",
    "    if hasattr(layer, \"bias\"):\n",
    "        if layer.bias is not None:\n",
    "            layer.bias.data.fill_(0.)\n",
    "\n",
    "\n",
    "def init_bn(bn):\n",
    "    bn.bias.data.fill_(0.)\n",
    "    bn.weight.data.fill_(1.0)\n",
    "\n",
    "\n",
    "def init_weights(model):\n",
    "    classname = model.__class__.__name__\n",
    "    if classname.find(\"Conv2d\") != -1:\n",
    "        nn.init.xavier_uniform_(model.weight, gain=np.sqrt(2))\n",
    "        model.bias.data.fill_(0)\n",
    "    elif classname.find(\"BatchNorm\") != -1:\n",
    "        model.weight.data.normal_(1.0, 0.02)\n",
    "        model.bias.data.fill_(0)\n",
    "    elif classname.find(\"GRU\") != -1:\n",
    "        for weight in model.parameters():\n",
    "            if len(weight.size()) > 1:\n",
    "                nn.init.orghogonal_(weight.data)\n",
    "    elif classname.find(\"Linear\") != -1:\n",
    "        model.weight.data.normal_(0, 0.01)\n",
    "        model.bias.data.zero_()\n",
    "\n",
    "\n",
    "def interpolate(x, ratio):\n",
    "    (batch_size, time_steps, classes_num) = x.shape\n",
    "    upsampled = x[:, :, None, :].repeat(1, 1, ratio, 1)\n",
    "    upsampled = upsampled.reshape(batch_size, time_steps * ratio, classes_num)\n",
    "    return upsampled\n",
    "\n",
    "\n",
    "def pad_framewise_output(framewise_output, frames_num):\n",
    "    output = F.interpolate(\n",
    "        framewise_output.unsqueeze(1),\n",
    "        size=(frames_num, framewise_output.size(2)),\n",
    "        align_corners=True,\n",
    "        mode=\"bilinear\").squeeze(1)\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "class AttBlockV2(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_features: int,\n",
    "                 out_features: int,\n",
    "                 activation=\"linear\"):\n",
    "        super().__init__()\n",
    "\n",
    "        self.activation = activation\n",
    "        self.att = nn.Conv1d(\n",
    "            in_channels=in_features,\n",
    "            out_channels=out_features,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            bias=True)\n",
    "        self.cla = nn.Conv1d(\n",
    "            in_channels=in_features,\n",
    "            out_channels=out_features,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            bias=True)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        init_layer(self.att)\n",
    "        init_layer(self.cla)\n",
    "\n",
    "    def forward(self, x):\n",
    "        norm_att = torch.softmax(torch.tanh(self.att(x)), dim=-1)\n",
    "        cla = self.nonlinear_transform(self.cla(x))\n",
    "        x = torch.sum(norm_att * cla, dim=2)\n",
    "        return x, norm_att, cla\n",
    "\n",
    "    def nonlinear_transform(self, x):\n",
    "        if self.activation == 'linear':\n",
    "            return x\n",
    "        elif self.activation == 'sigmoid':\n",
    "            return torch.sigmoid(x)\n",
    "\n",
    "\n",
    "class TimmSED(nn.Module):\n",
    "    def __init__(self, base_model_name: str, pretrained=False, num_classes=24, in_channels=1, n_mels=24):\n",
    "        super().__init__()\n",
    "\n",
    "        self.bn0 = nn.BatchNorm2d(n_mels)\n",
    "\n",
    "        base_model = timm.create_model(\n",
    "            base_model_name, pretrained=pretrained, in_chans=in_channels)\n",
    "        layers = list(base_model.children())[:-2]\n",
    "        self.encoder = nn.Sequential(*layers)\n",
    "\n",
    "        in_features = base_model.num_features\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features, in_features, bias=True)\n",
    "        self.att_block2 = AttBlockV2(\n",
    "            in_features, num_classes, activation=\"sigmoid\")\n",
    "\n",
    "        self.init_weight()\n",
    "\n",
    "    def init_weight(self):\n",
    "        init_bn(self.bn0)\n",
    "        init_layer(self.fc1)\n",
    "        \n",
    "\n",
    "    def forward(self, input_data):\n",
    "        x = input_data.transpose(2,3)\n",
    "        x = torch.cat((x,x,x),1)\n",
    "\n",
    "        x = x.transpose(2, 3)\n",
    "\n",
    "        x = self.encoder(x)\n",
    "        \n",
    "        x = torch.mean(x, dim=2)\n",
    "\n",
    "        x1 = F.max_pool1d(x, kernel_size=3, stride=1, padding=1)\n",
    "        x2 = F.avg_pool1d(x, kernel_size=3, stride=1, padding=1)\n",
    "        x = x1 + x2\n",
    "\n",
    "        x = x.transpose(1, 2)\n",
    "        x = F.relu_(self.fc1(x))\n",
    "        x = x.transpose(1, 2)\n",
    "\n",
    "        (clipwise_output, norm_att, segmentwise_output) = self.att_block2(x)\n",
    "        logit = torch.sum(norm_att * self.att_block2.cla(x), dim=2)\n",
    "\n",
    "        output_dict = {\n",
    "            'logit': logit,\n",
    "        }\n",
    "\n",
    "        return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f98f61c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-03T06:39:27.045455Z",
     "iopub.status.busy": "2025-06-03T06:39:27.045172Z",
     "iopub.status.idle": "2025-06-03T06:39:30.891626Z",
     "shell.execute_reply": "2025-06-03T06:39:30.890657Z"
    },
    "papermill": {
     "duration": 3.856755,
     "end_time": "2025-06-03T06:39:30.893474",
     "exception": false,
     "start_time": "2025-06-03T06:39:27.036719",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenVINO 模型 'eca_nfnet_l0_0' 已编译到设备: CPU\n",
      "OpenVINO 模型 'eca_nfnet_l0_1' 已编译到设备: CPU\n",
      "OpenVINO 模型 'eca_nfnet_l0_2' 已编译到设备: CPU\n"
     ]
    }
   ],
   "source": [
    "base_model_name='eca_nfnet_l0'\n",
    "pretrained=False\n",
    "in_channels=3\n",
    "models= load_openvino_model(xml_paths=model_dicts['eca_nfnet_l0'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9f9eab4b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-03T06:39:30.917192Z",
     "iopub.status.busy": "2025-06-03T06:39:30.916802Z",
     "iopub.status.idle": "2025-06-03T06:39:30.926827Z",
     "shell.execute_reply": "2025-06-03T06:39:30.925859Z"
    },
    "papermill": {
     "duration": 0.021225,
     "end_time": "2025-06-03T06:39:30.928395",
     "exception": false,
     "start_time": "2025-06-03T06:39:30.907170",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prediction(afile):    \n",
    "    path = test_audio_dir + afile + '.ogg'\n",
    "    \n",
    "    sig = audio_to_mel(path).numpy()\n",
    "    ov_input = sig.astype(np.float32)\n",
    "    \n",
    "    outputs = None\n",
    "    for model in models:\n",
    "        \n",
    "        infer_request = model.create_infer_request()\n",
    "        output = infer_request.infer(ov_input)['output0'] \n",
    "        \n",
    "        probs = sigmoid(output)\n",
    "        p = apply_power_to_low_ranked_cols(probs, top_k=30,exponent=2)\n",
    "        if outputs is None: outputs = p\n",
    "        else: outputs += p\n",
    "    \n",
    "    outputs /= len(models)\n",
    "    \n",
    "    local_pred_results = {'row_id': []}\n",
    "    for species_code in class_labels: # class_labels 应该在全局或作为参数传入\n",
    "        local_pred_results[species_code] = []\n",
    "        \n",
    "    for i in range(12):        \n",
    "        chunk_end_time = (i + 1) * 5\n",
    "        row_id = afile + '_' + str(chunk_end_time)\n",
    "        local_pred_results['row_id'].append(row_id)\n",
    "        bird_no = 0\n",
    "        for bird in class_labels:         \n",
    "            local_pred_results[bird].append(outputs[i,bird_no])\n",
    "            bird_no += 1\n",
    "    gc.collect()\n",
    "\n",
    "    return local_pred_results \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "399309c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-03T06:39:30.946142Z",
     "iopub.status.busy": "2025-06-03T06:39:30.945799Z",
     "iopub.status.idle": "2025-06-03T06:39:30.953513Z",
     "shell.execute_reply": "2025-06-03T06:39:30.952276Z"
    },
    "papermill": {
     "duration": 0.018438,
     "end_time": "2025-06-03T06:39:30.955336",
     "exception": false,
     "start_time": "2025-06-03T06:39:30.936898",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "推理完成，耗时 0.00 秒\n",
      "finish\n"
     ]
    }
   ],
   "source": [
    "all_prediction_results = []\n",
    "start = time.time()\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
    "    # executor.map 返回一个迭代器，其元素是每个 prediction 函数的返回值\n",
    "    for result_dict in executor.map(prediction, file_list):\n",
    "        all_prediction_results.append(result_dict)\n",
    "end_t = time.time()\n",
    "\n",
    "print(f\"推理完成，耗时 {end_t - start:.2f} 秒\")\n",
    "\n",
    "final_pred_dataframe_data = {'row_id': []}\n",
    "for species_code in class_labels: # class_labels 应该可访问\n",
    "    final_pred_dataframe_data[species_code] = []\n",
    "\n",
    "for single_file_results in all_prediction_results:\n",
    "    final_pred_dataframe_data['row_id'].extend(single_file_results['row_id'])\n",
    "    for species_code in class_labels:\n",
    "        final_pred_dataframe_data[species_code].extend(single_file_results[species_code])\n",
    "print(\"finish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eeb6dfca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-03T06:39:30.972330Z",
     "iopub.status.busy": "2025-06-03T06:39:30.971993Z",
     "iopub.status.idle": "2025-06-03T06:39:31.001786Z",
     "shell.execute_reply": "2025-06-03T06:39:31.000755Z"
    },
    "papermill": {
     "duration": 0.04016,
     "end_time": "2025-06-03T06:39:31.003438",
     "exception": false,
     "start_time": "2025-06-03T06:39:30.963278",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>1139490</th>\n",
       "      <th>1192948</th>\n",
       "      <th>1194042</th>\n",
       "      <th>126247</th>\n",
       "      <th>1346504</th>\n",
       "      <th>134933</th>\n",
       "      <th>135045</th>\n",
       "      <th>1462711</th>\n",
       "      <th>1462737</th>\n",
       "      <th>...</th>\n",
       "      <th>yebfly1</th>\n",
       "      <th>yebsee1</th>\n",
       "      <th>yecspi2</th>\n",
       "      <th>yectyr1</th>\n",
       "      <th>yehbla2</th>\n",
       "      <th>yehcar1</th>\n",
       "      <th>yelori1</th>\n",
       "      <th>yeofly1</th>\n",
       "      <th>yercac1</th>\n",
       "      <th>ywcpar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 207 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [row_id, 1139490, 1192948, 1194042, 126247, 1346504, 134933, 135045, 1462711, 1462737, 1564122, 21038, 21116, 21211, 22333, 22973, 22976, 24272, 24292, 24322, 41663, 41778, 41970, 42007, 42087, 42113, 46010, 47067, 476537, 476538, 48124, 50186, 517119, 523060, 528041, 52884, 548639, 555086, 555142, 566513, 64862, 65336, 65344, 65349, 65373, 65419, 65448, 65547, 65962, 66016, 66531, 66578, 66893, 67082, 67252, 714022, 715170, 787625, 81930, 868458, 963335, amakin1, amekes, ampkin1, anhing, babwar, bafibi1, banana, baymac, bbwduc, bicwre1, bkcdon, bkmtou1, blbgra1, blbwre1, blcant4, blchaw1, blcjay1, blctit1, blhpar1, blkvul, bobfly1, bobher1, brtpar1, bubcur1, bubwre1, bucmot3, bugtan, butsal1, cargra1, cattyr, chbant1, chfmac1, cinbec1, cocher1, cocwoo1, colara1, colcha1, compau, compot1, ...]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 207 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = pd.DataFrame(final_pred_dataframe_data, columns = ['row_id'] + class_labels) \n",
    "display(results.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "90f30080",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-03T06:39:31.020896Z",
     "iopub.status.busy": "2025-06-03T06:39:31.020233Z",
     "iopub.status.idle": "2025-06-03T06:39:31.046523Z",
     "shell.execute_reply": "2025-06-03T06:39:31.045519Z"
    },
    "papermill": {
     "duration": 0.037339,
     "end_time": "2025-06-03T06:39:31.048957",
     "exception": false,
     "start_time": "2025-06-03T06:39:31.011618",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "results.to_csv(\"submission2.csv\", index=False)    \n",
    "\n",
    "sub = pd.read_csv('submission2.csv')\n",
    "cols = sub.columns[1:]\n",
    "groups = sub['row_id'].str.rsplit('_', n=1).str[0]\n",
    "groups = groups.values\n",
    "for group in np.unique(groups):\n",
    "    sub_group = sub[group == groups]\n",
    "    predictions = sub_group[cols].values\n",
    "    new_predictions = predictions.copy()\n",
    "    for i in range(1, predictions.shape[0]-1):\n",
    "        new_predictions[i] = (predictions[i-1] * 0.1) + (predictions[i] * 0.8) + (predictions[i+1] * 0.1)\n",
    "    new_predictions[0] = (predictions[0] * 0.9) + (predictions[1] * 0.1)\n",
    "    new_predictions[-1] = (predictions[-1] * 0.9) + (predictions[-2] * 0.1)\n",
    "    sub_group[cols] = new_predictions\n",
    "    sub[group == groups] = sub_group\n",
    "sub.to_csv(\"submission2.csv\", index=False)\n",
    "\n",
    "\n",
    "if debug:\n",
    "    display(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f94fbd",
   "metadata": {
    "papermill": {
     "duration": 0.007546,
     "end_time": "2025-06-03T06:39:31.069579",
     "exception": false,
     "start_time": "2025-06-03T06:39:31.062033",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# fourth convnestv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9439a5be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-03T06:39:31.087185Z",
     "iopub.status.busy": "2025-06-03T06:39:31.086730Z",
     "iopub.status.idle": "2025-06-03T06:39:32.804671Z",
     "shell.execute_reply": "2025-06-03T06:39:32.803760Z"
    },
    "papermill": {
     "duration": 1.728957,
     "end_time": "2025-06-03T06:39:32.806345",
     "exception": false,
     "start_time": "2025-06-03T06:39:31.077388",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Loading taxonomy data...\n",
      "Number of classes: 206\n",
      "main:Starting BirdCLEF-2025 inference...\n",
      "TTA enabled: False (variations: 0)\n",
      "OpenVINO 模型 'convnextv2_nano.fcmae_ft_in22k_in1k' 已编译到设备: CPU\n",
      "OpenVINO 模型 'convnextv2_nano.fcmae_ft_in22k_in1k' 已编译到设备: CPU\n",
      "Model usage: Ensemble of 2 models\n",
      "Found 0 test soundscapes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93f722b535ca463a8190732c93535689",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating submission dataframe...\n",
      "Inference completed in 0.03 minutes\n"
     ]
    }
   ],
   "source": [
    "def apply_power_to_low_ranked_cols(\n",
    "    p: np.ndarray,\n",
    "    top_k: int = 30,\n",
    "    exponent: Union[int, float] = 2,\n",
    "    inplace: bool = True\n",
    ") -> np.ndarray:\n",
    "    if not inplace:\n",
    "        p = p.copy()\n",
    "    tail_cols = np.argsort(-p.max(axis=0))[top_k:]\n",
    "    p[:, tail_cols] = p[:, tail_cols] ** exponent\n",
    "    return p\n",
    "    \n",
    "\n",
    "class CFG: \n",
    "    test_soundscapes = '/kaggle/input/birdclef-2025/test_soundscapes'\n",
    "    submission_csv = '/kaggle/input/birdclef-2025/sample_submission.csv'\n",
    "    taxonomy_csv = '/kaggle/input/birdclef-2025/taxonomy.csv'\n",
    "    \n",
    "    # ------------------------------------------- #\n",
    "    # [IMPORTANT]\n",
    "    # * Melspectrogram & Audio Params\n",
    "    # ------------------------------------------- #\n",
    "    FS = 32000  \n",
    "    WINDOW_SIZE = 5\n",
    "    N_FFT = 2048\n",
    "    HOP_LENGTH = 512\n",
    "    N_MELS = 512\n",
    "    FMIN = 20\n",
    "    FMAX = 16000\n",
    "    TARGET_SHAPE = (256, 256)\n",
    "\n",
    "    # ------------------------------------------- #\n",
    "    # * Model def\n",
    "    # ------------------------------------------- #\n",
    "    use_specific_folds = True\n",
    "    folds = [0,1]\n",
    "    in_channels = 1\n",
    "    device = 'cpu'  \n",
    "    \n",
    "    # Inference parameters\n",
    "    batch_size = 16\n",
    "    use_tta = False  \n",
    "    tta_count = 3\n",
    "    threshold = 0.5\n",
    "\n",
    "    # util\n",
    "    debug = False\n",
    "    debug_count = 3\n",
    "\n",
    "cfg = CFG()\n",
    "\n",
    "print(f\"Using device: {cfg.device}\")\n",
    "print(f\"Loading taxonomy data...\")\n",
    "taxonomy_df = pd.read_csv(cfg.taxonomy_csv)\n",
    "species_ids = taxonomy_df['primary_label'].tolist()\n",
    "num_classes = len(species_ids)\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "\n",
    "\n",
    "\n",
    "class GeM(nn.Module):\n",
    "    def __init__(self, p=3, eps=1e-6):\n",
    "        super(GeM, self).__init__()\n",
    "        self.p = nn.Parameter(torch.ones(1)*p)\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.gem(x, p=self.p, eps=self.eps)\n",
    "\n",
    "    def gem(self, x, p=3, eps=1e-6):\n",
    "        return F.avg_pool2d(x.clamp(min=eps).pow(p), (x.size(-2), x.size(-1))).pow(1./p)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + \\\n",
    "                '(' + 'p=' + '{:.4f}'.format(self.p.data.tolist()[0]) + \\\n",
    "                ', ' + 'eps=' + str(self.eps) + ')'\n",
    "class BirdCLEFModel(nn.Module):\n",
    "    def __init__(self, cfg, num_classes):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        \n",
    "        self.backbone = timm.create_model(\n",
    "            cfg.model_name,\n",
    "            pretrained=False,  \n",
    "            in_chans=cfg.in_channels,\n",
    "            drop_rate=0.0,    \n",
    "            drop_path_rate=0.0\n",
    "        )\n",
    "        \n",
    "        if 'efficientnet' in cfg.model_name:\n",
    "            backbone_out = self.backbone.classifier.in_features\n",
    "            self.backbone.classifier = nn.Identity()\n",
    "        elif 'resnet' in cfg.model_name:\n",
    "            backbone_out = self.backbone.fc.in_features\n",
    "            self.backbone.fc = nn.Identity()\n",
    "        else:\n",
    "            backbone_out = self.backbone.get_classifier().in_features\n",
    "            self.backbone.reset_classifier(0, '')\n",
    "        \n",
    "        self.pooling = nn.AdaptiveAvgPool2d(1)\n",
    "        self.feat_dim = backbone_out\n",
    "        self.classifier = nn.Linear(backbone_out, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        \n",
    "        if isinstance(features, dict):\n",
    "            features = features['features']\n",
    "            \n",
    "        if len(features.shape) == 4:\n",
    "            features = self.pooling(features)\n",
    "            features = features.view(features.size(0), -1)\n",
    "        \n",
    "        logits = self.classifier(features)\n",
    "        return logits\n",
    "def audio2melspec(audio_data, cfg):\n",
    "    \"\"\"Convert audio data to mel spectrogram\"\"\"\n",
    "    if np.isnan(audio_data).any():\n",
    "        mean_signal = np.nanmean(audio_data)\n",
    "        audio_data = np.nan_to_num(audio_data, nan=mean_signal)\n",
    "\n",
    "    mel_spec = librosa.feature.melspectrogram(\n",
    "        y=audio_data,\n",
    "        sr=cfg.FS,\n",
    "        n_fft=cfg.N_FFT,\n",
    "        hop_length=cfg.HOP_LENGTH,\n",
    "        n_mels=cfg.N_MELS,\n",
    "        fmin=cfg.FMIN,\n",
    "        fmax=cfg.FMAX,\n",
    "        power=2.0,\n",
    "        pad_mode=\"reflect\",\n",
    "        norm='slaney',\n",
    "        htk=True,\n",
    "        center=True,\n",
    "    )\n",
    "\n",
    "    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "    mel_spec_norm = (mel_spec_db - mel_spec_db.min()) / (mel_spec_db.max() - mel_spec_db.min() + 1e-8)\n",
    "    \n",
    "    return mel_spec_norm\n",
    "\n",
    "def process_audio_segment(audio_data, cfg):\n",
    "    \"\"\"Process audio segment to get mel spectrogram\"\"\"\n",
    "    if len(audio_data) < cfg.FS * cfg.WINDOW_SIZE:\n",
    "        audio_data = np.pad(audio_data, \n",
    "                          (0, cfg.FS * cfg.WINDOW_SIZE - len(audio_data)), \n",
    "                          mode='constant')\n",
    "    \n",
    "    mel_spec = audio2melspec(audio_data, cfg)\n",
    "    \n",
    "    # Resize if needed\n",
    "    if mel_spec.shape != cfg.TARGET_SHAPE:\n",
    "        mel_spec = cv2.resize(mel_spec, cfg.TARGET_SHAPE, interpolation=cv2.INTER_LINEAR)\n",
    "        \n",
    "    return mel_spec\n",
    "    \n",
    "\n",
    "def predict_on_spectrogram(audio_path, models, cfg, species_ids):\n",
    "    \"\"\"Process a single audio file and predict species presence for each 5-second segment\"\"\"\n",
    "    predictions = []\n",
    "    row_ids = []\n",
    "    soundscape_id = Path(audio_path).stem\n",
    "    \n",
    "    try:\n",
    "        print(f\"Processing {soundscape_id}\")\n",
    "        audio_data, _ = librosa.load(audio_path, sr=cfg.FS)\n",
    "        \n",
    "        total_segments = int(len(audio_data) / (cfg.FS * cfg.WINDOW_SIZE))\n",
    "        \n",
    "        for segment_idx in range(total_segments):\n",
    "            start_sample = segment_idx * cfg.FS * cfg.WINDOW_SIZE\n",
    "            end_sample = start_sample + cfg.FS * cfg.WINDOW_SIZE\n",
    "            segment_audio = audio_data[start_sample:end_sample]\n",
    "#            \n",
    "            end_time_sec = (segment_idx + 1) * cfg.WINDOW_SIZE\n",
    "            row_id = f\"{soundscape_id}_{end_time_sec}\"\n",
    "            row_ids.append(row_id)\n",
    "\n",
    "  \n",
    "   \n",
    "            mel_spec = process_audio_segment(segment_audio, cfg)\n",
    "            # print(\"000000000000000000\")\n",
    "            # print(mel_spec)\n",
    "            # print(\"00000000000000\")\n",
    "            ov_input = mel_spec[np.newaxis, np.newaxis, :, :].astype(np.float32)\n",
    "            # print(ov_input.shape)\n",
    "            if len(models) == 1:\n",
    "                outputs = models[0](ov_input)['output_0']\n",
    "                final_preds = sigmoid(outputs).squeeze()\n",
    "            else:\n",
    "                segment_preds = []\n",
    "                for model in models:\n",
    "                    outputs = model(ov_input)['output_0']\n",
    "                    probs = sigmoid(outputs).squeeze()\n",
    "                    segment_preds.append(probs)\n",
    "    \n",
    "                final_preds = np.mean(segment_preds, axis=0)\n",
    "                \n",
    "            predictions.append(final_preds)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {audio_path}: {e}\")\n",
    "\n",
    "    predictions = np.vstack(predictions) \n",
    "    predictions = apply_power_to_low_ranked_cols(predictions, top_k=30,exponent=2)\n",
    "    # print(predictions.shape)\n",
    "    return row_ids, predictions\n",
    "\n",
    "\n",
    "def apply_tta(spec, tta_idx):\n",
    "    \"\"\"Apply test-time augmentation\"\"\"\n",
    "    if tta_idx == 0:\n",
    "        # Original spectrogram\n",
    "        return spec\n",
    "    elif tta_idx == 1:\n",
    "        # Time shift (horizontal flip)\n",
    "        return np.flip(spec, axis=1)\n",
    "    elif tta_idx == 2:\n",
    "        # Frequency shift (vertical flip)\n",
    "        return np.flip(spec, axis=0)\n",
    "    else:\n",
    "        return spec\n",
    "\n",
    "def run_inference(cfg, models, species_ids):\n",
    "    \"\"\"Run inference on all test soundscapes\"\"\"\n",
    "    test_files = list(Path(cfg.test_soundscapes).glob('*.ogg'))\n",
    "    \n",
    "    if cfg.debug:\n",
    "        print(f\"Debug mode enabled, using only {cfg.debug_count} files\")\n",
    "        test_files = test_files[:cfg.debug_count]\n",
    "    \n",
    "    print(f\"Found {len(test_files)} test soundscapes\")\n",
    "\n",
    "    all_row_ids = []\n",
    "    all_predictions = []\n",
    "\n",
    "    for audio_path in tqdm(test_files):\n",
    "        row_ids, predictions = predict_on_spectrogram(str(audio_path), models, cfg, species_ids)\n",
    "        all_row_ids.extend(row_ids)\n",
    "        all_predictions.extend(predictions)\n",
    "    \n",
    "    return all_row_ids, all_predictions\n",
    "\n",
    "def create_submission(row_ids, predictions, species_ids, cfg):\n",
    "    \"\"\"Create submission dataframe\"\"\"\n",
    "    print(\"Creating submission dataframe...\")\n",
    "\n",
    "    submission_dict = {'row_id': row_ids}\n",
    "    \n",
    "    for i, species in enumerate(species_ids):\n",
    "        submission_dict[species] = [pred[i] for pred in predictions]\n",
    "\n",
    "    submission_df = pd.DataFrame(submission_dict)\n",
    "\n",
    "    submission_df.set_index('row_id', inplace=True)\n",
    "\n",
    "    sample_sub = pd.read_csv(cfg.submission_csv, index_col='row_id')\n",
    "\n",
    "    missing_cols = set(sample_sub.columns) - set(submission_df.columns)\n",
    "    if missing_cols:\n",
    "        print(f\"Warning: Missing {len(missing_cols)} species columns in submission\")\n",
    "        for col in missing_cols:\n",
    "            submission_df[col] = 0.0\n",
    "\n",
    "    submission_df = submission_df[sample_sub.columns]\n",
    "\n",
    "    submission_df = submission_df.reset_index()\n",
    "    \n",
    "    return submission_df\n",
    "def main():\n",
    "    start_time = time.time()\n",
    "    print(\"main:Starting BirdCLEF-2025 inference...\")\n",
    "    print(f\"TTA enabled: {cfg.use_tta} (variations: {cfg.tta_count if cfg.use_tta else 0})\")\n",
    "\n",
    "    models  =  load_openvino_model(xml_paths=model_dicts['convnextv2_nano.fcmae_ft_in22k_in1k'])\n",
    "\n",
    "    \n",
    "    if not models:\n",
    "        print(\"main:No models found! Please check model paths.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Model usage: {'Single model' if len(models) == 1 else f'Ensemble of {len(models)} models'}\")\n",
    "\n",
    "    row_ids, predictions = run_inference(cfg, models, species_ids)\n",
    "\n",
    "    submission_df = create_submission(row_ids, predictions, species_ids, cfg)\n",
    "\n",
    "    submission_path = 'submission3.csv'\n",
    "    submission_df.to_csv(submission_path, index=False)\n",
    "    # 时间平滑\n",
    "\n",
    "    sub = pd.read_csv('submission3.csv')\n",
    "    cols = sub.columns[1:]\n",
    "    groups = sub['row_id'].str.rsplit('_', n=1).str[0]\n",
    "    groups = groups.values\n",
    "    for group in np.unique(groups):\n",
    "        sub_group = sub[group == groups]\n",
    "        predictions = sub_group[cols].values\n",
    "        new_predictions = predictions.copy()\n",
    "        for i in range(1, predictions.shape[0]-1):\n",
    "            new_predictions[i] = (predictions[i-1] * 0.1) + (predictions[i] * 0.8) + (predictions[i+1] * 0.1)\n",
    "        new_predictions[0] = (predictions[0] * 0.8) + (predictions[1] * 0.2)\n",
    "        new_predictions[-1] = (predictions[-1] * 0.8) + (predictions[-2] * 0.2)\n",
    "        sub_group[cols] = new_predictions\n",
    "        sub[group == groups] = sub_group\n",
    "    sub.to_csv(\"submission3.csv\", index=False)\n",
    "        \n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"Inference completed in {(end_time - start_time)/60:.2f} minutes\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ed1235",
   "metadata": {
    "papermill": {
     "duration": 0.007982,
     "end_time": "2025-06-03T06:39:32.823172",
     "exception": false,
     "start_time": "2025-06-03T06:39:32.815190",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 融合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f90b8182",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-03T06:39:32.841506Z",
     "iopub.status.busy": "2025-06-03T06:39:32.840606Z",
     "iopub.status.idle": "2025-06-03T06:39:37.046517Z",
     "shell.execute_reply": "2025-06-03T06:39:37.045431Z"
    },
    "papermill": {
     "duration": 4.217157,
     "end_time": "2025-06-03T06:39:37.048449",
     "exception": false,
     "start_time": "2025-06-03T06:39:32.831292",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------- #\n",
    "# [IMPORTANT]\n",
    "# * Blending Weight\n",
    "# ------------------------------------------- #\n",
    "sub_w=[0.2,0.4,0.3,0.1]\n",
    "list_TARGETs = sorted(os.listdir('/kaggle/input/birdclef-2025/train_audio/'))\n",
    "list_targets_0 = [f'{TARGET} 0' for TARGET in list_TARGETs]\n",
    "list_targets_1 = [f'{TARGET} 1' for TARGET in list_TARGETs]\n",
    "list_targets_2 = [f'{TARGET} 2' for TARGET in list_TARGETs]\n",
    "list_targets_3 = [f'{TARGET} 3' for TARGET in list_TARGETs]\n",
    "\n",
    "\n",
    "\n",
    "df0 = pd.read_csv(\"/kaggle/working/submission0.csv\")\n",
    "df1 = pd.read_csv(\"/kaggle/working/submission1.csv\")\n",
    "df2 = pd.read_csv(\"/kaggle/working/submission2.csv\")\n",
    "df3 = pd.read_csv(\"/kaggle/working/submission3.csv\")\n",
    "\n",
    "\n",
    "\n",
    "df0 = df0.rename(columns={TARGET : f'{TARGET} 0' for TARGET in list_TARGETs})\n",
    "df1 = df1.rename(columns={TARGET : f'{TARGET} 1' for TARGET in list_TARGETs})\n",
    "df2 = df2.rename(columns={TARGET : f'{TARGET} 2' for TARGET in list_TARGETs})\n",
    "df3 = df3.rename(columns={TARGET : f'{TARGET} 3' for TARGET in list_TARGETs})\n",
    "\n",
    "\n",
    "\n",
    "dfs_merged1 = pd.merge(df0, df1, on='row_id', how='inner') \n",
    "dfs_merged2 = pd.merge(df3, df2, on='row_id', how='inner')\n",
    "dfs = pd.merge(dfs_merged1, dfs_merged2, on='row_id', how='inner')\n",
    "\n",
    "\n",
    "for i in range(len(list_TARGETs)):\n",
    "    dfs[list_TARGETs[i]] = dfs[list_targets_0[i]]*sub_w[0] +  sub_w[1]*dfs[list_targets_1[i]] + sub_w[2]*dfs[list_targets_2[i]]+ sub_w[3]*dfs[list_targets_3[i]]\n",
    "             \n",
    "for col0,col1 in zip(list_targets_0, list_targets_1):\n",
    "    del dfs[col0]\n",
    "    del dfs[col1]\n",
    "for col2,col3 in zip(list_targets_2, list_targets_3):\n",
    "    del dfs[col2]\n",
    "    del dfs[col3]\n",
    "\n",
    "    \n",
    "dfs.to_csv(\"submission.csv\", index=False)\n",
    "print(\"finish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f01af60",
   "metadata": {
    "papermill": {
     "duration": 0.008423,
     "end_time": "2025-06-03T06:39:37.065405",
     "exception": false,
     "start_time": "2025-06-03T06:39:37.056982",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d6d8e3",
   "metadata": {
    "papermill": {
     "duration": 0.008533,
     "end_time": "2025-06-03T06:39:37.082553",
     "exception": false,
     "start_time": "2025-06-03T06:39:37.074020",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 11361821,
     "sourceId": 91844,
     "sourceType": "competition"
    },
    {
     "sourceId": 235777618,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 242312421,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 242383854,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 242385124,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 242385199,
     "sourceType": "kernelVersion"
    }
   ],
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 63.929916,
   "end_time": "2025-06-03T06:39:39.811273",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-06-03T06:38:35.881357",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "00774f5cfb7d4a2cba24fba5d9d37707": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "0433fedb5f5f4c5d8a475991b595afc0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": "20px"
      }
     },
     "0f277362fdbe401c9ec7d17ff878cfab": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1f1d0b1f569c40e5abb048211e0ac93a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2230281032b84f648c520df58d186d62": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_add8b28849d649a083f8f73f0187f018",
       "placeholder": "​",
       "style": "IPY_MODEL_e3debbb5278843c1982e44628baa3f94",
       "tabbable": null,
       "tooltip": null,
       "value": ""
      }
     },
     "34e0ad01b71849b1aa6297ce22ea4a87": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_6522eafe6aa04beea1631b7ef833d95e",
       "placeholder": "​",
       "style": "IPY_MODEL_ccc9016556dc4480a291bf8f84153838",
       "tabbable": null,
       "tooltip": null,
       "value": " 0/0 [00:00&lt;?, ?it/s]"
      }
     },
     "45e0d0cf42b04292ada50cb11a25c58b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5924b37aa6c447d5a042d2f105351c65": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_aab7eec9956c40b8aacb3d9531caf77c",
        "IPY_MODEL_f2837ec6f8b2408483833e7683fa7f49",
        "IPY_MODEL_788180134bae4841857bbfc8e2c85c8d"
       ],
       "layout": "IPY_MODEL_45e0d0cf42b04292ada50cb11a25c58b",
       "tabbable": null,
       "tooltip": null
      }
     },
     "6522eafe6aa04beea1631b7ef833d95e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6b39c330d4c348c4a3f615b24b74774c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "6f1d106f045e48cfbf1c945978ef6d69": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "788180134bae4841857bbfc8e2c85c8d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_0f277362fdbe401c9ec7d17ff878cfab",
       "placeholder": "​",
       "style": "IPY_MODEL_d39693ccd817410d930d7ad7829f1ec0",
       "tabbable": null,
       "tooltip": null,
       "value": " 0/0 [00:00&lt;?, ?it/s]"
      }
     },
     "93f722b535ca463a8190732c93535689": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_2230281032b84f648c520df58d186d62",
        "IPY_MODEL_e0cdc65de9df45be90c2ad62618fda3c",
        "IPY_MODEL_34e0ad01b71849b1aa6297ce22ea4a87"
       ],
       "layout": "IPY_MODEL_f142294fd7524f6abc5ca8a80b1095a8",
       "tabbable": null,
       "tooltip": null
      }
     },
     "aab7eec9956c40b8aacb3d9531caf77c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_1f1d0b1f569c40e5abb048211e0ac93a",
       "placeholder": "​",
       "style": "IPY_MODEL_6b39c330d4c348c4a3f615b24b74774c",
       "tabbable": null,
       "tooltip": null,
       "value": ""
      }
     },
     "add8b28849d649a083f8f73f0187f018": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ccc9016556dc4480a291bf8f84153838": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "d39693ccd817410d930d7ad7829f1ec0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "e0cdc65de9df45be90c2ad62618fda3c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_0433fedb5f5f4c5d8a475991b595afc0",
       "max": 1.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_6f1d106f045e48cfbf1c945978ef6d69",
       "tabbable": null,
       "tooltip": null,
       "value": 0.0
      }
     },
     "e3debbb5278843c1982e44628baa3f94": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "e75742a2b54a423aaf6dcf0e768b7ed5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": "20px"
      }
     },
     "f142294fd7524f6abc5ca8a80b1095a8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f2837ec6f8b2408483833e7683fa7f49": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_e75742a2b54a423aaf6dcf0e768b7ed5",
       "max": 1.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_00774f5cfb7d4a2cba24fba5d9d37707",
       "tabbable": null,
       "tooltip": null,
       "value": 0.0
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
