{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9336aced",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-04T10:02:07.642758Z",
     "iopub.status.busy": "2025-06-04T10:02:07.642486Z",
     "iopub.status.idle": "2025-06-04T10:02:23.194956Z",
     "shell.execute_reply": "2025-06-04T10:02:23.194173Z"
    },
    "papermill": {
     "duration": 15.560705,
     "end_time": "2025-06-04T10:02:23.196257",
     "exception": false,
     "start_time": "2025-06-04T10:02:07.635552",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import warnings\n",
    "import logging\n",
    "import time\n",
    "import math\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "from soundfile import SoundFile \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import timm\n",
    "from tqdm.auto import tqdm\n",
    "from glob import glob\n",
    "import torchaudio\n",
    "import random\n",
    "import itertools\n",
    "from typing import Union\n",
    "\n",
    "import pickle\n",
    "import torchaudio\n",
    "import torchaudio.transforms as AT\n",
    "from contextlib import contextmanager\n",
    "import concurrent.futures\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3093dc3a",
   "metadata": {
    "papermill": {
     "duration": 0.004183,
     "end_time": "2025-06-04T10:02:23.205571",
     "exception": false,
     "start_time": "2025-06-04T10:02:23.201388",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# first convnestv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49205209",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-04T10:02:23.215806Z",
     "iopub.status.busy": "2025-06-04T10:02:23.215365Z",
     "iopub.status.idle": "2025-06-04T10:02:24.849221Z",
     "shell.execute_reply": "2025-06-04T10:02:24.848469Z"
    },
    "papermill": {
     "duration": 1.64089,
     "end_time": "2025-06-04T10:02:24.850820",
     "exception": false,
     "start_time": "2025-06-04T10:02:23.209930",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Loading taxonomy data...\n",
      "Number of classes: 206\n",
      "main:Starting BirdCLEF-2025 inference...\n",
      "TTA enabled: False (variations: 0)\n",
      "load_models:Loading model:efficientnet_b0 :/kaggle/input/effnetb0seed2024/model_fold1.pth\n",
      "load_models:Loading model:efficientnet_b0 :/kaggle/input/effnetb0seed2025/effnetb0focalseed2025/model_fold0.pth\n",
      "一共有2个模型。\n",
      "Model usage: Ensemble of 2 models\n",
      "run_inference:Found 0 test soundscapes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2885b81b14fa431abb17727466906319",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create_submission:Creating submission dataframe...\n",
      "Inference completed in 0.03 minutes\n"
     ]
    }
   ],
   "source": [
    "class CFG:\n",
    " \n",
    "    test_soundscapes = '/kaggle/input/birdclef-2025/test_soundscapes'\n",
    "    submission_csv = '/kaggle/input/birdclef-2025/sample_submission.csv'\n",
    "    taxonomy_csv = '/kaggle/input/birdclef-2025/taxonomy.csv'\n",
    "\n",
    "    model_dicts = {\n",
    "    \"efficientnet_b0\":[\n",
    "                       '/kaggle/input/effnetb0seed2024/model_fold1.pth' ,\n",
    "        \"/kaggle/input/effnetb0seed2025/effnetb0focalseed2025/model_fold0.pth\",\n",
    "                     ]\n",
    "\n",
    "    # \"tf_mobilenetv3_large_100.in1k\":[\"/kaggle/input/mobile-bcefocal/model_fold0.pth\",\n",
    "    #                                  '/kaggle/input/mobile-bcefocal/model_fold1.pth',\n",
    "    #                                  \"/kaggle/input/mobile-bcefocal/model_fold2.pth\"\n",
    "    #                                 ]\n",
    "        \n",
    "    # \"regnety_008\":\n",
    "        # \"/kaggle/input/bird25-all-train-effnet-v2-s-bcefocalloss/model_fold1.pth\"    \n",
    "}\n",
    "\n",
    "\n",
    "    # Audio parameters\n",
    "    FS = 32000  \n",
    "    WINDOW_SIZE = 5  \n",
    "    \n",
    "    # Mel spectrogram parameters\n",
    "    N_FFT = 1024\n",
    "    HOP_LENGTH = 512\n",
    "    N_MELS = 128\n",
    "    FMIN = 50\n",
    "    FMAX = 16000\n",
    "    TARGET_SHAPE = (256, 256)\n",
    "\n",
    "  \n",
    "    in_channels = 1\n",
    "    device = 'cpu'  \n",
    "    \n",
    "    # Inference parameters\n",
    "    batch_size = 32\n",
    "    use_tta = False\n",
    "    # 是否使用测试时增强（Test Time Augmentation，TTA）。 TTA 是一种通过对测试样本进行增强来提高模型性能的技术。\n",
    "    tta_count = 2   \n",
    "    # TTA 的次数。 如果 use_tta 为 True，则指定对每个测试样本进行多少次增强。\n",
    "    threshold = 0.5\n",
    "\n",
    "    debug =  False\n",
    "    # True  False\n",
    "    debug_count = 3\n",
    "\n",
    "cfg = CFG()\n",
    "print(f\"Using device: {cfg.device}\")\n",
    "print(f\"Loading taxonomy data...\")\n",
    "taxonomy_df = pd.read_csv(cfg.taxonomy_csv)\n",
    "species_ids = taxonomy_df['primary_label'].tolist()\n",
    "num_classes = len(species_ids)\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "\n",
    "\n",
    "class BirdCLEFModel(nn.Module):\n",
    "    def __init__(self,cfg,model_name,num_classes):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        \n",
    "        self.backbone = timm.create_model(\n",
    "            model_name,\n",
    "            pretrained=False,  \n",
    "            in_chans=cfg.in_channels,\n",
    "            drop_rate=0.0,       # 训练时就不需要使用暂退法来类似进行正则化了\n",
    "            drop_path_rate=0.0\n",
    "        )\n",
    "        \n",
    "        if 'efficientnet' in model_name:\n",
    "            backbone_out = self.backbone.classifier.in_features\n",
    "            self.backbone.classifier = nn.Identity()\n",
    "        elif 'resnet' in model_name:\n",
    "            backbone_out = self.backbone.fc.in_features\n",
    "            self.backbone.fc = nn.Identity()\n",
    "        else:\n",
    "            backbone_out = self.backbone.get_classifier().in_features\n",
    "            self.backbone.reset_classifier(0, '')\n",
    "        \n",
    "        self.pooling = nn.AdaptiveAvgPool2d(1)\n",
    "        self.feat_dim = backbone_out\n",
    "        self.classifier = nn.Linear(backbone_out, num_classes)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        \n",
    "        if isinstance(features, dict):\n",
    "            features = features['features']\n",
    "            \n",
    "        if len(features.shape) == 4:\n",
    "            features = self.pooling(features)\n",
    "            features = features.view(features.size(0), -1)\n",
    "        \n",
    "        logits = self.classifier(features)\n",
    "        \n",
    "        return logits\n",
    "def audio2melspec(audio_data, cfg):\n",
    "    \"\"\"Convert audio data to mel spectrogram\"\"\"\n",
    "    if np.isnan(audio_data).any():\n",
    "        mean_signal = np.nanmean(audio_data)\n",
    "        audio_data = np.nan_to_num(audio_data, nan=mean_signal)\n",
    "\n",
    "    mel_spec = librosa.feature.melspectrogram(\n",
    "        y=audio_data,\n",
    "        sr=cfg.FS,\n",
    "        n_fft=cfg.N_FFT,\n",
    "        hop_length=cfg.HOP_LENGTH,\n",
    "        n_mels=cfg.N_MELS,\n",
    "        fmin=cfg.FMIN,\n",
    "        fmax=cfg.FMAX,\n",
    "        power=2.0,\n",
    "        pad_mode=\"reflect\",\n",
    "        norm='slaney',\n",
    "        htk=True,\n",
    "        center=True,\n",
    "    )\n",
    "\n",
    "    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "    mel_spec_norm = (mel_spec_db - mel_spec_db.min()) / (mel_spec_db.max() - mel_spec_db.min() + 1e-8)\n",
    "    \n",
    "    return mel_spec_norm\n",
    "\n",
    "def process_audio_segment(audio_data, cfg):\n",
    "    \"\"\"Process audio segment to get mel spectrogram\"\"\"\n",
    "    if len(audio_data) < cfg.FS * cfg.WINDOW_SIZE:\n",
    "        audio_data = np.pad(audio_data, \n",
    "                          (0, cfg.FS * cfg.WINDOW_SIZE - len(audio_data)), \n",
    "                          mode='constant')\n",
    "    \n",
    "    mel_spec = audio2melspec(audio_data, cfg)\n",
    "    \n",
    "    # Resize if needed\n",
    "    if mel_spec.shape != cfg.TARGET_SHAPE:\n",
    "        mel_spec = cv2.resize(mel_spec, cfg.TARGET_SHAPE, interpolation=cv2.INTER_LINEAR)\n",
    "        \n",
    "    return mel_spec.astype(np.float32)\n",
    "def load_models(cfg, num_classes):\n",
    "    \"\"\"\n",
    "    Load all found model files and prepare them for ensemble\n",
    "    \"\"\"    \n",
    "    models = []\n",
    "    for model_name,model_paths in cfg.model_dicts.items():\n",
    "        try:\n",
    "            for model_path in model_paths:\n",
    "                print(f\"load_models:Loading model:{model_name} :{model_path}\")\n",
    "                checkpoint = torch.load(model_path, map_location=torch.device(cfg.device),weights_only=False)\n",
    "                # 将权重加载出来然后转移到cpu上。\n",
    "                model = BirdCLEFModel(cfg,model_name,num_classes)\n",
    "                model.load_state_dict(checkpoint['model_state_dict'])   # \n",
    "                model = model.to(cfg.device)\n",
    "                model.eval()\n",
    "                models.append(model)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"load_models:Error loading model {model_path}: {e}\")\n",
    "    print(f\"一共有{len(models)}个模型。\")\n",
    "    return models\n",
    "\n",
    "def predict_on_spectrogram(audio_path, models, cfg, species_ids):\n",
    "    \"\"\"Process a single audio file and predict species presence for each 5-second segment\"\"\"\n",
    "    predictions = []\n",
    "    row_ids = []\n",
    "    soundscape_id = Path(audio_path).stem   #提取音频文件的名称\n",
    "    \n",
    "    try:\n",
    "        print(f\"predict_on_spectrogram:Processing {soundscape_id}\")\n",
    "        audio_data, _ = librosa.load(audio_path, sr=cfg.FS)\n",
    "        \n",
    "        total_segments = int(len(audio_data) / (cfg.FS * cfg.WINDOW_SIZE))\n",
    "        #将   soundscape文件中所有的ogg音频文件截取成  5s/5s 的片段。\n",
    "        for segment_idx in range(total_segments):\n",
    "            start_sample = segment_idx * cfg.FS * cfg.WINDOW_SIZE\n",
    "            end_sample = start_sample + cfg.FS * cfg.WINDOW_SIZE\n",
    "            segment_audio = audio_data[start_sample:end_sample]\n",
    "            \n",
    "            end_time_sec = (segment_idx + 1) * cfg.WINDOW_SIZE\n",
    "            row_id = f\"{soundscape_id}_{end_time_sec}\"\n",
    "            row_ids.append(row_id)\n",
    "\n",
    "            if cfg.use_tta:\n",
    "                all_preds = []\n",
    "                \n",
    "                for tta_idx in range(cfg.tta_count):\n",
    "                    mel_spec = process_audio_segment(segment_audio, cfg)\n",
    "        \n",
    "                    mel_spec = apply_tta(mel_spec, tta_idx)\n",
    "                    mel_spec = mel_spec.copy()\n",
    "\n",
    "                    mel_spec = torch.tensor(mel_spec, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
    "                    mel_spec = mel_spec.to(cfg.device)\n",
    "\n",
    "                    if len(models) == 1:   # 单个fold的情况\n",
    "                        with torch.no_grad():\n",
    "                            outputs = models[0](mel_spec)\n",
    "                            probs = torch.sigmoid(outputs).cpu().numpy().squeeze()\n",
    "                            all_preds.append(probs)\n",
    "                    else:\n",
    "                        segment_preds = []\n",
    "                        for model in models:\n",
    "                            with torch.no_grad():\n",
    "                                outputs = model(mel_spec)\n",
    "                                probs = torch.sigmoid(outputs).cpu().numpy().squeeze()\n",
    "                                segment_preds.append(probs)\n",
    "                        \n",
    "                        avg_preds = np.mean(segment_preds, axis=0)\n",
    "                        all_preds.append(avg_preds)   #将每个片段的在各个fold 模型上的结果加入\n",
    "\n",
    "                final_preds = np.mean(all_preds, axis=0)\n",
    "            else:\n",
    "                mel_spec = process_audio_segment(segment_audio, cfg)\n",
    "                \n",
    "                mel_spec = torch.tensor(mel_spec, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
    "                mel_spec = mel_spec.to(cfg.device)\n",
    "                \n",
    "                if len(models) == 1:\n",
    "                    with torch.no_grad():\n",
    "                        outputs = models[0](mel_spec)\n",
    "                        final_preds = torch.sigmoid(outputs).cpu().numpy().squeeze()\n",
    "                else:\n",
    "                    segment_preds = []\n",
    "                    for model in models:\n",
    "                        with torch.no_grad():\n",
    "                            outputs = model(mel_spec)\n",
    "                            probs = torch.sigmoid(outputs).cpu().numpy().squeeze()\n",
    "                            segment_preds.append(probs)\n",
    "\n",
    "                    final_preds = np.mean(segment_preds, axis=0)\n",
    "                    \n",
    "            predictions.append(final_preds)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"predict_on_spectrogram:Error processing {audio_path}: {e}\")\n",
    "        \n",
    "\n",
    "    predictions = np.vstack(predictions) \n",
    "    # predictions = apply_power_to_low_ranked_cols(predictions, top_k=30,exponent=2)\n",
    "    \n",
    "    return row_ids, predictions\n",
    "def apply_tta(spec, tta_idx):\n",
    "    \"\"\"Apply test-time augmentation\"\"\"\n",
    "    if tta_idx == 0:\n",
    "        # Original spectrogram\n",
    "        return spec\n",
    "    elif tta_idx == 1:\n",
    "        # Time shift (horizontal flip)\n",
    "        return np.flip(spec, axis=1)\n",
    "    elif tta_idx == 2:\n",
    "        # Frequency shift (vertical flip)\n",
    "        return np.flip(spec, axis=0)\n",
    "    else:\n",
    "        return spec\n",
    "\n",
    "def run_inference(cfg, models, species_ids):\n",
    "    \"\"\"Run inference on all test soundscapes\"\"\"\n",
    "    test_files = list(Path(cfg.test_soundscapes).glob('*.ogg'))\n",
    "    \n",
    "    if cfg.debug:\n",
    "        print(f\"run_inference:Debug mode enabled, using only {cfg.debug_count} files\")\n",
    "        test_files = test_files[:cfg.debug_count]\n",
    "    \n",
    "    print(f\"run_inference:Found {len(test_files)} test soundscapes\")\n",
    "\n",
    "    all_row_ids = []\n",
    "    all_predictions = []\n",
    "    \n",
    "\n",
    "    for audio_path in tqdm(test_files):\n",
    "        row_ids, predictions = predict_on_spectrogram(str(audio_path), models, cfg, species_ids)\n",
    "        all_row_ids.extend(row_ids)\n",
    "        all_predictions.extend(predictions.tolist())\n",
    "    \n",
    "    return all_row_ids, all_predictions   # 记录的id列表。还有对应的预测向量。\n",
    "\n",
    "def create_submission(row_ids, predictions, species_ids, cfg):\n",
    "    \"\"\"Create submission dataframe\"\"\"\n",
    "    print(\"create_submission:Creating submission dataframe...\")\n",
    "\n",
    "    submission_dict = {'row_id': row_ids}\n",
    "    # print(f\"create_submission:row_ids:{row_ids}\")\n",
    "    \n",
    "    for i, species in enumerate(species_ids):\n",
    "        submission_dict[species] = [pred[i] for pred in predictions]\n",
    "\n",
    "    submission_df = pd.DataFrame(submission_dict)\n",
    "\n",
    "    submission_df.set_index('row_id', inplace=True)\n",
    "\n",
    "    sample_sub = pd.read_csv(cfg.submission_csv, index_col='row_id')\n",
    "\n",
    "    missing_cols = set(sample_sub.columns) - set(submission_df.columns)\n",
    "    if missing_cols:\n",
    "        print(f\"create_submission:Warning: Missing {len(missing_cols)} species columns in submission\")\n",
    "        for col in missing_cols:\n",
    "            submission_df[col] = 0.0\n",
    "\n",
    "    submission_df = submission_df[sample_sub.columns]\n",
    "    # print(f'create_submission:使用row_id作为索引的submission_df:{submission_df}')\n",
    "\n",
    "    submission_df = submission_df.reset_index()\n",
    "    # print(f'create_submission:reset_index之后的submission_df:{submission_df}')\n",
    "    \n",
    "    return submission_df\n",
    "def main():\n",
    "    start_time = time.time()\n",
    "    print(\"main:Starting BirdCLEF-2025 inference...\")\n",
    "    print(f\"TTA enabled: {cfg.use_tta} (variations: {cfg.tta_count if cfg.use_tta else 0})\")\n",
    "\n",
    "    models = load_models(cfg, num_classes)\n",
    "    \n",
    "    if not models:\n",
    "        print(\"main:No models found! Please check model paths.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Model usage: {'Single model' if len(models) == 1 else f'Ensemble of {len(models)} models'}\")\n",
    "\n",
    "    row_ids, predictions = run_inference(cfg, models, species_ids)\n",
    "\n",
    "    submission_df = create_submission(row_ids, predictions, species_ids, cfg)\n",
    "\n",
    "    submission_path = 'submission0.csv'\n",
    "    submission_df.to_csv(submission_path, index=False)\n",
    "    # 时间平滑\n",
    "\n",
    "    sub = pd.read_csv('submission0.csv')\n",
    "    cols = sub.columns[1:]\n",
    "    groups = sub['row_id'].str.rsplit('_', n=1).str[0]\n",
    "    groups = groups.values\n",
    "    for group in np.unique(groups):\n",
    "        sub_group = sub[group == groups]\n",
    "        predictions = sub_group[cols].values\n",
    "        new_predictions = predictions.copy()\n",
    "        for i in range(1, predictions.shape[0]-1):\n",
    "            new_predictions[i] = (predictions[i-1] * 0.1) + (predictions[i] * 0.8) + (predictions[i+1] * 0.1)\n",
    "        new_predictions[0] = (predictions[0] * 0.8) + (predictions[1] * 0.2)\n",
    "        new_predictions[-1] = (predictions[-1] * 0.8) + (predictions[-2] * 0.2)\n",
    "        sub_group[cols] = new_predictions\n",
    "        sub[group == groups] = sub_group\n",
    "    sub.to_csv(\"submission0.csv\", index=False)\n",
    "        \n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"Inference completed in {(end_time - start_time)/60:.2f} minutes\")\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1f2ef0",
   "metadata": {
    "papermill": {
     "duration": 0.004291,
     "end_time": "2025-06-04T10:02:24.859963",
     "exception": false,
     "start_time": "2025-06-04T10:02:24.855672",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Second seresnext26t_32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09dec6e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-04T10:02:24.870158Z",
     "iopub.status.busy": "2025-06-04T10:02:24.869576Z",
     "iopub.status.idle": "2025-06-04T10:02:24.873964Z",
     "shell.execute_reply": "2025-06-04T10:02:24.873318Z"
    },
    "papermill": {
     "duration": 0.010626,
     "end_time": "2025-06-04T10:02:24.875048",
     "exception": false,
     "start_time": "2025-06-04T10:02:24.864422",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    \n",
    "    seed = 42\n",
    "    print_freq = 100\n",
    "    num_workers = 4\n",
    "\n",
    "    stage = 'train_bce'\n",
    "\n",
    "    train_datadir = '/kaggle/input/birdclef-2025/test_audio'\n",
    "    train_csv = '/kaggle/input/birdclef-2025/train.csv'\n",
    "    test_soundscapes = '/kaggle/input/birdclef-2025/test_soundscapes'\n",
    "    submission_csv = '/kaggle/input/birdclef-2025/sample_submission.csv'\n",
    "    taxonomy_csv = '/kaggle/input/birdclef-2025/taxonomy.csv'\n",
    "    model_files = ['/kaggle/input/bird2025-sed-ckpt/sedmodel.pth'\n",
    "                  ]\n",
    " \n",
    "    model_name = 'seresnext26t_32x4d'  \n",
    "    pretrained = False\n",
    "    in_channels = 1\n",
    "\n",
    "    \n",
    "    SR = 32000\n",
    "    target_duration = 5\n",
    "    train_duration = 10\n",
    "    \n",
    "    \n",
    "    device = 'cpu'\n",
    "\n",
    "cfg = CFG()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1fa2fbbb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-04T10:02:24.886185Z",
     "iopub.status.busy": "2025-06-04T10:02:24.885547Z",
     "iopub.status.idle": "2025-06-04T10:02:24.892149Z",
     "shell.execute_reply": "2025-06-04T10:02:24.891522Z"
    },
    "papermill": {
     "duration": 0.012865,
     "end_time": "2025-06-04T10:02:24.893147",
     "exception": false,
     "start_time": "2025-06-04T10:02:24.880282",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Loading taxonomy data...\n",
      "Number of classes: 206\n"
     ]
    }
   ],
   "source": [
    "print(f\"Using device: {cfg.device}\")\n",
    "print(f\"Loading taxonomy data...\")\n",
    "taxonomy_df = pd.read_csv(cfg.taxonomy_csv)\n",
    "species_ids = taxonomy_df['primary_label'].tolist()\n",
    "num_classes = len(species_ids)\n",
    "print(f\"Number of classes: {num_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7155704a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-04T10:02:24.903506Z",
     "iopub.status.busy": "2025-06-04T10:02:24.903229Z",
     "iopub.status.idle": "2025-06-04T10:02:24.914620Z",
     "shell.execute_reply": "2025-06-04T10:02:24.913852Z"
    },
    "papermill": {
     "duration": 0.017667,
     "end_time": "2025-06-04T10:02:24.915754",
     "exception": false,
     "start_time": "2025-06-04T10:02:24.898087",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    \"\"\"\n",
    "    Set seed for reproducibility\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(cfg.seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba3bb75e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-04T10:02:24.926081Z",
     "iopub.status.busy": "2025-06-04T10:02:24.925846Z",
     "iopub.status.idle": "2025-06-04T10:02:24.932623Z",
     "shell.execute_reply": "2025-06-04T10:02:24.931913Z"
    },
    "papermill": {
     "duration": 0.013074,
     "end_time": "2025-06-04T10:02:24.933726",
     "exception": false,
     "start_time": "2025-06-04T10:02:24.920652",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AttBlockV2(nn.Module):\n",
    "    def __init__(self, in_features: int, out_features: int, activation=\"linear\"):\n",
    "        super().__init__()\n",
    "\n",
    "        self.activation = activation\n",
    "        self.att = nn.Conv1d(\n",
    "            in_channels=in_features,\n",
    "            out_channels=out_features,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            bias=True,\n",
    "        )\n",
    "        self.cla = nn.Conv1d(\n",
    "            in_channels=in_features,\n",
    "            out_channels=out_features,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            bias=True,\n",
    "        )\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        init_layer(self.att)\n",
    "        init_layer(self.cla)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (n_samples, n_in, n_time)\n",
    "        norm_att = torch.softmax(torch.tanh(self.att(x)), dim=-1)\n",
    "        cla = self.nonlinear_transform(self.cla(x))\n",
    "        x = torch.sum(norm_att * cla, dim=2)\n",
    "        return x, norm_att, cla\n",
    "\n",
    "    def nonlinear_transform(self, x):\n",
    "        if self.activation == \"linear\":\n",
    "            return x\n",
    "        elif self.activation == \"sigmoid\":\n",
    "            return torch.sigmoid(x)\n",
    "\n",
    "\n",
    "def init_layer(layer):\n",
    "    nn.init.xavier_uniform_(layer.weight)\n",
    "\n",
    "    if hasattr(layer, \"bias\"):\n",
    "        if layer.bias is not None:\n",
    "            layer.bias.data.fill_(0.0)\n",
    "\n",
    "def init_bn(bn):\n",
    "    bn.bias.data.fill_(0.0)\n",
    "    bn.weight.data.fill_(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e074cc29",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-04T10:02:24.943969Z",
     "iopub.status.busy": "2025-06-04T10:02:24.943781Z",
     "iopub.status.idle": "2025-06-04T10:02:24.957711Z",
     "shell.execute_reply": "2025-06-04T10:02:24.957065Z"
    },
    "papermill": {
     "duration": 0.020457,
     "end_time": "2025-06-04T10:02:24.958712",
     "exception": false,
     "start_time": "2025-06-04T10:02:24.938255",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class BirdCLEFModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        \n",
    "        taxonomy_df = pd.read_csv('/kaggle/input/birdclef-2025/taxonomy.csv')\n",
    "        self.num_classes = len(taxonomy_df)\n",
    "\n",
    "        self.bn0 = nn.BatchNorm2d(cfg['n_mels'])\n",
    "        \n",
    "        self.backbone = timm.create_model(\n",
    "            cfg['model_name'],\n",
    "            pretrained=False,\n",
    "            in_chans=cfg['in_channels'],\n",
    "            drop_rate=0.0,\n",
    "            drop_path_rate=0.0,\n",
    "        )\n",
    "\n",
    "        layers = list(self.backbone.children())[:-2]\n",
    "        self.encoder = nn.Sequential(*layers)\n",
    "        \n",
    "        if \"efficientnet\" in self.cfg['model_name']:\n",
    "            backbone_out = self.backbone.classifier.in_features\n",
    "        elif \"eca\" in self.cfg['model_name']:\n",
    "            backbone_out = self.backbone.head.fc.in_features\n",
    "        elif \"res\" in self.cfg['model_name']:\n",
    "            backbone_out = self.backbone.fc.in_features\n",
    "        else:\n",
    "            backbone_out = self.backbone.num_features\n",
    "            \n",
    "        \n",
    "        self.fc1 = nn.Linear(backbone_out, backbone_out, bias=True)\n",
    "        self.att_block = AttBlockV2(backbone_out, self.num_classes, activation=\"sigmoid\")\n",
    "\n",
    "        self.melspec_transform = torchaudio.transforms.MelSpectrogram(\n",
    "            sample_rate=self.cfg['SR'],\n",
    "            hop_length=self.cfg['hop_length'],\n",
    "            n_mels=self.cfg['n_mels'],\n",
    "            f_min=self.cfg['f_min'],\n",
    "            f_max=self.cfg['f_max'],\n",
    "            n_fft=self.cfg['n_fft'],\n",
    "            pad_mode=\"constant\",\n",
    "            norm=\"slaney\",\n",
    "            onesided=True,\n",
    "            mel_scale=\"htk\",\n",
    "        )\n",
    "        if self.cfg['device'] == \"cuda\":\n",
    "            self.melspec_transform = self.melspec_transform.cuda()\n",
    "        else:\n",
    "            self.melspec_transform = self.melspec_transform.cpu()\n",
    "\n",
    "        self.db_transform = torchaudio.transforms.AmplitudeToDB(\n",
    "            stype=\"power\", top_db=80\n",
    "        )\n",
    "\n",
    "\n",
    "    def extract_feature(self,x):\n",
    "        x = x.permute((0, 1, 3, 2))\n",
    "        frames_num = x.shape[2]\n",
    "        \n",
    "        x = x.transpose(1, 3)\n",
    "        x = self.bn0(x)\n",
    "        x = x.transpose(1, 3)\n",
    "        \n",
    "        # if self.training:\n",
    "        #    x = self.spec_augmenter(x)\n",
    "        \n",
    "        x = x.transpose(2, 3)\n",
    "        # (batch_size, channels, freq, frames)\n",
    "        x = self.encoder(x)\n",
    "        \n",
    "        # (batch_size, channels, frames)\n",
    "        x = torch.mean(x, dim=2)\n",
    "        \n",
    "        # channel smoothing\n",
    "        x1 = F.max_pool1d(x, kernel_size=3, stride=1, padding=1)\n",
    "        x2 = F.avg_pool1d(x, kernel_size=3, stride=1, padding=1)\n",
    "        x = x1 + x2\n",
    "        \n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = x.transpose(1, 2)\n",
    "        x = F.relu_(self.fc1(x))\n",
    "        x = x.transpose(1, 2)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        return x, frames_num\n",
    "        \n",
    "    @torch.cuda.amp.autocast(enabled=False)\n",
    "    def transform_to_spec(self, audio):\n",
    "\n",
    "        audio = audio.float()\n",
    "        \n",
    "        spec = self.melspec_transform(audio)\n",
    "        spec = self.db_transform(spec)\n",
    "\n",
    "        if self.cfg['normal'] == 80:\n",
    "            spec = (spec + 80) / 80\n",
    "        elif self.cfg['normal'] == 255:\n",
    "            spec = spec / 255\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "                \n",
    "        if self.cfg['in_channels'] == 3:\n",
    "            spec = image_delta(spec)\n",
    "        \n",
    "        return spec\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        with torch.no_grad():\n",
    "            x = self.transform_to_spec(x)\n",
    "\n",
    "        x, frames_num = self.extract_feature(x)\n",
    "        \n",
    "        (clipwise_output, norm_att, segmentwise_output) = self.att_block(x)\n",
    "        logit = torch.sum(norm_att * self.att_block.cla(x), dim=2)\n",
    "        segmentwise_logit = self.att_block.cla(x).transpose(1, 2)\n",
    "        segmentwise_output = segmentwise_output.transpose(1, 2)\n",
    "\n",
    "        return torch.logit(clipwise_output)\n",
    "\n",
    "    def infer(self, x, tta_delta=2):\n",
    "        with torch.no_grad():\n",
    "            x = self.transform_to_spec(x)\n",
    "        x,_ = self.extract_feature(x)\n",
    "        time_att = torch.tanh(self.att_block.att(x))\n",
    "        feat_time = x.size(-1)\n",
    "        start = (\n",
    "            feat_time / 2 - feat_time * (self.cfg['infer_duration'] / self.cfg['duration_train']) / 2\n",
    "        )\n",
    "        end = start + feat_time * (self.cfg['infer_duration'] / self.cfg['duration_train'])\n",
    "        start = int(start)\n",
    "        end = int(end)\n",
    "        pred = self.attention_infer(start,end,x,time_att)\n",
    "\n",
    "        start_minus = max(0, start-tta_delta)\n",
    "        end_minus=end-tta_delta\n",
    "        pred_minus = self.attention_infer(start_minus,end_minus,x,time_att)\n",
    "\n",
    "        start_plus = start+tta_delta\n",
    "        end_plus=min(feat_time, end+tta_delta)\n",
    "        pred_plus = self.attention_infer(start_plus,end_plus,x,time_att)\n",
    "\n",
    "        pred = 0.5*pred + 0.25*pred_minus + 0.25*pred_plus\n",
    "        return pred\n",
    "        \n",
    "    def attention_infer(self,start,end,x,time_att):\n",
    "        feat = x[:, :, start:end]\n",
    "        # att = torch.softmax(time_att[:, :, start:end], dim=-1)\n",
    "        #             print(feat_time, start, end)\n",
    "        #             print(att_a.sum(), att.sum(), time_att.shape)\n",
    "        framewise_pred = torch.sigmoid(self.att_block.cla(feat))\n",
    "        framewise_pred_max = framewise_pred.max(dim=2)[0]\n",
    "        # clipwise_output = torch.sum(framewise_pred * att, dim=-1)\n",
    "        #logits = torch.sum(\n",
    "        #    self.att_block.cla(feat) * att,\n",
    "        #    dim=-1,\n",
    "        #)\n",
    "\n",
    "        # return clipwise_output\n",
    "        return framewise_pred_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a495374",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-04T10:02:24.969205Z",
     "iopub.status.busy": "2025-06-04T10:02:24.968993Z",
     "iopub.status.idle": "2025-06-04T10:02:24.975330Z",
     "shell.execute_reply": "2025-06-04T10:02:24.974720Z"
    },
    "papermill": {
     "duration": 0.012993,
     "end_time": "2025-06-04T10:02:24.976745",
     "exception": false,
     "start_time": "2025-06-04T10:02:24.963752",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_sample(path, cfg):\n",
    "    audio, orig_sr = sf.read(path, dtype=\"float32\")\n",
    "    seconds = []\n",
    "    audio_length = cfg.SR * cfg.target_duration\n",
    "    step = audio_length\n",
    "    for i in range(audio_length, len(audio) + step, step):\n",
    "        start = max(0, i - audio_length)\n",
    "        end = start + audio_length\n",
    "        if end > len(audio):\n",
    "            pass\n",
    "        else:\n",
    "            seconds.append(int(end/cfg.SR))\n",
    "\n",
    "    audio = np.concatenate([audio,audio,audio])\n",
    "    audios = []\n",
    "    for i,second in enumerate(seconds):\n",
    "        end_seconds = int(second)\n",
    "        start_seconds = int(end_seconds - cfg.target_duration)\n",
    "    \n",
    "        end_index = int(cfg.SR * (end_seconds + (cfg.train_duration - cfg.target_duration) / 2) ) + len(audio) // 3\n",
    "        start_index = int(cfg.SR * (start_seconds - (cfg.train_duration - cfg.target_duration) / 2) ) + len(audio) // 3\n",
    "        end_pad = int(cfg.SR * (cfg.train_duration - cfg.target_duration) / 2) \n",
    "        start_pad = int(cfg.SR * (cfg.train_duration - cfg.target_duration) / 2) \n",
    "        y = audio[start_index:end_index].astype(np.float32)\n",
    "        if i==0:\n",
    "            y[:start_pad] = 0\n",
    "        elif i==(len(seconds)-1):\n",
    "            y[-end_pad:] = 0\n",
    "        audios.append(y)\n",
    "\n",
    "    return audios\n",
    "\n",
    "def sigmoid(x):\n",
    "    s = 1 / (1 + np.exp(-x))\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5dcfac1f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-04T10:02:24.987655Z",
     "iopub.status.busy": "2025-06-04T10:02:24.987447Z",
     "iopub.status.idle": "2025-06-04T10:02:24.996048Z",
     "shell.execute_reply": "2025-06-04T10:02:24.995320Z"
    },
    "papermill": {
     "duration": 0.015375,
     "end_time": "2025-06-04T10:02:24.997338",
     "exception": false,
     "start_time": "2025-06-04T10:02:24.981963",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_model_files(cfg):\n",
    "    \"\"\"\n",
    "    Find all .pth model files in the specified model directory\n",
    "    \"\"\"\n",
    "    model_files = []\n",
    "    \n",
    "    model_dir = Path(cfg.model_path)\n",
    "    \n",
    "    for path in model_dir.glob('**/*.pth'):\n",
    "        model_files.append(str(path))\n",
    "    \n",
    "    return model_files\n",
    "\n",
    "def load_models(cfg, num_classes):\n",
    "    \"\"\"\n",
    "    Load all found model files and prepare them for ensemble\n",
    "    \"\"\"\n",
    "    models = []\n",
    "    \n",
    "    # model_files = find_model_files(cfg)\n",
    "    model_files = cfg.model_files\n",
    "    \n",
    "    if not model_files:\n",
    "        print(f\"Warning: No model files found under {cfg.model_path}!\")\n",
    "        return models\n",
    "    \n",
    "    print(f\"Found a total of {len(model_files)} model files.\")\n",
    "    \n",
    "    for i, model_path in enumerate(model_files):\n",
    "        try:\n",
    "            print(f\"Loading model: {model_path}\")\n",
    "            checkpoint = torch.load(model_path, map_location=torch.device(cfg.device), weights_only=False)\n",
    "            cfg_temp = checkpoint['cfg']\n",
    "            cfg_temp['device'] = cfg.device\n",
    "            \n",
    "            model = BirdCLEFModel(cfg_temp)\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            model = model.to(cfg.device)\n",
    "            model.eval()\n",
    "            model.zero_grad()\n",
    "            model.half().float()\n",
    "            \n",
    "            models.append(model)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model {model_path}: {e}\")\n",
    "    \n",
    "    return models\n",
    "\n",
    "def predict_on_spectrogram(audio_path, models, cfg, species_ids):\n",
    "    \"\"\"Process a single audio file and predict species presence for each 5-second segment\"\"\"\n",
    "    audio_path = str(audio_path)\n",
    "    predictions = []\n",
    "    row_ids = []\n",
    "    soundscape_id = Path(audio_path).stem\n",
    "\n",
    "    print(f\"Processing {soundscape_id}\")\n",
    "    audio_data = load_sample(audio_path, cfg)\n",
    "    for segment_idx, audio_input in enumerate(audio_data):\n",
    "        \n",
    "        end_time_sec = (segment_idx + 1) * cfg.target_duration\n",
    "        row_id = f\"{soundscape_id}_{end_time_sec}\"\n",
    "        row_ids.append(row_id)\n",
    "        \n",
    "        mel_spec = torch.tensor(audio_input, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
    "        mel_spec = mel_spec.to(cfg.device)\n",
    "        \n",
    "        if len(models) == 1:\n",
    "            with torch.no_grad():\n",
    "                outputs = models[0].infer(mel_spec)\n",
    "                final_preds = outputs.squeeze()\n",
    "                # final_preds = torch.sigmoid(outputs).cpu().numpy().squeeze()\n",
    "\n",
    "        else:\n",
    "            segment_preds = []\n",
    "            for model in models:\n",
    "                with torch.no_grad():\n",
    "                    outputs = model.infer(mel_spec)\n",
    "                    probs = outputs.squeeze()\n",
    "                    # probs = torch.sigmoid(outputs).cpu().numpy().squeeze()\n",
    "                    segment_preds.append(probs)\n",
    "\n",
    "            \n",
    "            final_preds = np.mean(segment_preds, axis=0)\n",
    "                \n",
    "        predictions.append(final_preds)\n",
    "\n",
    "    predictions = np.stack(predictions,axis=0)\n",
    "    \n",
    "    return row_ids, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7ba116b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-04T10:02:25.008186Z",
     "iopub.status.busy": "2025-06-04T10:02:25.007955Z",
     "iopub.status.idle": "2025-06-04T10:02:25.017210Z",
     "shell.execute_reply": "2025-06-04T10:02:25.016409Z"
    },
    "papermill": {
     "duration": 0.016241,
     "end_time": "2025-06-04T10:02:25.018655",
     "exception": false,
     "start_time": "2025-06-04T10:02:25.002414",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_inference(cfg, models, species_ids):\n",
    "    \"\"\"Run inference on all test soundscapes\"\"\"\n",
    "    test_files = list(Path(cfg.test_soundscapes).glob('*.ogg'))\n",
    "    if len(test_files) == 0:\n",
    "        test_files = sorted(glob(str(Path('/kaggle/input/birdclef-2025/train_soundscapes') / '*.ogg')))[:3]\n",
    "    \n",
    "    print(f\"Found {len(test_files)} test soundscapes\")\n",
    "\n",
    "    all_row_ids = []\n",
    "    all_predictions = []\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        results = list(\n",
    "        executor.map(\n",
    "            predict_on_spectrogram,\n",
    "            test_files,\n",
    "            itertools.repeat(models),\n",
    "            itertools.repeat(cfg),\n",
    "            itertools.repeat(species_ids)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    for rids, preds in results:\n",
    "        all_row_ids.extend(rids)\n",
    "        all_predictions.extend(preds)\n",
    "    \n",
    "    return all_row_ids, all_predictions\n",
    "\n",
    "def create_submission(row_ids, predictions, species_ids, cfg):\n",
    "    \"\"\"Create submission dataframe\"\"\"\n",
    "    print(\"Creating submission dataframe...\")\n",
    "\n",
    "    submission_dict = {'row_id': row_ids}\n",
    "    \n",
    "    for i, species in enumerate(species_ids):\n",
    "        submission_dict[species] = [pred[i] for pred in predictions]\n",
    "\n",
    "    submission_df = pd.DataFrame(submission_dict)\n",
    "\n",
    "    submission_df.set_index('row_id', inplace=True)\n",
    "\n",
    "    sample_sub = pd.read_csv(cfg.submission_csv, index_col='row_id')\n",
    "\n",
    "    missing_cols = set(sample_sub.columns) - set(submission_df.columns)\n",
    "    if missing_cols:\n",
    "        print(f\"Warning: Missing {len(missing_cols)} species columns in submission\")\n",
    "        for col in missing_cols:\n",
    "            submission_df[col] = 0.0\n",
    "\n",
    "    submission_df = submission_df[sample_sub.columns]\n",
    "\n",
    "    submission_df = submission_df.reset_index()\n",
    "    \n",
    "    return submission_df\n",
    "\n",
    "\n",
    "def smooth_submission(submission_path):\n",
    "        \"\"\"\n",
    "        Post-process the submission CSV by smoothing predictions to enforce temporal consistency.\n",
    "        \n",
    "        For each soundscape (grouped by the file name part of 'row_id'), each row's predictions\n",
    "        are averaged with those of its neighbors using defined weights.\n",
    "        \n",
    "        :param submission_path: Path to the submission CSV file.\n",
    "        \"\"\"\n",
    "        print(\"Smoothing submission predictions...\")\n",
    "        sub = pd.read_csv(submission_path)\n",
    "        cols = sub.columns[1:]\n",
    "        # Extract group names by splitting row_id on the last underscore\n",
    "        groups = sub['row_id'].str.rsplit('_', n=1).str[0].values\n",
    "        unique_groups = np.unique(groups)\n",
    "        \n",
    "        for group in unique_groups:\n",
    "            # Get indices for the current group\n",
    "            idx = np.where(groups == group)[0]\n",
    "            sub_group = sub.iloc[idx].copy()\n",
    "            predictions = sub_group[cols].values\n",
    "            new_predictions = predictions.copy()\n",
    "            \n",
    "            if predictions.shape[0] > 1:\n",
    "                # Smooth the predictions using neighboring segments\n",
    "                new_predictions[0] = (predictions[0] * 0.8) + (predictions[1] * 0.2)\n",
    "                new_predictions[-1] = (predictions[-1] * 0.8) + (predictions[-2] * 0.2)\n",
    "                for i in range(1, predictions.shape[0]-1):\n",
    "                    new_predictions[i] = (predictions[i-1] * 0.1) + (predictions[i] * 0.8) + (predictions[i+1] * 0.1)\n",
    "            # Replace the smoothed values in the submission dataframe\n",
    "            sub.iloc[idx, 1:] = new_predictions\n",
    "        \n",
    "        sub.to_csv(submission_path, index=False)\n",
    "        print(f\"Smoothed submission saved to {submission_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "015c42f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-04T10:02:25.029241Z",
     "iopub.status.busy": "2025-06-04T10:02:25.029043Z",
     "iopub.status.idle": "2025-06-04T10:02:25.033700Z",
     "shell.execute_reply": "2025-06-04T10:02:25.033104Z"
    },
    "papermill": {
     "duration": 0.010929,
     "end_time": "2025-06-04T10:02:25.034701",
     "exception": false,
     "start_time": "2025-06-04T10:02:25.023772",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    start_time = time.time()\n",
    "    print(\"Starting BirdCLEF-2025 inference...\")\n",
    "\n",
    "    models = load_models(cfg, num_classes)\n",
    "    \n",
    "    if not models:\n",
    "        print(\"No models found! Please check model paths.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Model usage: {'Single model' if len(models) == 1 else f'Ensemble of {len(models)} models'}\")\n",
    "\n",
    "    row_ids, predictions = run_inference(cfg, models, species_ids)\n",
    "\n",
    "    submission_df = create_submission(row_ids, predictions, species_ids, cfg)\n",
    "\n",
    "    submission_path = 'submission1.csv'\n",
    "    submission_df.to_csv(submission_path, index=False)\n",
    "    print(f\"Submission saved to {submission_path}\")\n",
    "\n",
    "    smooth_submission(submission_path)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"Inference completed in {(end_time - start_time)/60:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a83515d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-04T10:02:25.045380Z",
     "iopub.status.busy": "2025-06-04T10:02:25.045174Z",
     "iopub.status.idle": "2025-06-04T10:02:33.983539Z",
     "shell.execute_reply": "2025-06-04T10:02:33.982694Z"
    },
    "papermill": {
     "duration": 8.944684,
     "end_time": "2025-06-04T10:02:33.984728",
     "exception": false,
     "start_time": "2025-06-04T10:02:25.040044",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting BirdCLEF-2025 inference...\n",
      "Found a total of 1 model files.\n",
      "Loading model: /kaggle/input/bird2025-sed-ckpt/sedmodel.pth\n",
      "Model usage: Single model\n",
      "Found 3 test soundscapes\n",
      "Processing H02_20230420_074000\n",
      "Processing H02_20230420_112000\n",
      "Processing H02_20230420_154500\n",
      "Creating submission dataframe...\n",
      "Submission saved to submission1.csv\n",
      "Smoothing submission predictions...\n",
      "Smoothed submission saved to submission1.csv\n",
      "Inference completed in 0.15 minutes\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48843bf4",
   "metadata": {
    "papermill": {
     "duration": 0.004516,
     "end_time": "2025-06-04T10:02:33.994138",
     "exception": false,
     "start_time": "2025-06-04T10:02:33.989622",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# second\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d34f4c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-04T10:02:34.004372Z",
     "iopub.status.busy": "2025-06-04T10:02:34.004051Z",
     "iopub.status.idle": "2025-06-04T10:02:34.009473Z",
     "shell.execute_reply": "2025-06-04T10:02:34.008821Z"
    },
    "papermill": {
     "duration": 0.011662,
     "end_time": "2025-06-04T10:02:34.010335",
     "exception": false,
     "start_time": "2025-06-04T10:02:33.998673",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debug mode: False\n",
      "Number of test soundscapes: 0\n"
     ]
    }
   ],
   "source": [
    "test_audio_dir = '../input/birdclef-2025/test_soundscapes/'\n",
    "file_list = [f for f in sorted(os.listdir(test_audio_dir))]\n",
    "file_list = [file.split('.')[0] for file in file_list if file.endswith('.ogg')]\n",
    "debug = False\n",
    "\n",
    "\n",
    "\n",
    "if debug == True:\n",
    "    file_list = file_list[:3]\n",
    "    \n",
    "print('Debug mode:', debug)\n",
    "print('Number of test soundscapes:', len(file_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "00e7cbbd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-04T10:02:34.020612Z",
     "iopub.status.busy": "2025-06-04T10:02:34.020360Z",
     "iopub.status.idle": "2025-06-04T10:02:34.034672Z",
     "shell.execute_reply": "2025-06-04T10:02:34.034143Z"
    },
    "papermill": {
     "duration": 0.020552,
     "end_time": "2025-06-04T10:02:34.035652",
     "exception": false,
     "start_time": "2025-06-04T10:02:34.015100",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "wav_sec = 5\n",
    "sample_rate = 32000\n",
    "min_segment = sample_rate*wav_sec\n",
    "\n",
    "class_labels = sorted(os.listdir('../input/birdclef-2025/train_audio/'))\n",
    "\n",
    "n_fft=1024\n",
    "win_length=1024\n",
    "hop_length=512\n",
    "f_min=50\n",
    "f_max=16000\n",
    "n_mels=128\n",
    "\n",
    "mel_spectrogram = AT.MelSpectrogram(\n",
    "    sample_rate=sample_rate,\n",
    "    n_fft=n_fft,\n",
    "    win_length=win_length,\n",
    "    hop_length=hop_length,\n",
    "    center=True,\n",
    "    f_min=f_min,\n",
    "    f_max=f_max,\n",
    "    pad_mode=\"reflect\",\n",
    "    power=2.0,\n",
    "    norm='slaney',\n",
    "    n_mels=n_mels,\n",
    "    mel_scale=\"htk\",\n",
    "    # normalized=True\n",
    ")\n",
    "\n",
    "def normalize_std(spec, eps=1e-6):\n",
    "    mean = torch.mean(spec)\n",
    "    std = torch.std(spec)\n",
    "    return torch.where(std == 0, spec-mean, (spec - mean) / (std+eps))\n",
    "\n",
    "def audio_to_mel(filepath=None):\n",
    "    waveform, sample_rate = torchaudio.load(filepath,backend=\"soundfile\")\n",
    "    len_wav = waveform.shape[1]\n",
    "    waveform = waveform[0,:].reshape(1, len_wav) # stereo->mono mono->mono\n",
    "    PREDS = []\n",
    "    for i in range(12):\n",
    "        waveform2 = waveform[:,i*sample_rate*5:i*sample_rate*5+sample_rate*5]\n",
    "        melspec = mel_spectrogram(waveform2)\n",
    "        melspec = torch.log(melspec+1e-6)\n",
    "        melspec = normalize_std(melspec)\n",
    "        melspec = torch.unsqueeze(melspec, dim=0)\n",
    "        \n",
    "        PREDS.append(melspec)\n",
    "    return torch.vstack(PREDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a897c8f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-04T10:02:34.045943Z",
     "iopub.status.busy": "2025-06-04T10:02:34.045750Z",
     "iopub.status.idle": "2025-06-04T10:02:34.057856Z",
     "shell.execute_reply": "2025-06-04T10:02:34.056969Z"
    },
    "papermill": {
     "duration": 0.018942,
     "end_time": "2025-06-04T10:02:34.059496",
     "exception": false,
     "start_time": "2025-06-04T10:02:34.040554",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def init_layer(layer):\n",
    "    nn.init.xavier_uniform_(layer.weight)\n",
    "    if hasattr(layer, \"bias\"):\n",
    "        if layer.bias is not None:\n",
    "            layer.bias.data.fill_(0.)\n",
    "\n",
    "\n",
    "def init_bn(bn):\n",
    "    bn.bias.data.fill_(0.)\n",
    "    bn.weight.data.fill_(1.0)\n",
    "\n",
    "\n",
    "def init_weights(model):\n",
    "    classname = model.__class__.__name__\n",
    "    if classname.find(\"Conv2d\") != -1:\n",
    "        nn.init.xavier_uniform_(model.weight, gain=np.sqrt(2))\n",
    "        model.bias.data.fill_(0)\n",
    "    elif classname.find(\"BatchNorm\") != -1:\n",
    "        model.weight.data.normal_(1.0, 0.02)\n",
    "        model.bias.data.fill_(0)\n",
    "    elif classname.find(\"GRU\") != -1:\n",
    "        for weight in model.parameters():\n",
    "            if len(weight.size()) > 1:\n",
    "                nn.init.orghogonal_(weight.data)\n",
    "    elif classname.find(\"Linear\") != -1:\n",
    "        model.weight.data.normal_(0, 0.01)\n",
    "        model.bias.data.zero_()\n",
    "\n",
    "\n",
    "def interpolate(x, ratio):\n",
    "    (batch_size, time_steps, classes_num) = x.shape\n",
    "    upsampled = x[:, :, None, :].repeat(1, 1, ratio, 1)\n",
    "    upsampled = upsampled.reshape(batch_size, time_steps * ratio, classes_num)\n",
    "    return upsampled\n",
    "\n",
    "\n",
    "def pad_framewise_output(framewise_output, frames_num):\n",
    "    output = F.interpolate(\n",
    "        framewise_output.unsqueeze(1),\n",
    "        size=(frames_num, framewise_output.size(2)),\n",
    "        align_corners=True,\n",
    "        mode=\"bilinear\").squeeze(1)\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "class AttBlockV2(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_features: int,\n",
    "                 out_features: int,\n",
    "                 activation=\"linear\"):\n",
    "        super().__init__()\n",
    "\n",
    "        self.activation = activation\n",
    "        self.att = nn.Conv1d(\n",
    "            in_channels=in_features,\n",
    "            out_channels=out_features,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            bias=True)\n",
    "        self.cla = nn.Conv1d(\n",
    "            in_channels=in_features,\n",
    "            out_channels=out_features,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            bias=True)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        init_layer(self.att)\n",
    "        init_layer(self.cla)\n",
    "\n",
    "    def forward(self, x):\n",
    "        norm_att = torch.softmax(torch.tanh(self.att(x)), dim=-1)\n",
    "        cla = self.nonlinear_transform(self.cla(x))\n",
    "        x = torch.sum(norm_att * cla, dim=2)\n",
    "        return x, norm_att, cla\n",
    "\n",
    "    def nonlinear_transform(self, x):\n",
    "        if self.activation == 'linear':\n",
    "            return x\n",
    "        elif self.activation == 'sigmoid':\n",
    "            return torch.sigmoid(x)\n",
    "\n",
    "\n",
    "class TimmSED(nn.Module):\n",
    "    def __init__(self, base_model_name: str, pretrained=False, num_classes=24, in_channels=1, n_mels=24):\n",
    "        super().__init__()\n",
    "\n",
    "        self.bn0 = nn.BatchNorm2d(n_mels)\n",
    "\n",
    "        base_model = timm.create_model(\n",
    "            base_model_name, pretrained=pretrained, in_chans=in_channels)\n",
    "        layers = list(base_model.children())[:-2]\n",
    "        self.encoder = nn.Sequential(*layers)\n",
    "\n",
    "        in_features = base_model.num_features\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features, in_features, bias=True)\n",
    "        self.att_block2 = AttBlockV2(\n",
    "            in_features, num_classes, activation=\"sigmoid\")\n",
    "\n",
    "        self.init_weight()\n",
    "\n",
    "    def init_weight(self):\n",
    "        init_bn(self.bn0)\n",
    "        init_layer(self.fc1)\n",
    "        \n",
    "\n",
    "    def forward(self, input_data):\n",
    "        x = input_data.transpose(2,3)\n",
    "        x = torch.cat((x,x,x),1)\n",
    "\n",
    "        x = x.transpose(2, 3)\n",
    "\n",
    "        x = self.encoder(x)\n",
    "        \n",
    "        x = torch.mean(x, dim=2)\n",
    "\n",
    "        x1 = F.max_pool1d(x, kernel_size=3, stride=1, padding=1)\n",
    "        x2 = F.avg_pool1d(x, kernel_size=3, stride=1, padding=1)\n",
    "        x = x1 + x2\n",
    "\n",
    "        x = x.transpose(1, 2)\n",
    "        x = F.relu_(self.fc1(x))\n",
    "        x = x.transpose(1, 2)\n",
    "\n",
    "        (clipwise_output, norm_att, segmentwise_output) = self.att_block2(x)\n",
    "        logit = torch.sum(norm_att * self.att_block2.cla(x), dim=2)\n",
    "\n",
    "        output_dict = {\n",
    "            'logit': logit,\n",
    "        }\n",
    "\n",
    "        return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0418b19a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-04T10:02:34.071001Z",
     "iopub.status.busy": "2025-06-04T10:02:34.070736Z",
     "iopub.status.idle": "2025-06-04T10:02:34.075888Z",
     "shell.execute_reply": "2025-06-04T10:02:34.075241Z"
    },
    "papermill": {
     "duration": 0.011758,
     "end_time": "2025-06-04T10:02:34.076875",
     "exception": false,
     "start_time": "2025-06-04T10:02:34.065117",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/kaggle/input/birdclef-2025-sed-models-p/sed0.pth',\n",
       " '/kaggle/input/birdclef-2025-sed-models-p/sed1.pth',\n",
       " '/kaggle/input/birdclef-2025-sed-models-p/sed2.pth']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model_name='eca_nfnet_l0'\n",
    "pretrained=False\n",
    "in_channels=3\n",
    "\n",
    "MODELS = [f'/kaggle/input/birdclef-2025-sed-models-p/sed{i}.pth' for i in range(3)]\n",
    "\n",
    "MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9a377b36",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-04T10:02:34.088338Z",
     "iopub.status.busy": "2025-06-04T10:02:34.088086Z",
     "iopub.status.idle": "2025-06-04T10:02:37.583380Z",
     "shell.execute_reply": "2025-06-04T10:02:37.582659Z"
    },
    "papermill": {
     "duration": 3.50252,
     "end_time": "2025-06-04T10:02:37.584828",
     "exception": false,
     "start_time": "2025-06-04T10:02:34.082308",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "models = []\n",
    "for path in MODELS:\n",
    "    model = TimmSED(base_model_name=base_model_name,\n",
    "               pretrained=pretrained,\n",
    "               num_classes=len(class_labels),\n",
    "               in_channels=in_channels,\n",
    "               n_mels=n_mels);\n",
    "    model.load_state_dict(torch.load(path, weights_only=False, map_location=torch.device('cpu')))\n",
    "    model.eval();\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "be23fbad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-04T10:02:37.596923Z",
     "iopub.status.busy": "2025-06-04T10:02:37.596560Z",
     "iopub.status.idle": "2025-06-04T10:02:37.603281Z",
     "shell.execute_reply": "2025-06-04T10:02:37.602552Z"
    },
    "papermill": {
     "duration": 0.01425,
     "end_time": "2025-06-04T10:02:37.604727",
     "exception": false,
     "start_time": "2025-06-04T10:02:37.590477",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prediction(afile):    \n",
    "    global pred\n",
    "    path = test_audio_dir + afile + '.ogg'\n",
    "    with torch.inference_mode():\n",
    "        sig = audio_to_mel(path)\n",
    "        outputs = None\n",
    "        for model in models:\n",
    "            model.eval()\n",
    "            p = model(sig)\n",
    "            p = torch.sigmoid(p['logit']).detach().cpu().numpy() \n",
    "            # p = apply_power_to_low_ranked_cols(p, top_k=30,exponent=2)\n",
    "            if outputs is None: outputs = p\n",
    "            else: outputs += p\n",
    "            \n",
    "        outputs /= len(models)\n",
    "        chunks = [[] for i in range(12)]\n",
    "        for i in range(len(chunks)):        \n",
    "            chunk_end_time = (i + 1) * 5\n",
    "            row_id = afile + '_' + str(chunk_end_time)\n",
    "            pred['row_id'].append(row_id)\n",
    "            bird_no = 0\n",
    "            for bird in class_labels:         \n",
    "                pred[bird].append(outputs[i,bird_no])\n",
    "                bird_no += 1\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d5ff2065",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-04T10:02:37.615569Z",
     "iopub.status.busy": "2025-06-04T10:02:37.615331Z",
     "iopub.status.idle": "2025-06-04T10:02:37.620178Z",
     "shell.execute_reply": "2025-06-04T10:02:37.619304Z"
    },
    "papermill": {
     "duration": 0.011579,
     "end_time": "2025-06-04T10:02:37.621470",
     "exception": false,
     "start_time": "2025-06-04T10:02:37.609891",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pred = {'row_id': []}\n",
    "for species_code in class_labels:\n",
    "    pred[species_code] = []\n",
    "    \n",
    "start = time.time()\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
    "    _ = list(executor.map(prediction, file_list))\n",
    "end_t = time.time()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dfa68733",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-04T10:02:37.635796Z",
     "iopub.status.busy": "2025-06-04T10:02:37.634818Z",
     "iopub.status.idle": "2025-06-04T10:02:37.658882Z",
     "shell.execute_reply": "2025-06-04T10:02:37.657875Z"
    },
    "papermill": {
     "duration": 0.03246,
     "end_time": "2025-06-04T10:02:37.660349",
     "exception": false,
     "start_time": "2025-06-04T10:02:37.627889",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>1139490</th>\n",
       "      <th>1192948</th>\n",
       "      <th>1194042</th>\n",
       "      <th>126247</th>\n",
       "      <th>1346504</th>\n",
       "      <th>134933</th>\n",
       "      <th>135045</th>\n",
       "      <th>1462711</th>\n",
       "      <th>1462737</th>\n",
       "      <th>...</th>\n",
       "      <th>yebfly1</th>\n",
       "      <th>yebsee1</th>\n",
       "      <th>yecspi2</th>\n",
       "      <th>yectyr1</th>\n",
       "      <th>yehbla2</th>\n",
       "      <th>yehcar1</th>\n",
       "      <th>yelori1</th>\n",
       "      <th>yeofly1</th>\n",
       "      <th>yercac1</th>\n",
       "      <th>ywcpar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 207 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [row_id, 1139490, 1192948, 1194042, 126247, 1346504, 134933, 135045, 1462711, 1462737, 1564122, 21038, 21116, 21211, 22333, 22973, 22976, 24272, 24292, 24322, 41663, 41778, 41970, 42007, 42087, 42113, 46010, 47067, 476537, 476538, 48124, 50186, 517119, 523060, 528041, 52884, 548639, 555086, 555142, 566513, 64862, 65336, 65344, 65349, 65373, 65419, 65448, 65547, 65962, 66016, 66531, 66578, 66893, 67082, 67252, 714022, 715170, 787625, 81930, 868458, 963335, amakin1, amekes, ampkin1, anhing, babwar, bafibi1, banana, baymac, bbwduc, bicwre1, bkcdon, bkmtou1, blbgra1, blbwre1, blcant4, blchaw1, blcjay1, blctit1, blhpar1, blkvul, bobfly1, bobher1, brtpar1, bubcur1, bubwre1, bucmot3, bugtan, butsal1, cargra1, cattyr, chbant1, chfmac1, cinbec1, cocher1, cocwoo1, colara1, colcha1, compau, compot1, ...]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 207 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = pd.DataFrame(pred, columns = ['row_id'] + class_labels) \n",
    "display(results.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "77482251",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-04T10:02:37.675596Z",
     "iopub.status.busy": "2025-06-04T10:02:37.675231Z",
     "iopub.status.idle": "2025-06-04T10:02:37.699812Z",
     "shell.execute_reply": "2025-06-04T10:02:37.698739Z"
    },
    "papermill": {
     "duration": 0.034295,
     "end_time": "2025-06-04T10:02:37.701781",
     "exception": false,
     "start_time": "2025-06-04T10:02:37.667486",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "results.to_csv(\"submission2.csv\", index=False)    \n",
    "\n",
    "sub = pd.read_csv('submission2.csv')\n",
    "cols = sub.columns[1:]\n",
    "groups = sub['row_id'].str.rsplit('_', n=1).str[0]\n",
    "groups = groups.values\n",
    "for group in np.unique(groups):\n",
    "    sub_group = sub[group == groups]\n",
    "    predictions = sub_group[cols].values\n",
    "    new_predictions = predictions.copy()\n",
    "    for i in range(1, predictions.shape[0]-1):\n",
    "        new_predictions[i] = (predictions[i-1] * 0.1) + (predictions[i] * 0.8) + (predictions[i+1] * 0.1)\n",
    "    new_predictions[0] = (predictions[0] * 0.9) + (predictions[1] * 0.1)\n",
    "    new_predictions[-1] = (predictions[-1] * 0.9) + (predictions[-2] * 0.1)\n",
    "    sub_group[cols] = new_predictions\n",
    "    sub[group == groups] = sub_group\n",
    "sub.to_csv(\"submission2.csv\", index=False)\n",
    "\n",
    "\n",
    "if debug:\n",
    "    display(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21acffb5",
   "metadata": {
    "papermill": {
     "duration": 0.006804,
     "end_time": "2025-06-04T10:02:37.714791",
     "exception": false,
     "start_time": "2025-06-04T10:02:37.707987",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 融合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9325ad17",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-04T10:02:37.730208Z",
     "iopub.status.busy": "2025-06-04T10:02:37.729945Z",
     "iopub.status.idle": "2025-06-04T10:02:37.736286Z",
     "shell.execute_reply": "2025-06-04T10:02:37.735486Z"
    },
    "papermill": {
     "duration": 0.014711,
     "end_time": "2025-06-04T10:02:37.737535",
     "exception": false,
     "start_time": "2025-06-04T10:02:37.722824",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def scale_low_ranked_probabilities(\n",
    "    df: pd.DataFrame,\n",
    "    top_k: int = 30,\n",
    "    exponent: float = 2.0,\n",
    "    id_column: str = 'row_id'\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    对DataFrame中每一行的概率值进行处理。\n",
    "    保留每行中最高的 `top_k` 个概率值不变，将其余（排名30名之后）\n",
    "    的概率值进行幂运算（例如平方），以缩小它们。\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): 输入的DataFrame，包含'row_id'列和多个概率列。\n",
    "        top_k (int): 每一行中要保留不变的最高概率列的数量。\n",
    "        exponent (float): 对低排名概率值应用的幂指数。例如，2.0 表示平方。\n",
    "        id_column (str): DataFrame中用于标识行ID的列名。该列不会被处理。\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: 一个新的DataFrame，其中低排名的概率值已被修改。\n",
    "    \"\"\"\n",
    "    df_processed = df.copy()\n",
    "    probability_columns = [col for col in df_processed.columns if col != id_column]\n",
    "    \n",
    "    def _apply_power_to_row(row_series: pd.Series) -> pd.Series:\n",
    "        current_probs = row_series[probability_columns].values\n",
    "        sorted_indices = np.argsort(current_probs)\n",
    "        \n",
    "        indices_to_modify_in_probs_array = sorted_indices[:len(probability_columns) - top_k]\n",
    "\n",
    "        current_probs[indices_to_modify_in_probs_array] = current_probs[indices_to_modify_in_probs_array] ** exponent\n",
    "\n",
    "        row_series[probability_columns] = current_probs\n",
    "        return row_series\n",
    "\n",
    "    df_processed = df_processed.apply(_apply_power_to_row, axis=1)\n",
    "\n",
    "    return df_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c5f08a59",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-04T10:02:37.754039Z",
     "iopub.status.busy": "2025-06-04T10:02:37.753137Z",
     "iopub.status.idle": "2025-06-04T10:02:39.587961Z",
     "shell.execute_reply": "2025-06-04T10:02:39.587066Z"
    },
    "papermill": {
     "duration": 1.844399,
     "end_time": "2025-06-04T10:02:39.589411",
     "exception": false,
     "start_time": "2025-06-04T10:02:37.745012",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------- #\n",
    "# [IMPORTANT]\n",
    "# * Blending Weight\n",
    "# ------------------------------------------- #\n",
    "# sub_w=[0.2,0.6,0.2]\n",
    "list_TARGETs = sorted(os.listdir('/kaggle/input/birdclef-2025/train_audio/'))\n",
    "list_targets_0 = [f'{TARGET} 0' for TARGET in list_TARGETs]\n",
    "list_targets_1 = [f'{TARGET} 1' for TARGET in list_TARGETs]\n",
    "list_targets_2 = [f'{TARGET} 2' for TARGET in list_TARGETs]\n",
    "\n",
    "\n",
    "df0 = pd.read_csv(\"/kaggle/working/submission0.csv\")\n",
    "df1 = pd.read_csv(\"/kaggle/working/submission1.csv\")\n",
    "df2 = pd.read_csv(\"/kaggle/working/submission2.csv\")\n",
    "\n",
    "\n",
    "df0 = df0.rename(columns={TARGET : f'{TARGET} 0' for TARGET in list_TARGETs})\n",
    "df1 = df1.rename(columns={TARGET : f'{TARGET} 1' for TARGET in list_TARGETs})\n",
    "df2 = df2.rename(columns={TARGET : f'{TARGET} 2' for TARGET in list_TARGETs})\n",
    "\n",
    "\n",
    "dfs_merged = pd.merge(df0, df1, on='row_id', how='inner') \n",
    "dfs = pd.merge(dfs_merged, df2, on='row_id', how='inner')\n",
    "\n",
    "for i in range(len(list_TARGETs)):\n",
    "    # dfs[list_TARGETs[i]] = dfs[list_targets_0[i]]*sub_w[0] +  sub_w[1]*dfs[list_targets_1[i]] + sub_w[2]*dfs[list_targets_2[i]]\n",
    "    dfs[list_TARGETs[i]] = pd.concat([\n",
    "        dfs[list_targets_0[i]],\n",
    "        dfs[list_targets_1[i]],\n",
    "        dfs[list_targets_2[i]]\n",
    "    ], axis=1).min(axis=1)\n",
    "\n",
    "for col0,col2 in zip(list_targets_0, list_targets_2):\n",
    "    del dfs[col0]\n",
    "    del dfs[col2]\n",
    "for col1 in list_targets_1:\n",
    "    del dfs[col1]\n",
    "\n",
    "dfs = scale_low_ranked_probabilities(dfs, top_k=30, exponent=2.0)\n",
    "dfs.to_csv(\"submission.csv\", index=False)\n",
    "\n",
    "print(\"finish\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1c0af097",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-04T10:02:39.600997Z",
     "iopub.status.busy": "2025-06-04T10:02:39.600757Z",
     "iopub.status.idle": "2025-06-04T10:02:39.604572Z",
     "shell.execute_reply": "2025-06-04T10:02:39.603864Z"
    },
    "papermill": {
     "duration": 0.010854,
     "end_time": "2025-06-04T10:02:39.605564",
     "exception": false,
     "start_time": "2025-06-04T10:02:39.594710",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "# data = {\n",
    "#     'value_set_A': [10, 5, 8],\n",
    "#     'value_set_B': [3, 12, 6],\n",
    "#     'value_set_C': [7, 4, 9]\n",
    "# }\n",
    "# dfs = pd.DataFrame(data)\n",
    "\n",
    "# list_targets_0 = dfs.iloc[0]\n",
    "# list_targets_1 = dfs.iloc[1]\n",
    "# list_targets_2 = dfs.iloc[2]\n",
    "# print(list_targets_0)\n",
    "# print(list_targets_1)\n",
    "# print(list_targets_2)\n",
    "\n",
    "\n",
    "# print(\"--- 原始 DataFrame (dfs) ---\")\n",
    "# print(dfs)\n",
    "# print(\"-\" * 40)\n",
    "\n",
    "# combined_series = pd.concat([\n",
    "#     dfs.iloc[0],  # dfs['value_set_A']\n",
    "#     dfs.iloc[1],  # dfs['value_set_B']\n",
    "#     dfs.iloc[2]   # dfs['value_set_C']\n",
    "# ], axis=1) # axis=0 表示按行堆叠\n",
    "\n",
    "# print(combined_series)\n",
    "# print(\"-\" * 40)\n",
    "\n",
    "\n",
    "# global_min_scalar = combined_series.min(axis=1) # axis=0 对 Series 来说是默认且唯一的轴\n",
    "\n",
    "# # print(f\"\\n--- 步骤 B: .min(axis=0) 后的结果 (一个标量值) ---\")\n",
    "# print(global_min_scalar) # 输出: 3\n",
    "# # print(f\"数据类型: {type(global_min_scalar)}\") # 输出: <class 'numpy.int64'> (或类似)\n",
    "# # print(\"-\" * 40)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "239ef140",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-04T10:02:39.616547Z",
     "iopub.status.busy": "2025-06-04T10:02:39.616302Z",
     "iopub.status.idle": "2025-06-04T10:02:39.619673Z",
     "shell.execute_reply": "2025-06-04T10:02:39.619044Z"
    },
    "papermill": {
     "duration": 0.010095,
     "end_time": "2025-06-04T10:02:39.620799",
     "exception": false,
     "start_time": "2025-06-04T10:02:39.610704",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "#     dfs = pd.read_csv(\"/kaggle/working/submission0.csv\")\n",
    "#     top_k = 30\n",
    "#     print(\"--- 原始 DataFrame (部分) ---\")\n",
    "#     print(dfs.iloc[:5,:5]) # 只打印前5列看看\n",
    "#     print(\"\\nShape:\", dfs.shape)\n",
    "\n",
    "#     dfs_modified,probability_columns = scale_low_ranked_probabilities(dfs, top_k=30, exponent=2.0)\n",
    "\n",
    "#     print(\"\\n--- 修改后的 DataFrame (部分) ---\")\n",
    "#     print(dfs_modified.iloc[:5, :5]) # 同样只打印前5列看看\n",
    "#     print(\"\\nShape:\", dfs_modified.shape)\n",
    "\n",
    "\n",
    "#     # row_idx = 0\n",
    "#     # original_row_values = dfs.iloc[row_idx, 1:].values # 排除row_id\n",
    "#     # modified_row_values = dfs_modified.iloc[row_idx, 1:].values\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312ce95d",
   "metadata": {
    "papermill": {
     "duration": 0.004968,
     "end_time": "2025-06-04T10:02:39.631395",
     "exception": false,
     "start_time": "2025-06-04T10:02:39.626427",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 11361821,
     "sourceId": 91844,
     "sourceType": "competition"
    },
    {
     "datasetId": 7430593,
     "sourceId": 11828260,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7457365,
     "sourceId": 11867185,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7459455,
     "sourceId": 11870082,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7459522,
     "sourceId": 11870175,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7459867,
     "sourceId": 11870659,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7477284,
     "sourceId": 11895503,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7130272,
     "sourceId": 11948763,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 38.939318,
   "end_time": "2025-06-04T10:02:42.801187",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-06-04T10:02:03.861869",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "2885b81b14fa431abb17727466906319": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_5ffdc4300e5b47a0b05764cf7500d754",
        "IPY_MODEL_598c266ac40848bda9b0ee4c67bf2137",
        "IPY_MODEL_87eda954dc8745b5be5174d580d2ad31"
       ],
       "layout": "IPY_MODEL_ede1bd732bd44647a85bd24cc9d80778",
       "tabbable": null,
       "tooltip": null
      }
     },
     "35d99457158d4cdf91007a5f0e06a2a8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "598c266ac40848bda9b0ee4c67bf2137": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_e70dc88643554713ae5644bd8ae6fe89",
       "max": 1.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_c76ef6c107324995a3f430ee3b4dfd56",
       "tabbable": null,
       "tooltip": null,
       "value": 0.0
      }
     },
     "5ffdc4300e5b47a0b05764cf7500d754": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_a281346b713f4f96889dbf1828587b26",
       "placeholder": "​",
       "style": "IPY_MODEL_35d99457158d4cdf91007a5f0e06a2a8",
       "tabbable": null,
       "tooltip": null,
       "value": ""
      }
     },
     "87eda954dc8745b5be5174d580d2ad31": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_a19454cc239f4bdba37e53eead1d9664",
       "placeholder": "​",
       "style": "IPY_MODEL_cb9594be12904911be2ae1a9b81811fd",
       "tabbable": null,
       "tooltip": null,
       "value": " 0/0 [00:00&lt;?, ?it/s]"
      }
     },
     "a19454cc239f4bdba37e53eead1d9664": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a281346b713f4f96889dbf1828587b26": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c76ef6c107324995a3f430ee3b4dfd56": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "cb9594be12904911be2ae1a9b81811fd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "e70dc88643554713ae5644bd8ae6fe89": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": "20px"
      }
     },
     "ede1bd732bd44647a85bd24cc9d80778": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
