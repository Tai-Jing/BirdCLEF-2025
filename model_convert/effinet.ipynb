{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f0f1f6d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T15:10:23.399596Z",
     "iopub.status.busy": "2025-05-28T15:10:23.399246Z",
     "iopub.status.idle": "2025-05-28T15:10:46.082340Z",
     "shell.execute_reply": "2025-05-28T15:10:46.081206Z"
    },
    "papermill": {
     "duration": 22.689665,
     "end_time": "2025-05-28T15:10:46.084087",
     "exception": false,
     "start_time": "2025-05-28T15:10:23.394422",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import warnings\n",
    "import logging\n",
    "import time\n",
    "import math\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "from soundfile import SoundFile \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import timm\n",
    "from tqdm.auto import tqdm\n",
    "from glob import glob\n",
    "import torchaudio\n",
    "import random\n",
    "import itertools\n",
    "from typing import Union\n",
    "\n",
    "import pickle\n",
    "import torchaudio\n",
    "import torchaudio.transforms as AT\n",
    "from contextlib import contextmanager\n",
    "import concurrent.futures\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d6295ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T15:10:46.091138Z",
     "iopub.status.busy": "2025-05-28T15:10:46.090583Z",
     "iopub.status.idle": "2025-05-28T15:10:46.119429Z",
     "shell.execute_reply": "2025-05-28T15:10:46.118363Z"
    },
    "papermill": {
     "duration": 0.034114,
     "end_time": "2025-05-28T15:10:46.121110",
     "exception": false,
     "start_time": "2025-05-28T15:10:46.086996",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Loading taxonomy data...\n"
     ]
    }
   ],
   "source": [
    "class CFG:\n",
    " \n",
    "    taxonomy_csv = '/kaggle/input/birdclef-2025/taxonomy.csv'\n",
    "\n",
    "    model_dicts = {\n",
    "    \"efficientnet_b0\":[\n",
    "                       '/kaggle/input/effinetb0-all-2025/model_fold1.pth' ,\n",
    "        '/kaggle/input/effnetb0seed2024/model_fold0.pth',\n",
    "        '/kaggle/input/effientb0-2023/model_fold0.pth'],\n",
    "\n",
    "\n",
    "    # \"tf_mobilenetv3_large_100.in1k\":[\"/kaggle/input/mobile-bcefocal/model_fold0.pth\",\n",
    "    #                                  '/kaggle/input/mobile-bcefocal/model_fold1.pth',\n",
    "    #                                  \"/kaggle/input/mobile-bcefocal/model_fold2.pth\"\n",
    "    #                                 ]\n",
    "        \n",
    "    # \"regnety_008\":\n",
    "        # \"/kaggle/input/bird25-all-train-effnet-v2-s-bcefocalloss/model_fold1.pth\"    \n",
    "}\n",
    "\n",
    "    model_path = \"/kaggle/input/effinetb0-all-2025/model_fold4.pth\" \n",
    "    model_name = \"efficientnet_b0\"\n",
    "    # # Audio parameters\n",
    "    # FS = 32000  \n",
    "    # WINDOW_SIZE = 5  \n",
    "    \n",
    "    # # Mel spectrogram parameters\n",
    "    # N_FFT = 1024\n",
    "    # HOP_LENGTH = 512\n",
    "    # N_MELS = 128\n",
    "    # FMIN = 50\n",
    "    # FMAX = 16000\n",
    "    # TARGET_SHAPE = (256, 256)\n",
    "    num_classes = 206\n",
    "  \n",
    "    in_channels = 1\n",
    "    device = 'cpu'  \n",
    "    \n",
    "    # Inference parameters\n",
    "    batch_size = 32  \n",
    "    # TTA 的次数。 如果 use_tta 为 True，则指定对每个测试样本进行多少次增强。\n",
    "    threshold = 0.5\n",
    "\n",
    "    debug =  True\n",
    "    # True  False\n",
    "    debug_count = 3\n",
    "\n",
    "cfg = CFG()\n",
    "print(f\"Using device: {cfg.device}\")\n",
    "print(f\"Loading taxonomy data...\")\n",
    "taxonomy_df = pd.read_csv(cfg.taxonomy_csv)\n",
    "species_ids = taxonomy_df['primary_label'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a952e39e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T15:10:46.128070Z",
     "iopub.status.busy": "2025-05-28T15:10:46.127694Z",
     "iopub.status.idle": "2025-05-28T15:10:46.136456Z",
     "shell.execute_reply": "2025-05-28T15:10:46.135385Z"
    },
    "papermill": {
     "duration": 0.014372,
     "end_time": "2025-05-28T15:10:46.138194",
     "exception": false,
     "start_time": "2025-05-28T15:10:46.123822",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Efficientb0Model(nn.Module):\n",
    "    def __init__(self,cfg,model_name):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        \n",
    "        self.backbone = timm.create_model(\n",
    "            model_name,\n",
    "            pretrained=False,  \n",
    "            in_chans=cfg.in_channels,\n",
    "            drop_rate=0.0,       # 训练时就不需要使用暂退法来类似进行正则化了\n",
    "            drop_path_rate=0.0\n",
    "        )\n",
    "        \n",
    "        if 'efficientnet' in model_name:\n",
    "            backbone_out = self.backbone.classifier.in_features\n",
    "            self.backbone.classifier = nn.Identity()\n",
    "        elif 'resnet' in model_name:\n",
    "            backbone_out = self.backbone.fc.in_features\n",
    "            self.backbone.fc = nn.Identity()\n",
    "        else:\n",
    "            backbone_out = self.backbone.get_classifier().in_features\n",
    "            self.backbone.reset_classifier(0, '')\n",
    "        \n",
    "        self.pooling = nn.AdaptiveAvgPool2d(1)\n",
    "        self.feat_dim = backbone_out\n",
    "        self.classifier = nn.Linear(backbone_out, cfg.num_classes)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        \n",
    "        if isinstance(features, dict):\n",
    "            features = features['features']\n",
    "            \n",
    "        if len(features.shape) == 4:\n",
    "            features = self.pooling(features)\n",
    "            features = features.view(features.size(0), -1)\n",
    "        \n",
    "        logits = self.classifier(features)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b93837f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T15:10:46.145429Z",
     "iopub.status.busy": "2025-05-28T15:10:46.144579Z",
     "iopub.status.idle": "2025-05-28T15:10:58.572535Z",
     "shell.execute_reply": "2025-05-28T15:10:58.571338Z"
    },
    "papermill": {
     "duration": 12.433797,
     "end_time": "2025-05-28T15:10:58.574798",
     "exception": false,
     "start_time": "2025-05-28T15:10:46.141001",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: ../input/openvino-wheels\r\n",
      "Processing /kaggle/input/openvino-wheels/openvino_dev-2024.6.0-17404-py3-none-any.whl (from openvino-dev[onnx]==2024.6.0->-r ../input/openvino-wheels/requirements.txt (line 1))\r\n",
      "Requirement already satisfied: defusedxml>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from openvino-dev==2024.6.0->openvino-dev[onnx]==2024.6.0->-r ../input/openvino-wheels/requirements.txt (line 1)) (0.7.1)\r\n",
      "Processing /kaggle/input/openvino-wheels/networkx-3.1-py3-none-any.whl (from openvino-dev==2024.6.0->openvino-dev[onnx]==2024.6.0->-r ../input/openvino-wheels/requirements.txt (line 1))\r\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.16.6 in /usr/local/lib/python3.11/dist-packages (from openvino-dev==2024.6.0->openvino-dev[onnx]==2024.6.0->-r ../input/openvino-wheels/requirements.txt (line 1)) (1.26.4)\r\n",
      "Processing /kaggle/input/openvino-wheels/openvino_telemetry-2025.1.0-py3-none-any.whl (from openvino-dev==2024.6.0->openvino-dev[onnx]==2024.6.0->-r ../input/openvino-wheels/requirements.txt (line 1))\r\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from openvino-dev==2024.6.0->openvino-dev[onnx]==2024.6.0->-r ../input/openvino-wheels/requirements.txt (line 1)) (25.0)\r\n",
      "Requirement already satisfied: pyyaml>=5.4.1 in /usr/local/lib/python3.11/dist-packages (from openvino-dev==2024.6.0->openvino-dev[onnx]==2024.6.0->-r ../input/openvino-wheels/requirements.txt (line 1)) (6.0.2)\r\n",
      "Requirement already satisfied: requests>=2.25.1 in /usr/local/lib/python3.11/dist-packages (from openvino-dev==2024.6.0->openvino-dev[onnx]==2024.6.0->-r ../input/openvino-wheels/requirements.txt (line 1)) (2.32.3)\r\n",
      "Processing /kaggle/input/openvino-wheels/openvino-2024.6.0-17404-cp311-cp311-manylinux2014_x86_64.whl (from openvino-dev==2024.6.0->openvino-dev[onnx]==2024.6.0->-r ../input/openvino-wheels/requirements.txt (line 1))\r\n",
      "Processing /kaggle/input/openvino-wheels/fastjsonschema-2.17.1-py3-none-any.whl (from openvino-dev[onnx]==2024.6.0->-r ../input/openvino-wheels/requirements.txt (line 1))\r\n",
      "Requirement already satisfied: onnx<=1.17.0,>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from openvino-dev[onnx]==2024.6.0->-r ../input/openvino-wheels/requirements.txt (line 1)) (1.17.0)\r\n",
      "Requirement already satisfied: protobuf<4.0.0,>=3.18.1 in /usr/local/lib/python3.11/dist-packages (from openvino-dev[onnx]==2024.6.0->-r ../input/openvino-wheels/requirements.txt (line 1)) (3.20.3)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<2.0.0,>=1.16.6->openvino-dev==2024.6.0->openvino-dev[onnx]==2024.6.0->-r ../input/openvino-wheels/requirements.txt (line 1)) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<2.0.0,>=1.16.6->openvino-dev==2024.6.0->openvino-dev[onnx]==2024.6.0->-r ../input/openvino-wheels/requirements.txt (line 1)) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<2.0.0,>=1.16.6->openvino-dev==2024.6.0->openvino-dev[onnx]==2024.6.0->-r ../input/openvino-wheels/requirements.txt (line 1)) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<2.0.0,>=1.16.6->openvino-dev==2024.6.0->openvino-dev[onnx]==2024.6.0->-r ../input/openvino-wheels/requirements.txt (line 1)) (2025.1.0)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<2.0.0,>=1.16.6->openvino-dev==2024.6.0->openvino-dev[onnx]==2024.6.0->-r ../input/openvino-wheels/requirements.txt (line 1)) (2022.1.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<2.0.0,>=1.16.6->openvino-dev==2024.6.0->openvino-dev[onnx]==2024.6.0->-r ../input/openvino-wheels/requirements.txt (line 1)) (2.4.1)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.25.1->openvino-dev==2024.6.0->openvino-dev[onnx]==2024.6.0->-r ../input/openvino-wheels/requirements.txt (line 1)) (3.4.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.25.1->openvino-dev==2024.6.0->openvino-dev[onnx]==2024.6.0->-r ../input/openvino-wheels/requirements.txt (line 1)) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.25.1->openvino-dev==2024.6.0->openvino-dev[onnx]==2024.6.0->-r ../input/openvino-wheels/requirements.txt (line 1)) (2.4.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.25.1->openvino-dev==2024.6.0->openvino-dev[onnx]==2024.6.0->-r ../input/openvino-wheels/requirements.txt (line 1)) (2025.4.26)\r\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.0.0,>=1.16.6->openvino-dev==2024.6.0->openvino-dev[onnx]==2024.6.0->-r ../input/openvino-wheels/requirements.txt (line 1)) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.0.0,>=1.16.6->openvino-dev==2024.6.0->openvino-dev[onnx]==2024.6.0->-r ../input/openvino-wheels/requirements.txt (line 1)) (2022.1.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<2.0.0,>=1.16.6->openvino-dev==2024.6.0->openvino-dev[onnx]==2024.6.0->-r ../input/openvino-wheels/requirements.txt (line 1)) (1.3.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<2.0.0,>=1.16.6->openvino-dev==2024.6.0->openvino-dev[onnx]==2024.6.0->-r ../input/openvino-wheels/requirements.txt (line 1)) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<2.0.0,>=1.16.6->openvino-dev==2024.6.0->openvino-dev[onnx]==2024.6.0->-r ../input/openvino-wheels/requirements.txt (line 1)) (2024.2.0)\r\n",
      "Installing collected packages: openvino-telemetry, fastjsonschema, networkx, openvino, openvino-dev\r\n",
      "  Attempting uninstall: fastjsonschema\r\n",
      "    Found existing installation: fastjsonschema 2.21.1\r\n",
      "    Uninstalling fastjsonschema-2.21.1:\r\n",
      "      Successfully uninstalled fastjsonschema-2.21.1\r\n",
      "  Attempting uninstall: networkx\r\n",
      "    Found existing installation: networkx 3.4.2\r\n",
      "    Uninstalling networkx-3.4.2:\r\n",
      "      Successfully uninstalled networkx-3.4.2\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "nx-cugraph-cu12 25.2.0 requires networkx>=3.2, but you have networkx 3.1 which is incompatible.\r\n",
      "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.9.0.13 which is incompatible.\r\n",
      "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\r\n",
      "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.4.0.6 which is incompatible.\r\n",
      "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.10.19 which is incompatible.\r\n",
      "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.7.4.40 which is incompatible.\r\n",
      "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.9.5 which is incompatible.\r\n",
      "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.9.41 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed fastjsonschema-2.17.1 networkx-3.1 openvino-2024.6.0 openvino-dev-2024.6.0 openvino-telemetry-2025.1.0\r\n"
     ]
    }
   ],
   "source": [
    "! python -m pip install --no-index --find-links=../input/openvino-wheels -r ../input/openvino-wheels/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8bedc24d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T15:10:58.583569Z",
     "iopub.status.busy": "2025-05-28T15:10:58.583212Z",
     "iopub.status.idle": "2025-05-28T15:10:58.948989Z",
     "shell.execute_reply": "2025-05-28T15:10:58.947221Z"
    },
    "papermill": {
     "duration": 0.373377,
     "end_time": "2025-05-28T15:10:58.951811",
     "exception": false,
     "start_time": "2025-05-28T15:10:58.578434",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from openvino.tools import mo # 用于模型转换\n",
    "import openvino as ov\n",
    "from openvino.runtime import Core # 用于模型加载和推理\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def convert_pytorch_to_openvino(pytorch_model, model_name, output_dir, example_input_shape,i):\n",
    "    \"\"\"\n",
    "    将 PyTorch 模型转换为 OpenVINO IR 格式。\n",
    "\n",
    "    Args:\n",
    "        pytorch_model: 一个已训练的 PyTorch 模型实例。\n",
    "        model_name (str): 目标 OpenVINO 模型文件的名称 (例如 'my_model')。\n",
    "        output_dir (str): 保存 .xml 和 .bin 文件的目录。\n",
    "        example_input_shape (tuple): 模型期望的输入形状 (例如 (batch_size, channels, height, width))。\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    onnx_path = os.path.join(output_dir, f\"{model_name}.onnx\")\n",
    "    xml_path = os.path.join(output_dir, f\"{model_name}.xml\")\n",
    "    \n",
    "    pytorch_model.eval()\n",
    "    pytorch_model.cpu() \n",
    "\n",
    "    print(f\"将 PyTorch 模型 '{model_name}' 导出为 ONNX...\")\n",
    "    dummy_input = torch.randn(example_input_shape)\n",
    "    \n",
    "    try:\n",
    "        torch.onnx.export(pytorch_model,\n",
    "                          dummy_input,\n",
    "                          onnx_path,\n",
    "                          verbose=False,\n",
    "                          opset_version=13, # 选择合适的 ONNX opset 版本\n",
    "                          input_names=['input_0'], # 输入层名称\n",
    "                          output_names=['output_0'], # 输出层名称\n",
    "                          dynamic_axes={'input_0': {0: 'batch_size'}} # 如果需要动态 batch size\n",
    "                         )\n",
    "        print(f\"ONNX 模型已保存到: {onnx_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"导出 ONNX 失败: {e}\")\n",
    "        return None\n",
    "\n",
    "    # 2. 使用 Model Optimizer 将 ONNX 模型转换为 OpenVINO IR 格式\n",
    "    print(f\"使用 Model Optimizer 将 ONNX 模型转换为 OpenVINO IR...\")\n",
    "    try:\n",
    "        ov_model = mo.convert_model(onnx_path, \n",
    "                                    compress_to_fp16=True # 可以选择压缩为 FP16 精度以减小模型大小和提高推理速度\n",
    "                                   )\n",
    "        \n",
    "        # 保存 OpenVINO IR 模型\n",
    "        ov.save_model(ov_model,xml_path)\n",
    "        print(f\"OpenVINO IR 模型已保存到: {xml_path} 和 {Path(xml_path).with_suffix('.bin')}\")\n",
    "        return xml_path\n",
    "    except Exception as e:\n",
    "        print(f\"转换为 OpenVINO IR 失败: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8464c4f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T15:10:58.961407Z",
     "iopub.status.busy": "2025-05-28T15:10:58.960862Z",
     "iopub.status.idle": "2025-05-28T15:10:58.968647Z",
     "shell.execute_reply": "2025-05-28T15:10:58.967297Z"
    },
    "papermill": {
     "duration": 0.014906,
     "end_time": "2025-05-28T15:10:58.970553",
     "exception": false,
     "start_time": "2025-05-28T15:10:58.955647",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_openvino_model(xml_path, device=\"CPU\"):\n",
    "    \"\"\"\n",
    "    加载 OpenVINO IR 模型并编译。\n",
    "\n",
    "    Args:\n",
    "        xml_path (str): OpenVINO IR 模型的 .xml 文件路径。\n",
    "        device (str): 推理设备\n",
    "    Returns:\n",
    "        openvino.runtime.CompiledModel: 编译后的 OpenVINO 模型对象。\n",
    "    \"\"\"\n",
    "    core = Core()\n",
    "    model = core.read_model(model=xml_path)\n",
    "    \n",
    "    # 编译模型以优化到指定设备\n",
    "    compiled_model = core.compile_model(model=model, device_name=device)\n",
    "    print(f\"OpenVINO 模型 '{Path(xml_path).stem}' 已编译到设备: {device}\")\n",
    "    return compiled_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6c3d414",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T15:10:58.980304Z",
     "iopub.status.busy": "2025-05-28T15:10:58.979890Z",
     "iopub.status.idle": "2025-05-28T15:11:11.308910Z",
     "shell.execute_reply": "2025-05-28T15:11:11.307136Z"
    },
    "papermill": {
     "duration": 12.336665,
     "end_time": "2025-05-28T15:11:11.311142",
     "exception": false,
     "start_time": "2025-05-28T15:10:58.974477",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "将 PyTorch 模型 'efficientnet_b0' 导出为 ONNX...\n",
      "ONNX 模型已保存到: /kaggle/working/model_0/efficientnet_b0.onnx\n",
      "使用 Model Optimizer 将 ONNX 模型转换为 OpenVINO IR...\n",
      "[ INFO ] MO command line tool is considered as the legacy conversion API as of OpenVINO 2023.2 release.\n",
      "In 2025.0 MO command line tool and openvino.tools.mo.convert_model() will be removed. Please use OpenVINO Model Converter (OVC) or openvino.convert_model(). OVC represents a lightweight alternative of MO and provides simplified model conversion API. \n",
      "Find more information about transition from MO to OVC at https://docs.openvino.ai/2023.2/openvino_docs_OV_Converter_UG_prepare_model_convert_model_MO_OVC_transition.html\n",
      "OpenVINO IR 模型已保存到: /kaggle/working/model_0/efficientnet_b0.xml 和 /kaggle/working/model_0/efficientnet_b0.bin\n",
      "模型转换成功，IR路径: /kaggle/working/model_0/efficientnet_b0.xml\n",
      "将 PyTorch 模型 'efficientnet_b0' 导出为 ONNX...\n",
      "ONNX 模型已保存到: /kaggle/working/model_1/efficientnet_b0.onnx\n",
      "使用 Model Optimizer 将 ONNX 模型转换为 OpenVINO IR...\n",
      "[ INFO ] MO command line tool is considered as the legacy conversion API as of OpenVINO 2023.2 release.\n",
      "In 2025.0 MO command line tool and openvino.tools.mo.convert_model() will be removed. Please use OpenVINO Model Converter (OVC) or openvino.convert_model(). OVC represents a lightweight alternative of MO and provides simplified model conversion API. \n",
      "Find more information about transition from MO to OVC at https://docs.openvino.ai/2023.2/openvino_docs_OV_Converter_UG_prepare_model_convert_model_MO_OVC_transition.html\n",
      "OpenVINO IR 模型已保存到: /kaggle/working/model_1/efficientnet_b0.xml 和 /kaggle/working/model_1/efficientnet_b0.bin\n",
      "模型转换成功，IR路径: /kaggle/working/model_1/efficientnet_b0.xml\n",
      "将 PyTorch 模型 'efficientnet_b0' 导出为 ONNX...\n",
      "ONNX 模型已保存到: /kaggle/working/model_2/efficientnet_b0.onnx\n",
      "使用 Model Optimizer 将 ONNX 模型转换为 OpenVINO IR...\n",
      "[ INFO ] MO command line tool is considered as the legacy conversion API as of OpenVINO 2023.2 release.\n",
      "In 2025.0 MO command line tool and openvino.tools.mo.convert_model() will be removed. Please use OpenVINO Model Converter (OVC) or openvino.convert_model(). OVC represents a lightweight alternative of MO and provides simplified model conversion API. \n",
      "Find more information about transition from MO to OVC at https://docs.openvino.ai/2023.2/openvino_docs_OV_Converter_UG_prepare_model_convert_model_MO_OVC_transition.html\n",
      "OpenVINO IR 模型已保存到: /kaggle/working/model_2/efficientnet_b0.xml 和 /kaggle/working/model_2/efficientnet_b0.bin\n",
      "模型转换成功，IR路径: /kaggle/working/model_2/efficientnet_b0.xml\n",
      "finish\n"
     ]
    }
   ],
   "source": [
    "# one example\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    for model_name,model_paths in cfg.model_dicts.items():\n",
    "        for i,model_path in enumerate(model_paths):\n",
    "            dummy_model = Efficientb0Model(cfg,model_name)\n",
    "            checkpoint = torch.load(model_path, map_location='cpu',weights_only = False)\n",
    "            dummy_model.load_state_dict(checkpoint['model_state_dict']) # 加载训练好的权重\n",
    "        \n",
    "            output_ir_path = convert_pytorch_to_openvino(\n",
    "                dummy_model, \n",
    "                model_name, \n",
    "                output_dir=f\"/kaggle/working/model_{i}\", \n",
    "                example_input_shape=(1, 1, 256,256) ,\n",
    "                i=i\n",
    "            )\n",
    "            if output_ir_path:\n",
    "                print(f\"模型转换成功，IR路径: {output_ir_path}\")\n",
    "            else:\n",
    "                print(\"模型转换失败。\")\n",
    "    print(\"finish\")\n",
    "\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 11361821,
     "sourceId": 91844,
     "sourceType": "competition"
    },
    {
     "datasetId": 7430593,
     "sourceId": 11828260,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7457365,
     "sourceId": 11867185,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7459455,
     "sourceId": 11870082,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7459522,
     "sourceId": 11870175,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7459867,
     "sourceId": 11870659,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7477284,
     "sourceId": 11895503,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7487257,
     "sourceId": 11909812,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 235777618,
     "sourceType": "kernelVersion"
    }
   ],
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 55.893243,
   "end_time": "2025-05-28T15:11:14.038070",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-05-28T15:10:18.144827",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
