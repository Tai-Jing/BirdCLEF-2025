{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ecd47fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T08:14:06.927061Z",
     "iopub.status.busy": "2025-05-28T08:14:06.926180Z",
     "iopub.status.idle": "2025-05-28T08:14:18.044563Z",
     "shell.execute_reply": "2025-05-28T08:14:18.043499Z"
    },
    "papermill": {
     "duration": 11.124591,
     "end_time": "2025-05-28T08:14:18.046766",
     "exception": false,
     "start_time": "2025-05-28T08:14:06.922175",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: ../input/openvino-wheels\r\n",
      "Processing /kaggle/input/openvino-wheels/openvino_dev-2024.6.0-17404-py3-none-any.whl (from openvino-dev[onnx]==2024.6.0->-r ../input/openvino-wheels/requirements.txt (line 1))\r\n",
      "Requirement already satisfied: defusedxml>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from openvino-dev==2024.6.0->openvino-dev[onnx]==2024.6.0->-r ../input/openvino-wheels/requirements.txt (line 1)) (0.7.1)\r\n",
      "Processing /kaggle/input/openvino-wheels/networkx-3.1-py3-none-any.whl (from openvino-dev==2024.6.0->openvino-dev[onnx]==2024.6.0->-r ../input/openvino-wheels/requirements.txt (line 1))\r\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.16.6 in /usr/local/lib/python3.11/dist-packages (from openvino-dev==2024.6.0->openvino-dev[onnx]==2024.6.0->-r ../input/openvino-wheels/requirements.txt (line 1)) (1.26.4)\r\n",
      "Processing /kaggle/input/openvino-wheels/openvino_telemetry-2025.1.0-py3-none-any.whl (from openvino-dev==2024.6.0->openvino-dev[onnx]==2024.6.0->-r ../input/openvino-wheels/requirements.txt (line 1))\r\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from openvino-dev==2024.6.0->openvino-dev[onnx]==2024.6.0->-r ../input/openvino-wheels/requirements.txt (line 1)) (25.0)\r\n",
      "Requirement already satisfied: pyyaml>=5.4.1 in /usr/local/lib/python3.11/dist-packages (from openvino-dev==2024.6.0->openvino-dev[onnx]==2024.6.0->-r ../input/openvino-wheels/requirements.txt (line 1)) (6.0.2)\r\n",
      "Requirement already satisfied: requests>=2.25.1 in /usr/local/lib/python3.11/dist-packages (from openvino-dev==2024.6.0->openvino-dev[onnx]==2024.6.0->-r ../input/openvino-wheels/requirements.txt (line 1)) (2.32.3)\r\n",
      "Processing /kaggle/input/openvino-wheels/openvino-2024.6.0-17404-cp311-cp311-manylinux2014_x86_64.whl (from openvino-dev==2024.6.0->openvino-dev[onnx]==2024.6.0->-r ../input/openvino-wheels/requirements.txt (line 1))\r\n",
      "Processing /kaggle/input/openvino-wheels/fastjsonschema-2.17.1-py3-none-any.whl (from openvino-dev[onnx]==2024.6.0->-r ../input/openvino-wheels/requirements.txt (line 1))\r\n",
      "Requirement already satisfied: onnx<=1.17.0,>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from openvino-dev[onnx]==2024.6.0->-r ../input/openvino-wheels/requirements.txt (line 1)) (1.17.0)\r\n",
      "Requirement already satisfied: protobuf<4.0.0,>=3.18.1 in /usr/local/lib/python3.11/dist-packages (from openvino-dev[onnx]==2024.6.0->-r ../input/openvino-wheels/requirements.txt (line 1)) (3.20.3)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<2.0.0,>=1.16.6->openvino-dev==2024.6.0->openvino-dev[onnx]==2024.6.0->-r ../input/openvino-wheels/requirements.txt (line 1)) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<2.0.0,>=1.16.6->openvino-dev==2024.6.0->openvino-dev[onnx]==2024.6.0->-r ../input/openvino-wheels/requirements.txt (line 1)) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<2.0.0,>=1.16.6->openvino-dev==2024.6.0->openvino-dev[onnx]==2024.6.0->-r ../input/openvino-wheels/requirements.txt (line 1)) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<2.0.0,>=1.16.6->openvino-dev==2024.6.0->openvino-dev[onnx]==2024.6.0->-r ../input/openvino-wheels/requirements.txt (line 1)) (2025.1.0)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<2.0.0,>=1.16.6->openvino-dev==2024.6.0->openvino-dev[onnx]==2024.6.0->-r ../input/openvino-wheels/requirements.txt (line 1)) (2022.1.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<2.0.0,>=1.16.6->openvino-dev==2024.6.0->openvino-dev[onnx]==2024.6.0->-r ../input/openvino-wheels/requirements.txt (line 1)) (2.4.1)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.25.1->openvino-dev==2024.6.0->openvino-dev[onnx]==2024.6.0->-r ../input/openvino-wheels/requirements.txt (line 1)) (3.4.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.25.1->openvino-dev==2024.6.0->openvino-dev[onnx]==2024.6.0->-r ../input/openvino-wheels/requirements.txt (line 1)) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.25.1->openvino-dev==2024.6.0->openvino-dev[onnx]==2024.6.0->-r ../input/openvino-wheels/requirements.txt (line 1)) (2.4.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.25.1->openvino-dev==2024.6.0->openvino-dev[onnx]==2024.6.0->-r ../input/openvino-wheels/requirements.txt (line 1)) (2025.4.26)\r\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.0.0,>=1.16.6->openvino-dev==2024.6.0->openvino-dev[onnx]==2024.6.0->-r ../input/openvino-wheels/requirements.txt (line 1)) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.0.0,>=1.16.6->openvino-dev==2024.6.0->openvino-dev[onnx]==2024.6.0->-r ../input/openvino-wheels/requirements.txt (line 1)) (2022.1.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<2.0.0,>=1.16.6->openvino-dev==2024.6.0->openvino-dev[onnx]==2024.6.0->-r ../input/openvino-wheels/requirements.txt (line 1)) (1.3.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<2.0.0,>=1.16.6->openvino-dev==2024.6.0->openvino-dev[onnx]==2024.6.0->-r ../input/openvino-wheels/requirements.txt (line 1)) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<2.0.0,>=1.16.6->openvino-dev==2024.6.0->openvino-dev[onnx]==2024.6.0->-r ../input/openvino-wheels/requirements.txt (line 1)) (2024.2.0)\r\n",
      "Installing collected packages: openvino-telemetry, fastjsonschema, networkx, openvino, openvino-dev\r\n",
      "  Attempting uninstall: fastjsonschema\r\n",
      "    Found existing installation: fastjsonschema 2.21.1\r\n",
      "    Uninstalling fastjsonschema-2.21.1:\r\n",
      "      Successfully uninstalled fastjsonschema-2.21.1\r\n",
      "  Attempting uninstall: networkx\r\n",
      "    Found existing installation: networkx 3.4.2\r\n",
      "    Uninstalling networkx-3.4.2:\r\n",
      "      Successfully uninstalled networkx-3.4.2\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "nx-cugraph-cu12 25.2.0 requires networkx>=3.2, but you have networkx 3.1 which is incompatible.\r\n",
      "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.9.0.13 which is incompatible.\r\n",
      "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\r\n",
      "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.4.0.6 which is incompatible.\r\n",
      "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.10.19 which is incompatible.\r\n",
      "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.7.4.40 which is incompatible.\r\n",
      "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.9.5 which is incompatible.\r\n",
      "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.9.41 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed fastjsonschema-2.17.1 networkx-3.1 openvino-2024.6.0 openvino-dev-2024.6.0 openvino-telemetry-2025.1.0\r\n"
     ]
    }
   ],
   "source": [
    "! python -m pip install --no-index --find-links=../input/openvino-wheels -r ../input/openvino-wheels/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fd8df00",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T08:14:18.056297Z",
     "iopub.status.busy": "2025-05-28T08:14:18.055469Z",
     "iopub.status.idle": "2025-05-28T08:14:48.896345Z",
     "shell.execute_reply": "2025-05-28T08:14:48.771862Z"
    },
    "papermill": {
     "duration": 30.856957,
     "end_time": "2025-05-28T08:14:48.907342",
     "exception": false,
     "start_time": "2025-05-28T08:14:18.050385",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用设备: cpu\n",
      "加载分类数据...\n",
      "类别数量: 206\n",
      "ONNX 导出使用的输入形状: (1, 1, 256, 768)\n",
      "将 PyTorch 模型 'seresnext26t_32x4d' 导出为 ONNX...\n",
      "ONNX 模型已保存到: ./openvino_models/seresnext26t_32x4d.onnx\n",
      "使用 Model Optimizer 将 ONNX 模型转换为 OpenVINO IR...\n",
      "OpenVINO IR 模型已保存到: ./openvino_models/seresnext26t_32x4d.xml 和 openvino_models/seresnext26t_32x4d.bin\n",
      "\n",
      "模型转换成功，IR 路径: ./openvino_models/seresnext26t_32x4d.xml\n",
      "\n",
      "--- 测试 OpenVINO 模型推理 ---\n",
      "OpenVINO 模型 'seresnext26t_32x4d' 已编译到设备: CPU\n",
      "OpenVINO 推理使用的示例输入形状: (4, 1, 256, 768)\n",
      "OpenVINO 推理片段级 logits 形状: (4, 206)\n",
      "OpenVINO 推理帧级概率形状: (4, 24, 206)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import warnings\n",
    "import logging\n",
    "import time\n",
    "import math\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "from soundfile import SoundFile \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.cuda.amp import autocast, GradScaler # 尽管此处未使用，但保留原始依赖\n",
    "import timm\n",
    "from tqdm.auto import tqdm # 进度条，此处未使用\n",
    "from glob import glob\n",
    "import torchaudio\n",
    "import random\n",
    "import itertools\n",
    "from typing import Union\n",
    "\n",
    "# OpenVINO 特有导入\n",
    "from openvino.tools import mo # 用于模型转换 (较旧API，推荐使用 ov.convert_model)\n",
    "import openvino as ov\n",
    "from openvino.runtime import Core # 用于模型加载和推理\n",
    "import openvino.torch # 对于 PyTorch 前端至关重要\n",
    "\n",
    "warnings.filterwarnings(\"ignore\") # 忽略警告\n",
    "logging.basicConfig(level=logging.ERROR) # 设置日志级别为ERROR，忽略普通信息\n",
    "\n",
    "class CFG:\n",
    "    \"\"\"\n",
    "    全局配置类\n",
    "    \"\"\"\n",
    "    seed = 42 # 随机种子\n",
    "    print_freq = 100 # 打印频率\n",
    "    num_workers = 4 # 数据加载工作线程数\n",
    "\n",
    "    stage = 'train_bce' # 模型阶段 (此处为训练阶段的名称，转换时实际不影响)\n",
    "\n",
    "    train_datadir = '/kaggle/input/birdclef-2025/train_audio' # 训练音频目录\n",
    "    train_csv = '/kaggle/input/birdclef-2025/train.csv' # 训练CSV文件\n",
    "    test_soundscapes = '/kaggle/input/birdclef-2025/test_soundscapes' # 测试声景目录\n",
    "    submission_csv = '/kaggle/input/birdclef-2025/sample_submission.csv' # 提交文件模板\n",
    "    taxonomy_csv = '/kaggle/input/birdclef-2025/taxonomy.csv' # 分类学CSV文件\n",
    "    model_files = ['/kaggle/input/bird2025-sed-ckpt/sedmodel.pth'] # 模型文件路径列表\n",
    " \n",
    "    model_name = 'seresnext26t_32x4d' # 模型骨干网络名称 \n",
    "    pretrained = False # 是否使用预训练权重 (此处为False，实际加载的是本地检查点)\n",
    "    in_channels = 1 # 输入通道数 (音频频谱图为单通道)\n",
    "\n",
    "    SR = 32000 # 采样率\n",
    "    target_duration = 5 # 目标输出片段时长 (例如，每5秒一个预测)\n",
    "    train_duration = 10 # 模型训练时使用的音频片段时长 (例如，模型期望10秒的输入)\n",
    "    \n",
    "    device = 'cpu' # 推理设备 (此处设置为CPU)\n",
    "\n",
    "cfg = CFG()\n",
    "print(f\"使用设备: {cfg.device}\")\n",
    "print(f\"加载分类数据...\")\n",
    "taxonomy_df = pd.read_csv(cfg.taxonomy_csv)\n",
    "species_ids = taxonomy_df['primary_label'].tolist()\n",
    "num_classes = len(species_ids)\n",
    "print(f\"类别数量: {num_classes}\")\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    \"\"\"\n",
    "    设置随机种子以确保结果可复现\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed) # CUDA随机种子，即使不使用CUDA也安全\n",
    "    torch.cuda.manual_seed_all(seed) # 所有CUDA设备随机种子\n",
    "    torch.backends.cudnn.deterministic = True # 确保确定性行为\n",
    "    torch.backends.cudnn.benchmark = False # 禁用CuDNN基准测试以提高确定性\n",
    "\n",
    "set_seed(cfg.seed)\n",
    "\n",
    "class AttBlockV2(nn.Module):\n",
    "    \"\"\"\n",
    "    注意力模块 V2\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features: int, out_features: int, activation=\"linear\"):\n",
    "        super().__init__()\n",
    "\n",
    "        self.activation = activation # 激活函数类型\n",
    "        self.att = nn.Conv1d( # 注意力卷积层\n",
    "            in_channels=in_features,\n",
    "            out_channels=out_features,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            bias=True,\n",
    "        )\n",
    "        self.cla = nn.Conv1d( # 分类卷积层\n",
    "            in_channels=in_features,\n",
    "            out_channels=out_features,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            bias=True,\n",
    "        )\n",
    "\n",
    "        self.init_weights() # 初始化权重\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\"初始化卷积层权重\"\"\"\n",
    "        init_layer(self.att)\n",
    "        init_layer(self.cla)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (批次大小, 输入特征维度, 时间步长)\n",
    "        norm_att = torch.softmax(torch.tanh(self.att(x)), dim=-1) # 注意力权重，经过tanh和softmax\n",
    "        cla = self.nonlinear_transform(self.cla(x)) # 分类输出，经过非线性变换\n",
    "        x = torch.sum(norm_att * cla, dim=2) # 将注意力权重应用于分类输出并求和\n",
    "        return x, norm_att, cla # 返回片段级输出、注意力权重和帧级分类输出\n",
    "\n",
    "    def nonlinear_transform(self, x):\n",
    "        \"\"\"根据配置应用非线性变换\"\"\"\n",
    "        if self.activation == \"linear\":\n",
    "            return x\n",
    "        elif self.activation == \"sigmoid\":\n",
    "            return torch.sigmoid(x)\n",
    "\n",
    "\n",
    "def init_layer(layer):\n",
    "    \"\"\"初始化神经网络层的权重\"\"\"\n",
    "    nn.init.xavier_uniform_(layer.weight) # 使用Xavier均匀分布初始化\n",
    "\n",
    "    if hasattr(layer, \"bias\"):\n",
    "        if layer.bias is not None:\n",
    "            layer.bias.data.fill_(0.0) # 偏差初始化为0\n",
    "\n",
    "def init_bn(bn):\n",
    "    \"\"\"初始化BatchNorm层的权重和偏差\"\"\"\n",
    "    bn.bias.data.fill_(0.0)\n",
    "    bn.weight.data.fill_(1.0)\n",
    "\n",
    "\n",
    "\n",
    "class BirdCLEFModel(nn.Module):\n",
    "    \"\"\"\n",
    "    BirdCLEF 竞赛主模型，用于声音事件检测\n",
    "    \"\"\"\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg # 加载自检查点的模型配置字典\n",
    "        \n",
    "        # 类别数量，从cfg字典中获取，如果不存在则使用全局num_classes\n",
    "        self.num_classes = cfg.get('num_classes', num_classes) \n",
    "\n",
    "        self.bn0 = nn.BatchNorm2d(self.cfg['n_mels']) # Mel频谱图的初始BatchNorm层\n",
    "        \n",
    "        self.backbone = timm.create_model( # 创建骨干网络模型\n",
    "            self.cfg['model_name'], # 模型名称，例如 'seresnext26t_32x4d'\n",
    "            pretrained=False, # 导出时通常不使用预训练，因为权重已加载\n",
    "            in_chans=self.cfg['in_channels'], # 输入通道数\n",
    "            drop_rate=0.0, # 导出时禁用Dropout以获得稳定的计算图\n",
    "            drop_path_rate=0.0, # 导出时禁用DropPath以获得稳定的计算图\n",
    "        )\n",
    "\n",
    "        layers = list(self.backbone.children())[:-2] # 获取骨干网络除最后两层外的所有层\n",
    "        self.encoder = nn.Sequential(*layers) # 构建编码器\n",
    "        \n",
    "        # 根据骨干网络名称确定其输出特征维度\n",
    "        if \"efficientnet\" in self.cfg['model_name']:\n",
    "            backbone_out = self.backbone.classifier.in_features\n",
    "        elif \"eca\" in self.cfg['model_name']:\n",
    "            backbone_out = self.backbone.head.fc.in_features\n",
    "        elif \"res\" in self.cfg['model_name']: # 包含 seresnext 系列\n",
    "            backbone_out = self.backbone.fc.in_features\n",
    "        else:\n",
    "            backbone_out = self.backbone.num_features\n",
    "            \n",
    "        self.fc1 = nn.Linear(backbone_out, backbone_out, bias=True) # 全连接层\n",
    "        self.att_block = AttBlockV2(backbone_out, self.num_classes, activation=\"sigmoid\") # 注意力模块\n",
    "        self.melspec_transform = torchaudio.transforms.MelSpectrogram(\n",
    "            sample_rate=self.cfg['SR'],\n",
    "            hop_length=self.cfg['hop_length'],\n",
    "            n_mels=self.cfg['n_mels'],\n",
    "            f_min=self.cfg['f_min'],\n",
    "            f_max=self.cfg['f_max'],\n",
    "            n_fft=self.cfg['n_fft'],\n",
    "            pad_mode=\"constant\",\n",
    "            norm=\"slaney\",\n",
    "            onesided=True,\n",
    "            mel_scale=\"htk\",\n",
    "        )\n",
    "\n",
    "        self.melspec_transform = self.melspec_transform.cpu()\n",
    "\n",
    "        self.db_transform = torchaudio.transforms.AmplitudeToDB(\n",
    "            stype=\"power\", top_db=80\n",
    "        )\n",
    "    def extract_feature(self, x):\n",
    "        \"\"\"\n",
    "        从频谱图提取特征\n",
    "        x 的形状为 (批次大小, 1, Mel通道数, 帧数)\n",
    "        \"\"\"\n",
    "        x = x.permute((0, 1, 3, 2)) # 调整维度顺序 (B, 1, Freq, Time) -> (B, 1, Time, Freq)\n",
    "        # frames_num = x.shape[2] # 帧数，此处不直接用于图导出\n",
    "\n",
    "        x = x.transpose(1, 3) # 为 BatchNorm2d 准备 (B, Freq, Time, 1) -> (B, Time, 1, Freq) 维度不对\n",
    "        # 更正：BatchNorm2d 期望 (N, C, H, W)。如果将 (B, 1, Time, Freq) 视为 (N, C, H, W)，则 H=Time, W=Freq。\n",
    "        # 那么 C 应该是 1。\n",
    "        # 如果 self.bn0 是 nn.BatchNorm2d(cfg['n_mels'])，那么它期望 C = n_mels。\n",
    "        # 因此，x 的形状应该是 (B, n_mels, Time, 1) 或者 (B, n_mels, 1, Time)\n",
    "        # 原始代码的 bn0 应用在 (B, C, H, W) 形状，其中 C=n_mels。\n",
    "        # x = x.transpose(1, 3) 这一步将 (B, 1, Time, Freq) 变成 (B, Freq, Time, 1)。\n",
    "        # 也就是 (B, n_mels, Time, 1)。这与 BatchNorm2d(n_mels) 的期望一致。\n",
    "        x = self.bn0(x) # 应用批量归一化\n",
    "        x = x.transpose(1, 3) # 再次调整维度顺序 (B, n_mels, Time, 1) -> (B, 1, Time, n_mels)\n",
    "        \n",
    "        x = x.transpose(2, 3) # 调整为 (B, 1, n_mels, Time) 适应骨干网络输入\n",
    "        \n",
    "        # (批次大小, 通道数, 频率, 帧数)\n",
    "        x = self.encoder(x) # 通过编码器（骨干网络）提取特征\n",
    "        \n",
    "        # (批次大小, 通道数, 帧数)\n",
    "        x = torch.mean(x, dim=2) # 在频率维度上进行平均池化\n",
    "        \n",
    "        # 通道平滑\n",
    "        x1 = F.max_pool1d(x, kernel_size=3, stride=1, padding=1)\n",
    "        x2 = F.avg_pool1d(x, kernel_size=3, stride=1, padding=1)\n",
    "        x = x1 + x2\n",
    "        \n",
    "        # Dropout 在 model.eval() 模式下会自动跳过，导出时不会出现在图中\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = x.transpose(1, 2) # (批次大小, 帧数, 通道数)\n",
    "        x = F.relu_(self.fc1(x)) # 激活全连接层\n",
    "        x = x.transpose(1, 2) # (批次大小, 通道数, 帧数)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        return x, x.shape[-1] # 返回特征图和其时间维度的大小\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        模型的前向传播，用于 OpenVINO 导出\n",
    "        x 是原始音频输入，形状为 (批次大小, 1, 采样点数)\n",
    "        \"\"\"\n",
    "\n",
    "        x_features, _ = self.extract_feature(x) # 从频谱图提取特征\n",
    "\n",
    "        # AttBlockV2 在 activation=\"sigmoid\" 时，其输出 (clipwise_output 和 segmentwise_output) 已经是 sigmoid 概率\n",
    "        (clipwise_output, norm_att, segmentwise_output) = self.att_block(x_features)\n",
    "\n",
    "        # 为了方便在 Python/NumPy 中进行 TTA 后处理，我们返回帧级别的概率和片段级别的概率。\n",
    "        # segmentwise_output 默认形状为 (batch_size, num_classes, time_frames)。\n",
    "        # 此处将其转置为 (batch_size, time_frames, num_classes) 以方便后续处理。\n",
    "        # clipwise_output 形状为 (batch_size, num_classes)。\n",
    "        \n",
    "        # 原始模型的 forward 返回 torch.logit(clipwise_output)。\n",
    "        # 为了保持一致性，我们在这里也返回 logit。\n",
    "        return torch.logit(clipwise_output), segmentwise_output.transpose(1, 2) # 返回片段级 logits 和帧级概率\n",
    "\n",
    "\n",
    "# --- OpenVINO 模型转换函数 ---\n",
    "def convert_pytorch_to_openvino(pytorch_model, model_name, output_dir, example_input_shape):\n",
    "    \"\"\"\n",
    "    将 PyTorch 模型转换为 OpenVINO IR (Intermediate Representation) 格式。\n",
    "\n",
    "    Args:\n",
    "        pytorch_model: 一个已训练的 PyTorch 模型实例。\n",
    "        model_name (str): 目标 OpenVINO 模型文件的名称 (例如 'my_model')。\n",
    "        output_dir (str): 保存 .xml 和 .bin 文件的目录。\n",
    "        example_input_shape (tuple): 模型期望的输入形状 (例如 (批次大小, 通道数, 音频采样点数))。\n",
    "    Returns:\n",
    "        str: 转换后的 OpenVINO IR 模型的 .xml 文件路径，如果转换失败则返回 None。\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True) # 创建输出目录\n",
    "    onnx_path = os.path.join(output_dir, f\"{model_name}.onnx\") # ONNX 模型路径\n",
    "    xml_path = os.path.join(output_dir, f\"{model_name}.xml\") # OpenVINO IR 模型路径\n",
    "    \n",
    "    pytorch_model.eval() # 设置模型为评估模式 (禁用 Dropout 等)\n",
    "    pytorch_model.cpu() # 将模型移动到 CPU 进行导出 (通常导出在CPU上进行)\n",
    "\n",
    "    print(f\"将 PyTorch 模型 '{model_name}' 导出为 ONNX...\")\n",
    "    dummy_input = torch.randn(example_input_shape) # 创建虚拟输入以追踪模型计算图\n",
    "    \n",
    "    try:\n",
    "        torch.onnx.export(pytorch_model,\n",
    "                          dummy_input,\n",
    "                          onnx_path,\n",
    "                          verbose=False, # 不打印详细的 ONNX 图信息\n",
    "                          opset_version=17, # 推荐的 ONNX opset 版本，兼容性好\n",
    "                          input_names=['input_audio'], # 为输入层定义名称\n",
    "                          output_names=['clipwise_logits', 'segmentwise_probabilities'], # 为输出层定义名称\n",
    "                          dynamic_axes={'input_audio': {0: 'batch_size'}} # 允许批次大小是动态的\n",
    "                         )\n",
    "        print(f\"ONNX 模型已保存到: {onnx_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"导出 ONNX 失败: {e}\")\n",
    "        return None\n",
    "\n",
    "    # 2. 使用 Model Optimizer (ov.convert_model) 将 ONNX 模型转换为 OpenVINO IR 格式\n",
    "    print(f\"使用 Model Optimizer 将 ONNX 模型转换为 OpenVINO IR...\")\n",
    "    try:\n",
    "        # ov.convert_model 是 OpenVINO 最新版本推荐的转换 API\n",
    "        ov_model = ov.convert_model(onnx_path, \n",
    "                                   )\n",
    "        \n",
    "        # 保存 OpenVINO IR 模型 (.xml 和 .bin 文件)\n",
    "        ov.save_model(ov_model, xml_path)\n",
    "        print(f\"OpenVINO IR 模型已保存到: {xml_path} 和 {Path(xml_path).with_suffix('.bin')}\")\n",
    "        return xml_path\n",
    "    except Exception as e:\n",
    "        print(f\"转换为 OpenVINO IR 失败: {e}\")\n",
    "        return None\n",
    "\n",
    "def load_openvino_model(xml_path, device=\"CPU\"):\n",
    "    \"\"\"\n",
    "    加载 OpenVINO IR 模型并编译。\n",
    "\n",
    "    Args:\n",
    "        xml_path (str): OpenVINO IR 模型的 .xml 文件路径。\n",
    "        device (str): 推理设备 (例如 \"CPU\", \"GPU\", \"NPU\")。\n",
    "    Returns:\n",
    "        openvino.runtime.CompiledModel: 编译后的 OpenVINO 模型对象。\n",
    "    \"\"\"\n",
    "    core = Core() # 创建 OpenVINO Core 对象\n",
    "    model = core.read_model(model=xml_path) # 读取 OpenVINO IR 模型\n",
    "    \n",
    "    # 编译模型以优化到指定设备\n",
    "    compiled_model = core.compile_model(model=model, device_name=device)\n",
    "    print(f\"OpenVINO 模型 '{Path(xml_path).stem}' 已编译到设备: {device}\")\n",
    "    return compiled_model\n",
    "\n",
    "# --- 主执行块 (模型转换和示例推理) ---\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # 加载检查点以获取训练时使用的配置 (例如 n_mels, SR, hop_length 等)\n",
    "    checkpoint = torch.load(cfg.model_files[0], map_location='cpu', weights_only=False)\n",
    "    cfg_temp = checkpoint[\"cfg\"] # 这包含了训练时的所有必要参数\n",
    "\n",
    "  \n",
    "    input_shape_for_export = (1, cfg.in_channels,256, 768 )\n",
    "    print(f\"ONNX 导出使用的输入形状: {input_shape_for_export}\")\n",
    "\n",
    "    # 使用从检查点加载的配置实例化模型\n",
    "    dummy_model = BirdCLEFModel(cfg_temp) \n",
    "    dummy_model.load_state_dict(checkpoint['model_state_dict']) # 加载训练好的权重\n",
    "\n",
    "    # 将 PyTorch 模型转换为 OpenVINO IR\n",
    "    output_ir_path = convert_pytorch_to_openvino(\n",
    "        dummy_model, \n",
    "        cfg.model_name, \n",
    "        output_dir=\"./openvino_models\", \n",
    "        example_input_shape=input_shape_for_export\n",
    "    )\n",
    "\n",
    "    if output_ir_path:\n",
    "        print(f\"\\n模型转换成功，IR 路径: {output_ir_path}\")\n",
    "        \n",
    "        # --- 测试 OpenVINO 模型推理 ---\n",
    "        print(\"\\n--- 测试 OpenVINO 模型推理 ---\")\n",
    "        ov_compiled_model = load_openvino_model(output_ir_path, device=\"CPU\")\n",
    "\n",
    "        # OpenVINO 推理的输入必须是 NumPy 数组，且形状与导出时一致\n",
    "        batch_size_for_test = 4 # 模拟单段推理\n",
    "        test_audio_input = np.random.rand(batch_size_for_test, cfg.in_channels, 256, 768).astype(np.float32)\n",
    "        print(f\"OpenVINO 推理使用的示例输入形状: {test_audio_input.shape}\")\n",
    "        \n",
    "        # 执行推理\n",
    "        # 通过导出时定义的输出名称来访问结果\n",
    "        results = ov_compiled_model(test_audio_input)\n",
    "        \n",
    "        # 结果是字典，键是 output_names 中定义的名称\n",
    "        clipwise_logits = results['clipwise_logits'] # 片段级 logits\n",
    "        segmentwise_probabilities = results['segmentwise_probabilities'] # 帧级概率\n",
    "\n",
    "        print(f\"OpenVINO 推理片段级 logits 形状: {clipwise_logits.shape}\")\n",
    "        print(f\"OpenVINO 推理帧级概率形状: {segmentwise_probabilities.shape}\")\n",
    "\n",
    "    else:\n",
    "        print(\"模型转换失败。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af657cc0",
   "metadata": {
    "papermill": {
     "duration": 0.007639,
     "end_time": "2025-05-28T08:14:48.926556",
     "exception": false,
     "start_time": "2025-05-28T08:14:48.918917",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 11361821,
     "sourceId": 91844,
     "sourceType": "competition"
    },
    {
     "datasetId": 7430593,
     "sourceId": 11828260,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7457365,
     "sourceId": 11867185,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7459455,
     "sourceId": 11870082,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7459522,
     "sourceId": 11870175,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7459867,
     "sourceId": 11870659,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7477284,
     "sourceId": 11895503,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7487257,
     "sourceId": 11909812,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 235777618,
     "sourceType": "kernelVersion"
    }
   ],
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 49.80784,
   "end_time": "2025-05-28T08:14:51.793790",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-05-28T08:14:01.985950",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
